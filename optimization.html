<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Marc Becker">
<meta name="author" content="Lennard Schneider">
<title>Flexible and Robust Machine Learning Using mlr3 in R - 4&nbsp; Hyperparameter Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./feature-selection.html" rel="next">
<link href="./performance.html" rel="prev">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script><script src="site_libs/quarto-diagram/mermaid.min.js"></script><script src="site_libs/quarto-diagram/mermaid-init.js"></script><link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Flexible and Robust Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="./Flexible-and-Robust-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Getting Started</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./performance.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resampling and Benchmarking</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimization.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./feature-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature Selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pipelines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preprocessing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Preprocessing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./special.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Special Tasks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./technical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Technical</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interpretation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Interpretation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extending.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Extending</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">Appendices</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solutions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tasks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Tasks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./overview-tables.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Overview Tables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Session Info</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-model-tuning" id="toc-sec-model-tuning" class="nav-link active" data-scroll-target="#sec-model-tuning"><span class="toc-section-number">4.1</span>  Model Tuning</a>
  <ul class="collapse">
<li><a href="#sec-learner-search-space" id="toc-sec-learner-search-space" class="nav-link" data-scroll-target="#sec-learner-search-space"><span class="toc-section-number">4.1.1</span>  Learner and Search Space</a></li>
  <li><a href="#sec-terminator" id="toc-sec-terminator" class="nav-link" data-scroll-target="#sec-terminator"><span class="toc-section-number">4.1.2</span>  Terminator</a></li>
  <li><a href="#sec-tuning-instance" id="toc-sec-tuning-instance" class="nav-link" data-scroll-target="#sec-tuning-instance"><span class="toc-section-number">4.1.3</span>  Tuning Instance with <code>ti</code></a></li>
  <li><a href="#sec-tuner" id="toc-sec-tuner" class="nav-link" data-scroll-target="#sec-tuner"><span class="toc-section-number">4.1.4</span>  Tuner</a></li>
  <li><a href="#trigger-the-tuning" id="toc-trigger-the-tuning" class="nav-link" data-scroll-target="#trigger-the-tuning"><span class="toc-section-number">4.1.5</span>  Trigger the Tuning</a></li>
  <li><a href="#sec-simplified-tuning" id="toc-sec-simplified-tuning" class="nav-link" data-scroll-target="#sec-simplified-tuning"><span class="toc-section-number">4.1.6</span>  Quick Tuning with <code>tune</code></a></li>
  <li><a href="#sec-analyzing-result" id="toc-sec-analyzing-result" class="nav-link" data-scroll-target="#sec-analyzing-result"><span class="toc-section-number">4.1.7</span>  Analyzing the Result</a></li>
  <li><a href="#sec-final-model" id="toc-sec-final-model" class="nav-link" data-scroll-target="#sec-final-model"><span class="toc-section-number">4.1.8</span>  Using a tuned model</a></li>
  </ul>
</li>
  <li>
<a href="#advanced-tuning" id="toc-advanced-tuning" class="nav-link" data-scroll-target="#advanced-tuning"><span class="toc-section-number">4.2</span>  Advanced Tuning</a>
  <ul class="collapse">
<li><a href="#sec-encapsulation-fallback" id="toc-sec-encapsulation-fallback" class="nav-link" data-scroll-target="#sec-encapsulation-fallback"><span class="toc-section-number">4.2.1</span>  Encapsulation and Fallback Learner</a></li>
  <li><a href="#sec-advanced-search-spaces" id="toc-sec-advanced-search-spaces" class="nav-link" data-scroll-target="#sec-advanced-search-spaces"><span class="toc-section-number">4.2.2</span>  Advanced Search Spaces</a></li>
  <li><a href="#sec-tuning-spaces" id="toc-sec-tuning-spaces" class="nav-link" data-scroll-target="#sec-tuning-spaces"><span class="toc-section-number">4.2.3</span>  Search Spaces Collection</a></li>
  </ul>
</li>
  <li><a href="#sec-multi-metrics-tuning" id="toc-sec-multi-metrics-tuning" class="nav-link" data-scroll-target="#sec-multi-metrics-tuning"><span class="toc-section-number">4.3</span>  Multi-Objective Tuning</a></li>
  <li><a href="#sec-autotuner" id="toc-sec-autotuner" class="nav-link" data-scroll-target="#sec-autotuner"><span class="toc-section-number">4.4</span>  Automated Tuning with <code>AutoTuner</code></a></li>
  <li>
<a href="#sec-nested-resampling" id="toc-sec-nested-resampling" class="nav-link" data-scroll-target="#sec-nested-resampling"><span class="toc-section-number">4.5</span>  Nested Resampling</a>
  <ul class="collapse">
<li><a href="#nested-resampling-with-autotuner" id="toc-nested-resampling-with-autotuner" class="nav-link" data-scroll-target="#nested-resampling-with-autotuner"><span class="toc-section-number">4.5.1</span>  Nested Resampling with <code>AutoTuner</code></a></li>
  <li><a href="#performance-comparison" id="toc-performance-comparison" class="nav-link" data-scroll-target="#performance-comparison"><span class="toc-section-number">4.5.2</span>  Performance comparison</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">4.6</span>  Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">4.7</span>  Exercises</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/optimization.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/optimization.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-optimization" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    Marc Becker <a href="https://orcid.org/0000-0002-8115-0400" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Ludwig-Maximilians-Universität München
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    Lennard Schneider <a href="https://orcid.org/0000-0003-4152-5308" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Ludwig-Maximilians-Universität München
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    Most machine learning algorithms are configurated by a set of hyperparameters. The goal of hyperparameter optimization is to find the optimal hyperparameter configuration of a machine learning algorithm for a given task. This chapter presents an introduction to hyperparameter optimization in the mlr3 ecosystem. As a practical example, we optimize the <code>cost</code> and <code>gamma</code> hyperparameters of a support vector machine on the sonar task. We introduce the tuning instance class that describes the tuning problem and the tuner class that wraps an optimization algorithm. After running the optimization, we show how to analyze the results and fit a final model. We also show how to run a multi-objective optimization with multiple measures. Then we move on to more advanced topics like search space transformations, fallback learners and encapsulation. Finally, we show how to use nested resampling to get an unbiased estimate of the performance of an optimized model.
  </div>
</div>

</header><div class="cell" data-hash="optimization_cache/html/unnamed-chunk-1_8e6db17bb10e9ebef062d073abb62bf4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="page-columns page-full"><p>Machine learning algorithms usually include parameters and hyperparameters. Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters. In contrast, hyperparameters, are configured by the user and determine how the model will fit its parameters. Examples include setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network. Building a neural network is sometimes referred to as an ‘art’ as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms. So in this chapter, we will demonstrate how to make this into more of a science.</p><div class="no-row-height column-margin column-container"><span class="">Hyperparameters</span></div></div>
<div class="page-columns page-full"><p>The goal of hyperparameter optimization (<a href="#sec-model-tuning"><span>Section&nbsp;4.1</span></a>) or model tuning is to find the optimal configuration of hyperparameters of an ML algorithm for a given task. There is no closed-form mathematical representation (nor analytic gradient information) for model agnostic HPO, instead, we follow a numerical black-box optimization: an ML algorithm is configured with values chosen for one or more hyperparameters, this algorithm is then evaluated (optimally with a robust resampling method) and its performance measured, this is repeated with multiple configurations and the configuration with the best performance is selected. We could think of finding the optimal configuration in the same way as selecting a model from a benchmark experiment, where in this case each model uses the same underlying algorithm but with different hyperparameter configurations. For example, we could naively tune the number of trees in a random forest using basic mlr3 code:</p><div class="no-row-height column-margin column-container"><span class="">HPO: Hyperparameter Optimization</span></div></div>
<div class="cell" data-hash="optimization_cache/html/fig-naivetuning_1ede0a0f84cfd09ded7c2597eb43eb21">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(</span>
<span id="cb2-2"><a href="#cb2-2"></a>  <span class="at">tasks =</span> <span class="fu">tsk</span>(<span class="st">"penguins_simple"</span>),</span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="at">learners =</span> <span class="fu">list</span>(</span>
<span id="cb2-4"><a href="#cb2-4"></a>    <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">num.trees =</span> <span class="dv">1</span>, <span class="at">id =</span> <span class="st">"1 tree"</span>),</span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">num.trees =</span> <span class="dv">10</span>, <span class="at">id =</span> <span class="st">"10 trees"</span>),</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">num.trees =</span> <span class="dv">100</span>, <span class="at">id =</span> <span class="st">"100 trees"</span>)),</span>
<span id="cb2-7"><a href="#cb2-7"></a>  <span class="at">resamplings =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb2-8"><a href="#cb2-8"></a>))</span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="fu">autoplot</span>(bmr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-naivetuning" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="optimization_files/figure-html/fig-naivetuning-1.png" class="img-fluid figure-img" alt="Boxplots for each of the three configurations showing classification error over the three folds. The image shows the worst performance in the model with 1 tree and similar performance with 10 and 100 trees." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.1: In this code example we benchmark three random forest models with 1, 10, and 100 trees respectively, using 3-fold resampling, classification error loss, and tested on the simplified penguin dataset. The plot shows that the models with 10 and 100 trees are better performing across all three folds and 100 trees may be better than 10.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Human trial-and-error (which is essentially what we are doing above), is time-consuming, often biased, error-prone, and computationally irreproducible. Instead, many sophisticated HPO methods (<a href="#sec-tuner"><span>Section&nbsp;4.1.4</span></a>) (or ‘tuners’) have been developed over the last few decades for robust and efficient HPO. Most HPO methods are iterative and propose different configurations until some termination criterion is met, at which point the optimal configuration is then returned (<a href="#fig-optimization-loop">Figure&nbsp;<span>4.2</span></a>). Popular, modern examples are given by algorithms based on evolutionary algorithms or Bayesian optimization methods. Recent HPO methods often also make use of evaluating a configuration at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and training data can always be subsampled to only include a smaller fraction of all available data. The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising). Another interesting direction of HPO is to optimize multiple metrics (<a href="#sec-multi-metrics-tuning"><span>Section&nbsp;4.3</span></a>) simultaneously, e.g., minimizing the generalization error along with the size of the model. This gives rise to multi-objective HPO. For more details on HPO in general, the reader is referred to <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="references.html#ref-hpo_practical" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="hpo_automl">Feurer and Hutter (<a href="references.html#ref-hpo_automl" role="doc-biblioref">2019</a>)</span>.</p>
<div class="cell" data-hash="optimization_cache/html/fig-optimization-loop_c07230e794404ad10318a27827494bb2">
<div class="cell-output-display">
<div id="fig-optimization-loop" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/hpo_loop.png" class="img-fluid figure-img" width="560"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.2: Representation of the hyperparameter optimization loop in mlr3tuning. Blue - Hyperparameter optimization loop. Purple - Objects of the tuning instance supplied by the user. Blue-Green - Internally created objects of the tuning instance. Green - Optimization Algorithm.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="sec-model-tuning" class="level2 page-columns page-full" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="sec-model-tuning">
<span class="header-section-number">4.1</span> Model Tuning</h2>
<p><a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> is the hyperparameter optimization package of the mlr3 ecosystem. At the heart of the package (and indeed any optimization problem) are the R6 classes</p>
<ul>
<li>
<a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html"><code>TuningInstanceMultiCrit</code></a>, which are used to construct a tuning ‘instance’ which describes the optimization problem and stores the results; and</li>
<li>
<a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a> which is used to get and set optimization algorithms.</li>
</ul>
<p>In this section, we will cover these classes as well as other supporting functions and classes. Throughout this section, we will look at optimizing a support vector machine (SVM) on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html"><code>sonar</code></a> data set as a running example.</p>
<section id="sec-learner-search-space" class="level3 page-columns page-full" data-number="4.1.1"><h3 data-number="4.1.1" class="anchored" data-anchor-id="sec-learner-search-space">
<span class="header-section-number">4.1.1</span> Learner and Search Space</h3>
<p>We begin by constructing a support vector machine from the <a href="https://cran.r-project.org/package=e1071"><code>e1071</code></a> with a radial kernel and specify we want to tune this using <code>"C-classification"</code> (the alternative is <code>"nu-classification"</code>, which has the same underlying algorithm but with a <code>nu</code> parameter to tune over [0,1] instead of <code>cost</code> over [0, <span class="math inline">\(\infty\)</span>)).</p>
<div class="cell" data-hash="optimization_cache/html/optimization-003_15ca828d735a6b380fe02361c8b4110c">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>, <span class="at">type =</span> <span class="st">"C-classification"</span>, <span class="at">kernel =</span> <span class="st">"radial"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Learner hyperparameter information is stored in the <code>$param_set</code> field, including parameter name, class (e.g., discrete or numeric), levels it can be tuned over, tuning limits, and more.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-004_38d422d7ae9cc3e7b304199c93e98253">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">as.data.table</span>(learner<span class="sc">$</span>param_set)[, <span class="fu">list</span>(id, class, lower, upper, nlevels)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 id    class lower upper nlevels
 1:       cachesize ParamDbl  -Inf   Inf     Inf
 2:   class.weights ParamUty    NA    NA     Inf
 3:           coef0 ParamDbl  -Inf   Inf     Inf
 4:            cost ParamDbl     0   Inf     Inf
 5:           cross ParamInt     0   Inf     Inf
 6: decision.values ParamLgl    NA    NA       2
 7:          degree ParamInt     1   Inf     Inf
 8:         epsilon ParamDbl     0   Inf     Inf
 9:          fitted ParamLgl    NA    NA       2
10:           gamma ParamDbl     0   Inf     Inf
11:          kernel ParamFct    NA    NA       4
12:              nu ParamDbl  -Inf   Inf     Inf
13:           scale ParamUty    NA    NA     Inf
14:       shrinking ParamLgl    NA    NA       2
15:       tolerance ParamDbl     0   Inf     Inf
16:            type ParamFct    NA    NA       2</code></pre>
</div>
</div>
<p>Note that <code>$param_set</code> also displays non-tunable parameters. Detailed information about parameters can be found in the help pages of the underlying implementation, for this example see <a href="https://www.rdocumentation.org/packages/e1071/topics/svm"><code>e1071::svm()</code></a>.</p>
<div class="page-columns page-full"><p>Given infinite resources, we could tune every single hyperparameter, but in reality that is not possible so instead only a subset of hyperparameters can be tuned. This subset is referred to as the search space or tuning space. In this example we will tune the regularization and influence hyperparameters, <code>cost</code> and <code>gamma</code>.</p><div class="no-row-height column-margin column-container"><span class="">Search Space</span></div></div>
<p>For numeric hyperparameters (we will explore others later) one must specify the bounds to tune over. We do this by constructing a learner and using <a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>to_tune()</code></a> to set the lower and upper limits for the parameters we want to tune. This function allows us to construct a learner in the usual way but to leave the hyperparameters of interest to be unspecified within a set range. This is best demonstrated by example:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-005_679eae4fe3e6a46655e8ab52decf9291">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb6-2"><a href="#cb6-2"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb6-3"><a href="#cb6-3"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb6-4"><a href="#cb6-4"></a>  <span class="at">type  =</span> <span class="st">"C-classification"</span>,</span>
<span id="cb6-5"><a href="#cb6-5"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>learner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifSVM:classif.svm&gt;
* Model: -
* Parameters: cost=&lt;RangeTuneToken&gt;, gamma=&lt;RangeTuneToken&gt;,
  type=C-classification, kernel=radial
* Packages: mlr3, mlr3learners, e1071
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric
* Properties: multiclass, twoclass</code></pre>
</div>
</div>
<p>Here we have constructed a classification SVM by setting the type to “C-classification”, the kernel to “radial”, and not fully specifying the <code>cost</code> and <code>gamma</code> hyperparameters but instead indicating that we will tune these parameters.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>cost</code> and <code>gamma</code> hyperparameters are usually tuned on the logarithmic scale. You can find out more in <a href="#sec-advanced-search-spaces"><span>Section&nbsp;4.2.2</span></a>.</p>
</div>
</div>
<p>Search spaces are usually chosen by experience. In some cases these can be quite complex, <a href="technical.html#sec-paradox"><span>Section&nbsp;9.4</span></a> describes how to construct these. <a href="#sec-tuning-spaces"><span>Section&nbsp;4.2.3</span></a> introduces the <a href="https://mlr3tuningspaces.mlr-org.com"><code>mlr3tuningspaces</code></a> extension package which allows loading of search spaces that have been established in published scientific articles.</p>
</section><section id="sec-terminator" class="level3 page-columns page-full" data-number="4.1.2"><h3 data-number="4.1.2" class="anchored" data-anchor-id="sec-terminator">
<span class="header-section-number">4.1.2</span> Terminator</h3>
<div class="page-columns page-full"><p>Theoretically, a tuner could search an entire search space exhaustively, however practically this is not possible and mathematically this is impossible for continuous hyperparameters. Therefore a core part of configuring tuning is to specify when to terminate the algorithm, this is also known as specifying the tuning budget. <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> includes many methods to specify when to terminate an algorithm, which are known as <a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminators</code></a>. Available terminators are listed in <a href="#tbl-terms">Table&nbsp;<span>4.1</span></a>.</p><div class="no-row-height column-margin column-container"><span class="">Tuning Budget</span><span class="">Terminators</span></div></div>
<div id="tbl-terms" class="anchored">
<table class="table">
<caption>Table&nbsp;4.1: Terminators available in <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>, their function call and default parameters.</caption>
<thead><tr class="header">
<th>Terminator</th>
<th>Function call and default parameters</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Number of Evaluations</td>
<td><code>trm("evals", n_evals = 500)</code></td>
</tr>
<tr class="even">
<td>Run Time</td>
<td><code>trm("run_time", secs = 100)</code></td>
</tr>
<tr class="odd">
<td>Performance Level</td>
<td><code>trm("perf_reached", level = 0.1)</code></td>
</tr>
<tr class="even">
<td>Stagnation</td>
<td><code>trm("stagnation", iters = 5, threshold = 1e-5)</code></td>
</tr>
<tr class="odd">
<td>None</td>
<td><code>trm("none")</code></td>
</tr>
<tr class="even">
<td>Clock Time</td>
<td><code>trm("clock_time", stop_time = "2022-11-06 08:42:53 CET"</code></td>
</tr>
<tr class="odd">
<td>Combo</td>
<td><code>trm("combo", terminators = list(run_time_100, evals_200)</code></td>
</tr>
</tbody>
</table>
</div>
<p>The most commonly used terminators are those that stop the tuning after a certain time (<code>"run_time"</code>) or the number of evaluations (<code>"evals"</code>). Choosing a runtime is often based on practical considerations and intuition. Using a time limit can be important on clusters so that the tuning is finished before the account budget is exhausted. The <code>"perf_reached"</code> terminator stops the tuning when a certain performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model. However, one needs to be careful using this terminator as if the level is set too optimistically, the tuning might never terminate. The <code>"stagnation"</code> terminator stops when no progress is made in a certain amount of iterations. Note, this could result in the optimization being terminated too early if the search space is too complex. We use <code>"none"</code> when tuners, such as Grid Search and Hyperband, control the termination themselves. Terminators can be freely combined with the <code>"combo"</code> terminator, this is explored in the exercises at the end of this chapter. A complete and always up-to-date list of terminators can be found on our website at <a href="https://mlr-org.com/terminators.html">https://mlr-org.com/terminators.html</a>.</p>
</section><section id="sec-tuning-instance" class="level3 page-columns page-full" data-number="4.1.3"><h3 data-number="4.1.3" class="anchored" data-anchor-id="sec-tuning-instance">
<span class="header-section-number">4.1.3</span> Tuning Instance with <code>ti</code>
</h3>
<div class="page-columns page-full"><p>A tuning instance can be constructed manually (<a href="#sec-tuning-instance"><span>Section&nbsp;4.1.3</span></a>) with the <a href="https://mlr3tuning.mlr-org.com/reference/ti.html"><code>ti()</code></a> function or automated (<a href="#sec-simplified-tuning"><span>Section&nbsp;4.1.6</span></a>) with the <a href="https://mlr3tuning.mlr-org.com/reference/tune.html"><code>tune()</code></a> function. We cover the manual approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>. The <a href="https://mlr3tuning.mlr-org.com/reference/ti.html"><code>ti</code></a> function constructs a tuning instance which collects together the information required to optimise a model.</p><div class="no-row-height column-margin column-container"><span class="">Tuning Instance</span></div></div>
<p>Now continuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the <a href="https://mlr3tuning.mlr-org.com/reference/ti.html"><code>ti()</code></a> function to create a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> (note: supplying two measures to <code>ti()</code> would result in <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html"><code>TuningInstanceMultiCrit</code></a> (<a href="#sec-multi-metrics-tuning"><span>Section&nbsp;4.3</span></a>)). For this example we will use three-fold resampling and will optimise the classification error measure. Note that we use <code>trm("none")</code> as we are using an exhaustive grid search.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-007_eb25f0728a42af85eba72c1b7e621122">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a>measure <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb8-6"><a href="#cb8-6"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb8-7"><a href="#cb8-7"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb8-8"><a href="#cb8-8"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb8-9"><a href="#cb8-9"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>)</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb8-13"><a href="#cb8-13"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb8-14"><a href="#cb8-14"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb8-15"><a href="#cb8-15"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb8-16"><a href="#cb8-16"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb8-17"><a href="#cb8-17"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb8-18"><a href="#cb8-18"></a>)</span>
<span id="cb8-19"><a href="#cb8-19"></a>instance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningInstanceSingleCrit&gt;
* State:  Not optimized
* Objective: &lt;ObjectiveTuning:classif.svm_on_sonar&gt;
* Search Space:
      id    class lower upper nlevels
1:  cost ParamDbl 1e-05 1e+05     Inf
2: gamma ParamDbl 1e-05 1e+05     Inf
* Terminator: &lt;TerminatorNone&gt;</code></pre>
</div>
</div>
</section><section id="sec-tuner" class="level3 page-columns page-full" data-number="4.1.4"><h3 data-number="4.1.4" class="anchored" data-anchor-id="sec-tuner">
<span class="header-section-number">4.1.4</span> Tuner</h3>
<div class="page-columns page-full"><p>After we created the tuning problem, we can look at <em>how</em> to tune. There are multiple <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuners</code></a> in <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>, which implement different HPO algorithms.</p><div class="no-row-height column-margin column-container"><span class="">Tuners</span></div></div>
<div id="tbl-tuners" class="anchored">
<table class="table">
<caption>Table&nbsp;4.2: Tuning algorithms available in <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>, their function call and the methodology.</caption>
<thead><tr class="header">
<th>Tuner</th>
<th>Function call</th>
<th>Method</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Random Search</td>
<td><code>tnr("random_search")</code></td>
<td>Samples configurations from a uniform distribution randomly <span class="citation" data-cites="bergstra2012">(<a href="references.html#ref-bergstra2012" role="doc-biblioref">Bergstra and Bengio 2012</a>)</span>.</td>
</tr>
<tr class="even">
<td>Grid Search</td>
<td><code>tnr("grid_search")</code></td>
<td>Discretizes the range of each configuration and exhaustively evaluates each combination.</td>
</tr>
<tr class="odd">
<td>Iterative Racing</td>
<td><code>tnr("irace")</code></td>
<td>Races down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space <span class="citation" data-cites="lopez2016">(<a href="references.html#ref-lopez2016" role="doc-biblioref">López-Ibáñez et al. 2016</a>)</span>.</td>
</tr>
<tr class="even">
<td>Bayesian Optimization</td>
<td><code>tnr("mbo")</code></td>
<td>Iterative algorithms that make use of a continuously updated surrogate model built for the objective function. By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency.</td>
</tr>
<tr class="odd">
<td>Hyperband</td>
<td><code>tnr("hyperband")</code></td>
<td>Multi-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping <span class="citation" data-cites="li2017">(<a href="references.html#ref-li2017" role="doc-biblioref">Li et al. 2017</a>)</span>.</td>
</tr>
<tr class="even">
<td>Covariance Matrix Adaptation Evolution Strategy</td>
<td><code>tnr("cmaes")</code></td>
<td>Evolution strategy algorithm with sampling from a multivariate Gaussian who is updated with the success of the previous population <span class="citation" data-cites="hansen2011">(<a href="references.html#ref-hansen2011" role="doc-biblioref">Hansen and Auger 2011</a>)</span>.</td>
</tr>
<tr class="odd">
<td>Generalized Simulated Annealing</td>
<td><code>tnr("gensa")</code></td>
<td>Probabilistic algorithm for numeric search spaces <span class="citation" data-cites="xiang2013 tsallis1996">(<a href="references.html#ref-xiang2013" role="doc-biblioref">Xiang et al. 2013</a>; <a href="references.html#ref-tsallis1996" role="doc-biblioref">Tsallis and Stariolo 1996</a>)</span>.</td>
</tr>
<tr class="even">
<td>Nonlinear Optimization</td>
<td><code>tnr("nloptr")</code></td>
<td>Several nonlinear optimization algorithms for numeric search spaces.</td>
</tr>
</tbody>
</table>
</div>
<p>When selecting algorithms, grid search and random search are the most basic and are often selected first in initial experiments. They are ‘naive’ algorithms in that they try new configurations whilst ignoring performance from previous attempts. In contrast, more advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly. Some advanced algorithms are included in extension packages, for example the package <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> implements Bayesian optimization (also called Model-Based Optimization), and <a href="https://mlr3hyperband.mlr-org.com"><code>mlr3hyperband</code></a> implements algorithms of the hyperband family. A complete and up-to-date list of tuners can be found on the <a href="https://mlr-org.com/tuners.html">website</a>.</p>
<p>For our SVM example, we will use a simple grid search with a resolution of 5, which is the distinct values to try <em>per hyperparameter</em>. For example for a search space <span class="math inline">\(\{1, 2, 3, 4, 5, 6\}\)</span> then a grid search with resolution 3 would pick three values evenly apart in this search space, i.e., <span class="math inline">\(\{2, 4, 6\}\)</span>. The <code>batch_size</code> controls how many configurations are evaluated at the same time (see <a href="technical.html#sec-parallelization"><span>Section&nbsp;9.1</span></a>).</p>
<div class="cell" data-hash="optimization_cache/html/optimization-008_eacb123472b1744bcd2a9cdeefa92ddd">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a>tuner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TunerGridSearch&gt;: Grid Search
* Parameters: resolution=5, batch_size=5
* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct
* Properties: dependencies, single-crit, multi-crit
* Packages: mlr3tuning</code></pre>
</div>
</div>
<p>In our example we are tuning over two numeric parameters, <a href="https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html"><code>TunerGridSearch</code></a> will create an equidistant grid between the respective upper and lower bounds. This means our two-dimensional grid of resolution 5 consists of <span class="math inline">\(5^2 = 25\)</span> configurations. Each configuration is a distinct set of hyperparameter values that is used to construct a model from the chosen learner, which is fit to the chosen task (<a href="#fig-optimization-loop">Figure&nbsp;<span>4.2</span></a>).</p>
<p>All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (<a href="#sec-terminator"><span>Section&nbsp;4.1.2</span></a>) signals that the budget is exhausted.</p>
<div class="page-columns page-full"><p>Just like learners, tuners also have parameters, known as control parameters, which (as the name suggests) controls the behavior of the tuners. Unlike learners, default values for control parameters usually give good results and these rarely need to be changed. Control parameters are stored in the <code>$param_set</code> field.</p><div class="no-row-height column-margin column-container"><span class="">Control Parameters</span></div></div>
<div class="cell" data-hash="optimization_cache/html/optimization-009_17d81b2d4f2595a18e69940bd8016079">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>tuner<span class="sc">$</span>param_set</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ParamSet&gt;
                  id    class lower upper nlevels        default value
1:        batch_size ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;     5
2:        resolution ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;     5
3: param_resolutions ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;      </code></pre>
</div>
</div>
</section><section id="trigger-the-tuning" class="level3" data-number="4.1.5"><h3 data-number="4.1.5" class="anchored" data-anchor-id="trigger-the-tuning">
<span class="header-section-number">4.1.5</span> Trigger the Tuning</h3>
<p>Now we have all our components, we are ready to start tuning! To do this we simply pass the constructed <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> to the <code>$optimize()</code> method of the initialized <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a>. The tuner then proceeds with the HPO loop we discussed at the beginning of the chapter (<a href="#fig-optimization-loop">Figure&nbsp;<span>4.2</span></a>).</p>
<div class="cell" data-hash="optimization_cache/html/optimization-010_8018cbcce7cb2ca61ffb1c745f8caa09">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    cost gamma learner_param_vals  x_domain classif.ce
1: 25000 1e-05          &lt;list[4]&gt; &lt;list[2]&gt;  0.2358178</code></pre>
</div>
</div>
<p>The optimizer returns the best hyperparameter configuration and the corresponding measured performance. This information is also stored in <code>instance$result</code>.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The column <code>x_domain</code> contains transformed values and <code>learner_param_vals</code> optional constants (none in this example). See section <a href="#sec-advanced-search-spaces"><span>Section&nbsp;4.2.2</span></a> for more information.</p>
</div>
</div>
</section><section id="sec-simplified-tuning" class="level3" data-number="4.1.6"><h3 data-number="4.1.6" class="anchored" data-anchor-id="sec-simplified-tuning">
<span class="header-section-number">4.1.6</span> Quick Tuning with <code>tune</code>
</h3>
<p>In the previous section, we looked at creating a tuning instance manually using <a href="https://mlr3tuning.mlr-org.com/reference/ti.html"><code>ti()</code></a>, which offers more control over the tuning process. However, you can also simplify this (albeit with slightly less control) using the <a href="https://mlr3tuning.mlr-org.com/reference/tune.html"><code>tune()</code></a> sugar function. Internally this creates a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a>, starts the tuning and returns the result with the instance.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-012_67e03e2ad32cd9c6303435cca28fb04c">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb16-2"><a href="#cb16-2"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb16-3"><a href="#cb16-3"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb16-4"><a href="#cb16-4"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb16-5"><a href="#cb16-5"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb16-6"><a href="#cb16-6"></a>)</span>
<span id="cb16-7"><a href="#cb16-7"></a></span>
<span id="cb16-8"><a href="#cb16-8"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb16-9"><a href="#cb16-9"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb16-10"><a href="#cb16-10"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb16-11"><a href="#cb16-11"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb16-12"><a href="#cb16-12"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb16-13"><a href="#cb16-13"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb16-14"><a href="#cb16-14"></a>)</span>
<span id="cb16-15"><a href="#cb16-15"></a></span>
<span id="cb16-16"><a href="#cb16-16"></a>instance<span class="sc">$</span>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    cost gamma learner_param_vals  x_domain classif.ce
1: 25000 1e-05          &lt;list[4]&gt; &lt;list[2]&gt;  0.2309179</code></pre>
</div>
</div>
</section><section id="sec-analyzing-result" class="level3" data-number="4.1.7"><h3 data-number="4.1.7" class="anchored" data-anchor-id="sec-analyzing-result">
<span class="header-section-number">4.1.7</span> Analyzing the Result</h3>
<p>Whether you use <a href="https://mlr3tuning.mlr-org.com/reference/ti.html"><code>ti</code></a> or <a href="https://mlr3tuning.mlr-org.com/reference/tune.html"><code>tune</code></a> the output is the same and the ‘archive’ lists all evaluated hyperparameter configurations:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-013_301bc8a3bfc1ae3b2a499f60280935c3">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, <span class="fu">list</span>(cost, gamma, classif.ce)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost   gamma classif.ce
 1: 5.0e+04 5.0e+04  0.4661836
 2: 5.0e+04 1.0e+05  0.4661836
 3: 7.5e+04 7.5e+04  0.4661836
 4: 1.0e+05 1.0e-05  0.2499655
 5: 1.0e+05 7.5e+04  0.4661836
---                           
21: 1.0e-05 7.5e+04  0.4661836
22: 1.0e-05 1.0e+05  0.4661836
23: 2.5e+04 2.5e+04  0.4661836
24: 1.0e+05 2.5e+04  0.4661836
25: 1.0e+05 5.0e+04  0.4661836</code></pre>
</div>
</div>
<p>Each row of the archive is a different evaluated configuration (there are 25 rows in total in the full <code>data.table</code>). The columns here show the tested configurations, the measure we optimize, the completed configuration time stamp, and the total train and predict times. If we only specify a single-objective criterium then the instance will return the configuration that optimizes this measure however we can manually inspect the archive to determine other important features. For example, how long did the model take to run? Were there any errors in running?</p>
<div class="cell" data-hash="optimization_cache/html/optimization-014_9638fdf081e72ce830d4ef959131f330">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[,</span>
<span id="cb20-2"><a href="#cb20-2"></a>  <span class="fu">list</span>(timestamp, runtime_learners, errors, warnings)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              timestamp runtime_learners errors warnings
 1: 2023-02-27 19:32:38            0.059      0        0
 2: 2023-02-27 19:32:38            0.063      0        0
 3: 2023-02-27 19:32:38            0.069      0        0
 4: 2023-02-27 19:32:38            0.052      0        0
 5: 2023-02-27 19:32:38            0.067      0        0
---                                                     
21: 2023-02-27 19:32:41            0.060      0        0
22: 2023-02-27 19:32:41            0.066      0        0
23: 2023-02-27 19:32:41            0.059      0        0
24: 2023-02-27 19:32:41            0.067      0        0
25: 2023-02-27 19:32:41            0.059      0        0</code></pre>
</div>
</div>
<p>Now we see not only was our optimal configuration the best performing with respect to classification error, but also it had the fastest runtime.</p>
<p>Another powerful feature of the instance is that we can score the internal <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a>s on a different performance measure, for example looking at false negative rate (FNR) and false positive rate (FPR) as well as classification error:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-015_5df0e0af0e38b3aaa4dbf4b99aa3ff79">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive,</span>
<span id="cb22-2"><a href="#cb22-2"></a>  <span class="at">measures =</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.fpr"</span>, <span class="st">"classif.fnr"</span>)))[,</span>
<span id="cb22-3"><a href="#cb22-3"></a>  <span class="fu">list</span>(cost, gamma, classif.ce, classif.fpr, classif.fnr)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost   gamma classif.ce classif.fpr classif.fnr
 1: 5.0e+04 5.0e+04  0.4661836   1.0000000    0.000000
 2: 5.0e+04 1.0e+05  0.4661836   1.0000000    0.000000
 3: 7.5e+04 7.5e+04  0.4661836   1.0000000    0.000000
 4: 1.0e+05 1.0e-05  0.2499655   0.2628968    0.232703
 5: 1.0e+05 7.5e+04  0.4661836   1.0000000    0.000000
---                                                   
21: 1.0e-05 7.5e+04  0.4661836   1.0000000    0.000000
22: 1.0e-05 1.0e+05  0.4661836   1.0000000    0.000000
23: 2.5e+04 2.5e+04  0.4661836   1.0000000    0.000000
24: 1.0e+05 2.5e+04  0.4661836   1.0000000    0.000000
25: 1.0e+05 5.0e+04  0.4661836   1.0000000    0.000000</code></pre>
</div>
</div>
<p>Now we see our model is also the best performing with respect to FPR and FNR!</p>
<p>You can view all the resamplings in a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> object with <code>instance$archive$benchmark_result</code>.</p>
<p>Finally, for more visually appealing results you can use <a href="https://mlr3viz.mlr-org.com"><code>mlr3viz</code></a> (<a href="#fig-surface">Figure&nbsp;<span>4.3</span></a>).</p>
<div class="cell" data-hash="optimization_cache/html/fig-surface_67e6287412c9d0b642bd3c2ab9e71c07">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="fu">autoplot</span>(instance, <span class="at">type =</span> <span class="st">"surface"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-surface" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="optimization_files/figure-html/fig-surface-1.png" class="img-fluid figure-img" alt="Heatmap showing model performance during HPO. y-axis is 'gamma' parameter between (-10,10) and x-axis is 'cost' parameter between (-10,10). The heatmap shows squares covering all points on the plot and circular points indicating configurations tried in our optimisation. The top-left quadrant is all yellow indicating poor performance when gamma is high and cost is low. The bottom-right is dark blue indicating good performance when cost is high and gamme is low." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.3: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high <code>cost</code> values and <code>gamma</code> values around <code>exp(-5)</code> achieve the best performance.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-final-model" class="level3" data-number="4.1.8"><h3 data-number="4.1.8" class="anchored" data-anchor-id="sec-final-model">
<span class="header-section-number">4.1.8</span> Using a tuned model</h3>
<p>Once the learner has been tuned we can start to use it like any other model in the mlr3 universe. To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters with the optimal configurations:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-018_e6070ff75cf67f84637a6bfdfd4269bc">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a>svm_tuned <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>, <span class="at">id =</span> <span class="st">"SVM Tuned"</span>)</span>
<span id="cb25-2"><a href="#cb25-2"></a>svm_tuned<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> instance<span class="sc">$</span>result_learner_param_vals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can train the learner on the full dataset and we are ready to make predictions. The trained model can then be used to predict new, external data:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-019_55a8485dafc31bb24be4d1312a700d7f">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a>svm_tuned<span class="sc">$</span><span class="fu">train</span>(<span class="fu">tsk</span>(<span class="st">"sonar"</span>))</span>
<span id="cb26-2"><a href="#cb26-2"></a>svm_tuned<span class="sc">$</span>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
svm.default(x = data, y = task$truth(), type = "C-classification", 
    kernel = "radial", gamma = 1e-05, cost = 25000.0000075, probability = (self$predict_type == 
        "prob"))


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  25000 

Number of Support Vectors:  89</code></pre>
</div>
</div>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>A common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (<code>instance$result$classif.ce</code>) as the model’s performance. However, doing so would lead to bias and therefore nested resampling is required (<a href="#sec-nested-resampling"><span>Section&nbsp;4.5</span></a>). Therefore when tuning as above ensure that you do not make any statements about model performance without testing the model on more unseen data. We will come back to this in more detail in <a href="#sec-autotuner"><span>Section&nbsp;4.4</span></a>.</p>
</div>
</div>
</section></section><section id="advanced-tuning" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="advanced-tuning">
<span class="header-section-number">4.2</span> Advanced Tuning</h2>
<section id="sec-encapsulation-fallback" class="level3" data-number="4.2.1"><h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-encapsulation-fallback">
<span class="header-section-number">4.2.1</span> Encapsulation and Fallback Learner</h3>
<p>So far, we have only looked at the case where no issues occur. However, it often happens that learners with certain configurations do not converge, run out of memory, or terminate with an error. We can protect the tuning process from failing learners with encapsulation. The encapsulation separates the tuning from the training of the individual learner. The encapsulation method is set in the learner.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-020_5681a111266d5cfd53a73f7f3074e864">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a>learner<span class="sc">$</span>encapsulate <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"evaluate"</span>, <span class="at">predict =</span> <span class="st">"evaluate"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The encapsulation can be set individually for training and predicting. There are currently two options for encapsulating a learner. The <a href="https://cran.r-project.org/package=evaluate"><code>evaluate</code></a> package and the <a href="https://cran.r-project.org/package=callr"><code>callr</code></a> package. The <a href="https://cran.r-project.org/package=callr"><code>callr</code></a> package comes with more overhead because the encapsulation spawns a separate R process. Both packages allow setting a timeout which is useful when a learner does not converge. We set a timeout of 30 seconds.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-021_be6e7340fc50d86e9cee21485819283b">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a>learner<span class="sc">$</span>timeout <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="dv">30</span>, <span class="at">predict =</span> <span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With encapsulation, exceptions and timeouts do not stop the tuning. Instead, the error message is recorded and a fallback learner is fitted.</p>
<p>Fallback learners allow scoring a result when no model was fitted during training. A common approach is to predict a weak baseline e.g.&nbsp;predicting the mean of the data or just the majority class. See <span class="quarto-unresolved-ref">?sec-fallback-learner</span> for more detailed information.</p>
<p>The <a href="https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html"><code>featureless learner</code></a> predicts the most frequent label.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-022_94bf2c575d2c1da3d3f840961c6351cf">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a>learner<span class="sc">$</span>fallback <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.featureless"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Errors and warnings that occurred during tuning are stored in the archive.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-023_6714ae8ec80994d71732270022e89960">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, <span class="fu">list</span>(cost, gamma, classif.ce, errors, warnings)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost   gamma classif.ce errors warnings
 1: 5.0e+04 5.0e+04  0.4661836      0        0
 2: 5.0e+04 1.0e+05  0.4661836      0        0
 3: 7.5e+04 7.5e+04  0.4661836      0        0
 4: 1.0e+05 1.0e-05  0.2499655      0        0
 5: 1.0e+05 7.5e+04  0.4661836      0        0
---                                           
21: 1.0e-05 7.5e+04  0.4661836      0        0
22: 1.0e-05 1.0e+05  0.4661836      0        0
23: 2.5e+04 2.5e+04  0.4661836      0        0
24: 1.0e+05 2.5e+04  0.4661836      0        0
25: 1.0e+05 5.0e+04  0.4661836      0        0</code></pre>
</div>
</div>
</section><section id="sec-advanced-search-spaces" class="level3" data-number="4.2.2"><h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-advanced-search-spaces">
<span class="header-section-number">4.2.2</span> Advanced Search Spaces</h3>
<p>Usually, the <code>cost</code> and <code>gamma</code> hyperparameters are tuned on the logarithmic scale which means the optimization algorithm searches in <span class="math inline">\([log(1e-5), log(1e5)]\)</span> but transforms the selected configuration with <code><a href="https://rdrr.io/r/base/Log.html">exp()</a></code> before passing to the learner. Using the log transformation emphasizes smaller values but can also result in large values. The code below demonstrates this more clearly. The histograms show how the algorithm searches within a narrow range but exponentiating then results in the majority of points being relatively small but a few being very large.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-005gen_86781b59754211e76fb76357233a6340">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a>cost <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="fu">log</span>(<span class="fl">1e-5</span>), <span class="fu">log</span>(<span class="fl">1e5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fig-logscale" class="cell quarto-layout-panel" data-hash="optimization_cache/html/fig-logscale_306b7ad918e252841ff1a62c286555de">
<figure class="figure"><div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-logscale-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="optimization_files/figure-html/fig-logscale-1.png" class="img-fluid figure-img" data-ref-parent="fig-logscale" width="672"></p>
<p></p><figcaption class="figure-caption">(a) <code>cost</code> values sampled by the optimization algorithm.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-logscale-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="optimization_files/figure-html/fig-logscale-2.png" class="img-fluid figure-img" data-ref-parent="fig-logscale" width="672"></p>
<p></p><figcaption class="figure-caption">(b) <code>exp(cost)</code> values seen by the learner.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.4: Histogram of sampled <code>cost</code> values.</figcaption><p></p>
</figure>
</div>
<p>To add the <code><a href="https://rdrr.io/r/base/Log.html">exp()</a></code> transformation to a hyperparameter, we pass <code>logscale = TRUE</code> to <code>to_tune()</code>.</p>
<div class="cell" data-hash="optimization_cache/html/unnamed-chunk-6_2658d357941e45d564f21121a8da9461">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb34-2"><a href="#cb34-2"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb34-3"><a href="#cb34-3"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb34-4"><a href="#cb34-4"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb34-5"><a href="#cb34-5"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb34-6"><a href="#cb34-6"></a>)</span>
<span id="cb34-7"><a href="#cb34-7"></a></span>
<span id="cb34-8"><a href="#cb34-8"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb34-9"><a href="#cb34-9"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb34-10"><a href="#cb34-10"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb34-11"><a href="#cb34-11"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb34-12"><a href="#cb34-12"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb34-13"><a href="#cb34-13"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb34-14"><a href="#cb34-14"></a>)</span>
<span id="cb34-15"><a href="#cb34-15"></a></span>
<span id="cb34-16"><a href="#cb34-16"></a>instance<span class="sc">$</span>result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost     gamma learner_param_vals  x_domain classif.ce
1: 5.756463 -5.756463          &lt;list[4]&gt; &lt;list[2]&gt;  0.2014493</code></pre>
</div>
</div>
<p>The column <code>x_domain</code> contains the hyperparameter values after the transformation i.e.&nbsp;<code>exp(5.76)</code> and <code>exp(-5.76)</code>:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-011_4cd44b26fc6b3f085946f1691c6f9da5">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a>instance<span class="sc">$</span>result<span class="sc">$</span>x_domain</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[[1]]$cost
[1] 316.2278

[[1]]$gamma
[1] 0.003162278</code></pre>
</div>
</div>
</section><section id="sec-tuning-spaces" class="level3" data-number="4.2.3"><h3 data-number="4.2.3" class="anchored" data-anchor-id="sec-tuning-spaces">
<span class="header-section-number">4.2.3</span> Search Spaces Collection</h3>
<p>Selected search spaces can require a lot of background knowledge or expertise. The package <a href="https://mlr3tuningspaces.mlr-org.com"><code>mlr3tuningspaces</code></a> tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms. These search spaces should be applicable to a wide range of data sets, however, they may need to be adapted in specific situations. The search spaces are stored in the dictionary <a href="https://mlr3tuningspaces.mlr-org.com/reference/mlr_tuning_spaces.html"><code>mlr_tuning_spaces</code></a>.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-024_91b50c89fee3090c16aed51c916257dc">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="fu">as.data.table</span>(mlr_tuning_spaces)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                       key                              label        learner
 1: classif.glmnet.default    Classification GLM with Default classif.glmnet
 2:    classif.glmnet.rbv2  Classification GLM with RandomBot classif.glmnet
 3:   classif.kknn.default   Classification KKNN with Default   classif.kknn
 4:      classif.kknn.rbv2 Classification KKNN with RandomBot   classif.kknn
 5: classif.ranger.default Classification Ranger with Default classif.ranger
---                                                                         
20:        regr.rpart.rbv2    Regression Rpart with RandomBot     regr.rpart
21:       regr.svm.default        Regression SVM with Default       regr.svm
22:          regr.svm.rbv2      Regression SVM with RandomBot       regr.svm
23:   regr.xgboost.default    Regression XGBoost with Default   regr.xgboost
24:      regr.xgboost.rbv2  Regression XGBoost with RandomBot   regr.xgboost
1 variable not shown: [n_values]</code></pre>
</div>
</div>
<p>The tuning spaces are named according to the scheme <code>{learner-id}.{publication}</code>. The sugar function <a href="https://mlr3tuningspaces.mlr-org.com/reference/lts.html"><code>lts()</code></a> is used to retrieve a <a href="https://mlr3tuningspaces.mlr-org.com/reference/TuningSpace.html"><code>TuningSpace</code></a>.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-025_7018e5bf7e31bfe65f4be6786b63faba">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="fu">lts</span>(<span class="st">"classif.rpart.default"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningSpace:classif.rpart.default&gt;: Classification Rpart with Default
          id lower upper levels logscale
1:  minsplit 2e+00 128.0            TRUE
2: minbucket 1e+00  64.0            TRUE
3:        cp 1e-04   0.1            TRUE</code></pre>
</div>
</div>
<p>A tuning space can be passed to <code>ti()</code> as the <code>search_space</code>.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-026_df4d5609f4c573bc8a5c70df04925589">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb42-2"><a href="#cb42-2"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb42-3"><a href="#cb42-3"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb42-4"><a href="#cb42-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb42-5"><a href="#cb42-5"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb42-6"><a href="#cb42-6"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>),</span>
<span id="cb42-7"><a href="#cb42-7"></a>  <span class="at">search_space =</span> <span class="fu">lts</span>(<span class="st">"classif.rpart.rbv2"</span>)</span>
<span id="cb42-8"><a href="#cb42-8"></a>)</span>
<span id="cb42-9"><a href="#cb42-9"></a>instance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningInstanceSingleCrit&gt;
* State:  Not optimized
* Objective: &lt;ObjectiveTuning:classif.rpart_on_sonar&gt;
* Search Space:
          id    class    lower upper nlevels
1:        cp ParamDbl -9.21034     0     Inf
2:  maxdepth ParamInt  1.00000    30      30
3: minbucket ParamInt  1.00000   100     100
4:  minsplit ParamInt  1.00000   100     100
* Terminator: &lt;TerminatorEvals&gt;</code></pre>
</div>
</div>
<p>Alternatively, we can explicitly set the search space of a learner with <a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>TuneTokens</code></a></p>
<div class="cell" data-hash="optimization_cache/html/optimization-027_f9d6b28f023c08ed38b4162f74e256b5">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a>vals <span class="ot">=</span> <span class="fu">lts</span>(<span class="st">"classif.rpart.default"</span>)<span class="sc">$</span>values</span>
<span id="cb44-2"><a href="#cb44-2"></a>vals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$minsplit
Tuning over:
range [2, 128] (log scale)


$minbucket
Tuning over:
range [1, 64] (log scale)


$cp
Tuning over:
range [1e-04, 0.1] (log scale)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb46-2"><a href="#cb46-2"></a>learner<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(<span class="at">.values =</span> vals)</span>
<span id="cb46-3"><a href="#cb46-3"></a>learner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree
* Model: -
* Parameters: xval=0, minsplit=&lt;RangeTuneToken&gt;,
  minbucket=&lt;RangeTuneToken&gt;, cp=&lt;RangeTuneToken&gt;
* Packages: mlr3, rpart
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric, factor, ordered
* Properties: importance, missings, multiclass, selected_features,
  twoclass, weights</code></pre>
</div>
</div>
<p>When passing a learner to <a href="https://mlr3tuningspaces.mlr-org.com/reference/lts.html"><code>lts()</code></a>, the default search space from the <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="references.html#ref-hpo_practical" role="doc-biblioref">2021</a>)</span> article is applied.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-028_478a7db7d7877f93626ddd6b0b9bae34">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1"></a><span class="fu">lts</span>(<span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree
* Model: -
* Parameters: xval=0, minsplit=&lt;RangeTuneToken&gt;,
  minbucket=&lt;RangeTuneToken&gt;, cp=&lt;RangeTuneToken&gt;
* Packages: mlr3, rpart
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric, factor, ordered
* Properties: importance, missings, multiclass, selected_features,
  twoclass, weights</code></pre>
</div>
</div>
<p>It is possible to simply overwrite a predefined tuning space in construction, for example here we change the range of the <code>nrounds</code> hyperparameter in XGBoost:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-029_c3c3d91c67f1cfbd18dc1d2be425b7af">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a><span class="fu">lts</span>(<span class="st">"classif.xgboost.rbv2"</span>, <span class="at">nrounds =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">1024</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningSpace:classif.xgboost.rbv2&gt;: Classification XGBoost with RandomBot
                   id lower upper               levels logscale
 1:           booster    NA    NA gblinear,gbtree,dart    FALSE
 2:           nrounds 1e+00  1024                         FALSE
 3:               eta 1e-04     1                          TRUE
 4:             gamma 1e-05     7                          TRUE
 5:            lambda 1e-04  1000                          TRUE
 6:             alpha 1e-04  1000                          TRUE
 7:         subsample 1e-01     1                         FALSE
 8:         max_depth 1e+00    15                         FALSE
 9:  min_child_weight 1e+00   100                          TRUE
10:  colsample_bytree 1e-02     1                         FALSE
11: colsample_bylevel 1e-02     1                         FALSE
12:         rate_drop 0e+00     1                         FALSE
13:         skip_drop 0e+00     1                         FALSE</code></pre>
</div>
</div>
</section></section><section id="sec-multi-metrics-tuning" class="level2 page-columns page-full" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="sec-multi-metrics-tuning">
<span class="header-section-number">4.3</span> Multi-Objective Tuning</h2>
<div class="page-columns page-full"><p>So far we have considered optimizing a model with respect to one metric but multi-metric, or multi-objective optimization is also possible. A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive and false negative predictions. In a more complex example, consider the problem of deploying a classifier in a healthcare setting, there is clearly an ethical argument to tune the model to make the best possible predictions, however in machine learning this can often lead to models that are harder to interpret (think about deep neural networks!). In this case, we may be interested in minimizing <em>both</em> classification error (for example) and complexity.</p><div class="no-row-height column-margin column-container"><span class="">Multi-objective Optimization</span></div></div>
<div class="page-columns page-full"><p>In general, when optimizing multiple metrics, these will be in competition (if they were not we would only need to optimize with respect to one of them!) and so no single configuration exists that optimizes all metrics. Focus is therefore given to the concept of Pareto optimality. One hyperparameter configuration is said to Pareto-dominate another one if the resulting model is equal or better in all metrics and strictly better in at least one metric. All configurations that are not Pareto-dominated are referred to as Pareto efficient and the set of all these configurations is referred to as the Pareto front (<a href="#fig-pareto">Figure&nbsp;<span>4.5</span></a>).</p><div class="no-row-height column-margin column-container"><span class="">Pareto Front</span></div></div>
<p>The goal of multi-objective HPO is to approximate the true, unknown Pareto front. More methodological details on multi-objective HPO can be found in <span class="citation" data-cites="hpo_multi">Karl et al. (<a href="references.html#ref-hpo_multi" role="doc-biblioref">2022</a>)</span>.</p>
<p>We will now demonstrate multi-objective HPO by tuning a decision tree on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html"><code>Spam</code></a> data set with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables). We will tune</p>
<ul>
<li>The complexity hyperparameter <code>cp</code> that controls when the learner considers introducing another branch.</li>
<li>The <code>minsplit</code> hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.</li>
<li>The <code>maxdepth</code> hyperparameter that limits the depth of the tree.</li>
</ul>
<div class="cell" data-hash="optimization_cache/html/optimization-031_0f45c731b996065811b704cb509764dd">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>,</span>
<span id="cb52-2"><a href="#cb52-2"></a>  <span class="at">cp =</span> <span class="fu">to_tune</span>(<span class="fl">1e-04</span>, <span class="fl">1e-1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-3"><a href="#cb52-3"></a>  <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">128</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-4"><a href="#cb52-4"></a>  <span class="at">maxdepth =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb52-5"><a href="#cb52-5"></a>)</span>
<span id="cb52-6"><a href="#cb52-6"></a></span>
<span id="cb52-7"><a href="#cb52-7"></a>measures <span class="ot">=</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that as we tune with respect to multiple measures, the function <code>ti</code> creates a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html"><code>TuningInstanceMultiCrit</code></a> instead of a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a>.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-033_9c5637b83b6488fe56b2a300746f9181">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb53-2"><a href="#cb53-2"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"spam"</span>),</span>
<span id="cb53-3"><a href="#cb53-3"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb53-4"><a href="#cb53-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb53-5"><a href="#cb53-5"></a>  <span class="at">measures =</span> measures,</span>
<span id="cb53-6"><a href="#cb53-6"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>),</span>
<span id="cb53-7"><a href="#cb53-7"></a>  <span class="at">store_models =</span> <span class="cn">TRUE</span>  <span class="co"># required to inspect selected_features</span></span>
<span id="cb53-8"><a href="#cb53-8"></a>)</span>
<span id="cb53-9"><a href="#cb53-9"></a>instance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningInstanceMultiCrit&gt;
* State:  Not optimized
* Objective: &lt;ObjectiveTuning:classif.rpart_on_spam&gt;
* Search Space:
         id    class      lower     upper nlevels
1:       cp ParamDbl -9.2103404 -2.302585     Inf
2: minsplit ParamDbl  0.6931472  4.859812     Inf
3: maxdepth ParamInt  1.0000000 30.000000      30
* Terminator: &lt;TerminatorEvals&gt;</code></pre>
</div>
</div>
<p>As before we will then select and run a tuning algorithm, here we use random search:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-034_d500cad59fece816d8ccc0237f0962a4">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">20</span>)</span>
<span id="cb55-2"><a href="#cb55-2"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we inspect the best-performing configurations, i.e., the Pareto set. And then inspect the estimated Pareto set and visualize the estimated Pareto front:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-035_7712d7665dd5328b3e47ba65f5a0bee7">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()[, <span class="fu">list</span>(cp, minsplit, maxdepth, classif.ce, selected_features)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           cp minsplit maxdepth classif.ce selected_features
 1: -4.897655 3.338026       17 0.10302293          7.666667
 2: -7.051424 3.896918       12 0.10041637         12.333333
 3: -4.245193 1.541127        8 0.10780375          5.666667
 4: -2.774911 3.148014       30 0.16387611          2.333333
 5: -4.807622 1.987267       11 0.10432672          7.333333
 6: -4.853549 4.375677       19 0.10671726          6.000000
 7: -3.739909 1.772185       26 0.11193239          5.000000
 8: -4.937744 1.286328       29 0.10302293          7.666667
 9: -4.553218 2.750349       22 0.10563050          6.666667
10: -3.714380 2.555973       25 0.11193239          5.000000
11: -5.076189 2.735166       11 0.10085053          9.333333
12: -4.519566 4.470367       30 0.10845564          5.333333
13: -5.445322 1.663845       23 0.08715871         13.000000
14: -6.465093 1.536021       16 0.07759311         29.000000
15: -6.528747 2.155507        6 0.08585422         16.333333</code></pre>
</div>
</div>
<div class="cell" data-hash="optimization_cache/html/fig-pareto_08551420cfd08f9fe5afc2efbec6ebce">
<div class="cell-output-display">
<div id="fig-pareto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="optimization_files/figure-html/fig-pareto-1.png" class="img-fluid figure-img" alt="Scatter plot with selected_features on x-axis and classif.ce on y-axis. Black dots represent simulated tested configurations of selected_features vs. classif.ce and red dots and a red line along the bottom-left of the plot shows the Pareto front." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.5: Pareto front of selected features and classification error. Black dots represent tested configurations, each red dot individually represents a Pareto-optimal configuration and all red dots together represent the Pareto front.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-autotuner" class="level2" data-number="4.4"><h2 data-number="4.4" class="anchored" data-anchor-id="sec-autotuner">
<span class="header-section-number">4.4</span> Automated Tuning with <code>AutoTuner</code>
</h2>
<p>One of the most powerful classes in mlr3 is the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a>. The <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters – this allows transparent tuning of any learner, without the need to extract information on the best hyperparameter settings at the end. As the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> itself inherits from the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> base class, it can be used like any other learner!</p>
<p>Let us see this in practice. We will run the exact same example as above but this time using the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> for automated tuning:</p>
<div class="cell" data-hash="optimization_cache/html/optimization-038_e992d41f8096046de38f6389ee1771b1">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb58-2"><a href="#cb58-2"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb58-3"><a href="#cb58-3"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb58-4"><a href="#cb58-4"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb58-5"><a href="#cb58-5"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb58-6"><a href="#cb58-6"></a>)</span>
<span id="cb58-7"><a href="#cb58-7"></a></span>
<span id="cb58-8"><a href="#cb58-8"></a>at <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb58-9"><a href="#cb58-9"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb58-10"><a href="#cb58-10"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb58-11"><a href="#cb58-11"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb58-12"><a href="#cb58-12"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb58-13"><a href="#cb58-13"></a>)</span>
<span id="cb58-14"><a href="#cb58-14"></a></span>
<span id="cb58-15"><a href="#cb58-15"></a>at</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;AutoTuner:classif.svm.tuned&gt;
* Model: list
* Search Space:
&lt;ParamSet&gt;
      id    class     lower    upper nlevels        default value
1:  cost ParamDbl -11.51293 11.51293     Inf &lt;NoDefault[3]&gt;      
2: gamma ParamDbl -11.51293 11.51293     Inf &lt;NoDefault[3]&gt;      
Trafo is set.
* Packages: mlr3, mlr3tuning, mlr3learners, e1071
* Predict Type: response
* Feature Types: logical, integer, numeric
* Properties: multiclass, twoclass</code></pre>
</div>
</div>
<p>We can now use this like any other learner, calling the <code>$train()</code> and <code>$predict()</code> methods. The key difference to a normal learner, is that calling <code>$train()</code> also tunes the model.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-039_3105350918d56b3ad7f39cebf5c5504a">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb60-2"><a href="#cb60-2"></a>split <span class="ot">=</span> <span class="fu">partition</span>(task)</span>
<span id="cb60-3"><a href="#cb60-3"></a>at<span class="sc">$</span><span class="fu">train</span>(task, <span class="at">row_ids =</span> split<span class="sc">$</span>train)</span>
<span id="cb60-4"><a href="#cb60-4"></a>at<span class="sc">$</span><span class="fu">predict</span>(task, <span class="at">row_ids =</span> split<span class="sc">$</span>test)<span class="sc">$</span><span class="fu">score</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
 0.2608696 </code></pre>
</div>
</div>
<p>We could also pass the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> to <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> and <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a>, which would result in a nested resampling (<a href="#sec-nested-resampling"><span>Section&nbsp;4.5</span></a>), discussed next.</p>
</section><section id="sec-nested-resampling" class="level2 page-columns page-full" data-number="4.5"><h2 data-number="4.5" class="anchored" data-anchor-id="sec-nested-resampling">
<span class="header-section-number">4.5</span> Nested Resampling</h2>
<p>Hyperparameter optimization generally requires an additional layer or resampling to prevent bias in tuning. If the same data is used for determining the optimal configuration and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased <span class="citation" data-cites="Simon2007">(<a href="references.html#ref-Simon2007" role="doc-biblioref">Simon 2007</a>)</span>. This is analogous to optimism of the training error described in <span class="citation" data-cites="james_introduction_2014">(<a href="references.html#ref-james_introduction_2014" role="doc-biblioref">James et al. 2014</a>)</span>, which occurs when training error is taken as an estimate of out-of-sample performance. This bias is represented in <a href="#fig-bad-tuning">Figure&nbsp;<span>4.6</span></a> which shows an algorithm being tuned on data that has been split intro training and testing data, and then the same data is used to estimate the model performance after selecting the best configuration after HPO.</p>
<div class="cell" data-alt-text="Flowchart representing biased tuning. The diagram shows input 'Search Space' passed into 'Algorithm' which then has a black arrow to 'Train',then a black arrow to 'Test', then a black arrow to 'Optimal configuration'. This has a red arrow back to 'Algorithm' then to 'Train' then to 'Test' then to 'Performance'. The diagram clearly shows the reuse of the same training and testing data.">
<div class="cell-output-display">
<div id="fig-bad-tuning" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p>
</p>
<pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">%%{init: { 'flowchart': {'rankSpacing': 25}}}%%
flowchart LR
    search[(Search Space)]
    opt[(Optimal&lt;br&gt;configuration)]
    train[(Train)]
    test[(Test)]
    alg[/Algorithm/]
    search --&gt; alg
    alg --&gt; train
    alg --&gt; train
    train --&gt; test
    train --&gt; test
    test --&gt; opt
    test --&gt; perf(Performance)
    opt --&gt; alg

    linkStyle 1,3,6,7 stroke-width:2px, stroke:red;
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>

<p></p><figcaption class="figure-caption">Figure&nbsp;4.6: Illustration of biased tuning. An algorithm is tuned by training on the <code>Train</code> dataset and then the optimal configuration is selected by evaluation on the <code>Test</code> data. The model’s performance is then evaluated with the optimal configuration on the same data.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="page-columns page-full"><p>Nested resampling separates model optimization from the process of estimating the performance of the model by adding an additional layer of resampling, i.e., whilst model performance is estimated using a resampling method in the ‘usual way’, tuning is then performed by resampling the resampled data (<a href="#fig-nested-resampling">Figure&nbsp;<span>4.7</span></a>). For more details and a formal introduction to nested resampling the reader is referred to <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="references.html#ref-hpo_practical" role="doc-biblioref">2021</a>)</span>.</p><div class="no-row-height column-margin column-container"><span class="">Nested Resampling</span></div></div>
<p>A common confusion is how and when to use nested resampling. In the rest of this section we will answer the ‘how’ question but first the ‘when’. A common mistake is to confuse nested resampling for model evaluation and comparison, with tuning for model deployment. To put it differently, nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset, it is <em>not</em> a procedure to select optimal hyperparameters. Nested resampling produces many hyperparameter configurations which should not be used to construct a final model <span class="citation" data-cites="Simon2007">(<a href="references.html#ref-Simon2007" role="doc-biblioref">Simon 2007</a>)</span>.</p>
<div class="cell" data-hash="optimization_cache/html/fig-nested-resampling_8254582160c974cd276e5f537f8102fd">
<div class="cell-output-display">
<div id="fig-nested-resampling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/nested_resampling.png" class="img-fluid figure-img" alt="The image shows three rows of blocks in light and dark green representing three-fold cross-validation for the outer resampling. Below the dark green blocks are four further rows of blue and gray blocks representing four-fold cross-validation for the inner resampling." width="662"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4.7: An illustration of nested resampling. The green blocks represent 3-fold coss-validation for the outer resampling for model evaluation and the blue and gray blocks represent 4-fold cross-validation for the inner resampling for HPO.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In words this process runs as follows:</p>
<ol type="1">
<li>Outer resampling – Instantiate 3-fold cross-validation to create different testing and training data sets.</li>
<li>Inner resampling – Within the training data instantiate 4-fold cross-validation to create different inner testing and training data sets.</li>
<li>HPO – Tune the hyperparameters using the inner data splits (blue and gray blocks).</li>
<li>Training – Fit the learner on the outer training data set using the optimal hyperparameter configuration obtained from the inner resampling (dark green blocks).</li>
<li>Evaluation – Evaluate the performance of the learner on the outer testing data (light green blocks).</li>
<li>Cross-validation – Repeat (2)-(5) for each of the three folds.</li>
<li>Aggregation – Take the sample mean of the three performance values for an unbiased performance estimate.</li>
</ol>
<p>That is enough theory for now, let us take a look at how this works in mlr3.</p>
<section id="nested-resampling-with-autotuner" class="level3" data-number="4.5.1"><h3 data-number="4.5.1" class="anchored" data-anchor-id="nested-resampling-with-autotuner">
<span class="header-section-number">4.5.1</span> Nested Resampling with <code>AutoTuner</code>
</h3>
<p>Nested resampling in mlr3 becomes quite simple with the <code>AutoTuner</code> (<a href="#sec-autotuner"><span>Section&nbsp;4.4</span></a>). We simply specify the inner-resampling and tuning setup with the <code>AutoTuner</code> and then pass this to <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> or <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a>. Continuing with our previous example we will use the auto-tuner to resample a support vector classifier with 3-fold cross-validation in the outer-resampling and 4-fold cross-validation in the inner resampling.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-040_7f6a03dc20cb752a5506013b763734fb">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb62-2"><a href="#cb62-2"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb62-3"><a href="#cb62-3"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb62-4"><a href="#cb62-4"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb62-5"><a href="#cb62-5"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb62-6"><a href="#cb62-6"></a>)</span>
<span id="cb62-7"><a href="#cb62-7"></a></span>
<span id="cb62-8"><a href="#cb62-8"></a>at <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb62-9"><a href="#cb62-9"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb62-10"><a href="#cb62-10"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb62-11"><a href="#cb62-11"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">4</span>),</span>
<span id="cb62-12"><a href="#cb62-12"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb62-13"><a href="#cb62-13"></a>  <span class="at">term_evals =</span> <span class="dv">20</span>,</span>
<span id="cb62-14"><a href="#cb62-14"></a>)</span>
<span id="cb62-15"><a href="#cb62-15"></a></span>
<span id="cb62-16"><a href="#cb62-16"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb62-17"><a href="#cb62-17"></a>outer_resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb62-18"><a href="#cb62-18"></a></span>
<span id="cb62-19"><a href="#cb62-19"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, at, outer_resampling, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb62-20"><a href="#cb62-20"></a></span>
<span id="cb62-21"><a href="#cb62-21"></a>rr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResampleResult&gt; of 3 iterations
* Task: sonar
* Learner: classif.svm.tuned
* Warnings: 0 in 0 iterations
* Errors: 0 in 0 iterations</code></pre>
</div>
</div>
<p>Note we set <code>store_models = TRUE</code> so that the <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> models are stored to investigate the inner tuning. In this example, we utilized the same resampling strategy (K-fold cross-validation) but the mlr3 infrastructure is not limited to this, you can freely combine different inner and outer resampling strategies as you choose. You can also mix-and-match parallelization methods for controlling the process (<a href="technical.html#sec-nested-resampling-parallelization"><span>Section&nbsp;9.1.4</span></a>).</p>
<p>There are some special functions for nested resampling available in addition to the methods described in <a href="performance.html#sec-resampling"><span>Section&nbsp;3.2</span></a>.</p>
<p>The <a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_results.html"><code>extract_inner_tuning_results()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_archives.html"><code>extract_inner_tuning_archives()</code></a> functions return the optimal configurations (across all outer folds) and full tuning archives respectively.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-042_358b337893e09893d0fa48607d06cf95">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1"></a><span class="fu">extract_inner_tuning_results</span>(rr)[,</span>
<span id="cb64-2"><a href="#cb64-2"></a>  <span class="fu">list</span>(iteration, cost, gamma, classif.ce)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration      cost     gamma classif.ce
1:         1 11.512925 -5.756463  0.1663866
2:         2  5.756463 -5.756463  0.2090336
3:         3  5.756463 -5.756463  0.1941176</code></pre>
</div>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1"></a><span class="fu">extract_inner_tuning_archives</span>(rr)[,</span>
<span id="cb66-2"><a href="#cb66-2"></a>  <span class="fu">list</span>(iteration, cost, gamma, classif.ce)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    iteration       cost      gamma classif.ce
 1:         1 -11.512925 -11.512925  0.4567227
 2:         1 -11.512925   5.756463  0.4567227
 3:         1  -5.756463 -11.512925  0.4567227
 4:         1   0.000000   5.756463  0.4567227
 5:         1   5.756463 -11.512925  0.2747899
---                                           
56:         3   0.000000 -11.512925  0.4678571
57:         3   0.000000  -5.756463  0.2151261
58:         3   0.000000   5.756463  0.4678571
59:         3   5.756463   0.000000  0.4678571
60:         3  11.512925  -5.756463  0.1941176</code></pre>
</div>
</div>
<p>From the optimal results, we observe a trend toward larger <code>cost</code> and smaller <code>gamma</code> values. However, as we discussed earlier, these values should not be used to fit a final model as the selected hyperparameters might differ greatly between the resampling iterations. On the one hand, this could be due to the optimization algorithm used, for example, with simple algorithms like random search, we do not expect stability of hyperparameters. On the other hand, more advanced methods like irace converge to an optimal hyperparameter configuration. Another reason for instability in hyperparameters could be due to small data sets and/or a low number of resampling iterations (i.e., the usual small data high variance problem).</p>
</section><section id="performance-comparison" class="level3" data-number="4.5.2"><h3 data-number="4.5.2" class="anchored" data-anchor-id="performance-comparison">
<span class="header-section-number">4.5.2</span> Performance comparison</h3>
<p>Finally, we will compare the predictive performances estimated on the outer resampling to the inner resampling to gain an understanding of model overfitting and general performance.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-043_b89739bba54b5133e8238ee33f9367c6">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1"></a><span class="fu">extract_inner_tuning_results</span>(rr)[,</span>
<span id="cb68-2"><a href="#cb68-2"></a>  <span class="fu">list</span>(iteration, cost, gamma, classif.ce)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration      cost     gamma classif.ce
1:         1 11.512925 -5.756463  0.1663866
2:         2  5.756463 -5.756463  0.2090336
3:         3  5.756463 -5.756463  0.1941176</code></pre>
</div>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1"></a>rr<span class="sc">$</span><span class="fu">score</span>()[,</span>
<span id="cb70-2"><a href="#cb70-2"></a>  <span class="fu">list</span>(iteration, classif.ce)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration classif.ce
1:         1  0.1285714
2:         2  0.1014493
3:         3  0.1884058</code></pre>
</div>
</div>
<p>Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.</p>
<p>It is therefore important to ensure that the performance of a tuned model is <em>always</em> reported as the aggregated performance of all outer resampling iterations, which is an unbiased estimate of future model performance. Note here we use the term <em>unbiased</em> to refer only to the statistical procedure of the performance estimation. The underlying prediction of the model could still be biased e.g.&nbsp;due to a bias in the data set.</p>
<div class="cell" data-hash="optimization_cache/html/optimization-045_af28f5ead7fe0aa02f8b074f295cda0e">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
 0.1394755 </code></pre>
</div>
</div>
<p>As a final note, nested resampling is computationally expensive, as a simple example using five outer folds and three inner folds with a grid search of resolution 5 used to tune 2 parameters, results in 5<em>3</em>5*5 = 375 iterations of model training/testing. In practice, you may often see closer to three folds used in inner resampling or even holdout, or if you have the resources then we recommend parallelization (<a href="technical.html#sec-parallelization"><span>Section&nbsp;9.1</span></a>).</p>
</section></section><section id="conclusion" class="level2 page-columns page-full" data-number="4.6"><h2 data-number="4.6" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">4.6</span> Conclusion</h2>
<p>In this chapter, we learned how to optimize a model using tuning instances, about different tuners and terminators, how to make use of the automated methods for quicker implementation in larger experiments, and the importance of nested resampling. The most important functions and classes we learned about are in <a href="#tbl-api-optimization">Table&nbsp;<span>4.3</span></a> alongside their R6 classes. If you are interested in learning more about the underlying R6 classes to gain finer control of these methods, then take a look at the online API.</p>
<div id="tbl-api-optimization" class="anchored">
<table class="table">
<caption>Table&nbsp;4.3: Core S3 ‘sugar’ functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.</caption>
<thead><tr class="header">
<th>S3 function</th>
<th>R6 Class</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html"><code>tnr()</code></a></td>
<td><a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a></td>
<td>Determines an optimisation algorithm</td>
</tr>
<tr class="even">
<td><a href="https://bbotk.mlr-org.com/reference/trm.html"><code>trm()</code></a></td>
<td><a href="https://bbotk.mlr-org.com/reference/Terminator.html"><code>Terminator</code></a></td>
<td>Controls when to terminate the tuning algorithm</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3tuning.mlr-org.com/reference/ti.html"><code>ti()</code></a></td>
<td>
<a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html"><code>TuningInstanceSingleCrit</code></a> or <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html"><code>TuningInstanceMultiCrit</code></a>
</td>
<td>Stores tuning settings and save results</td>
</tr>
<tr class="even">
<td><a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>paradox::to_tune()</code></a></td>
<td><a href="https://paradox.mlr-org.com/reference/TuneToken.html"><code>paradox::TuneToken</code></a></td>
<td>Sets which parameters in a learner to tune and over what search space</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html"><code>auto_tuner()</code></a></td>
<td><a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a></td>
<td>Automates the tuning process</td>
</tr>
<tr class="even">
<td><a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_results.html"><code>extract_inner_tuning_results()</code></a></td>
<td>-</td>
<td>Extracts inner results from nested resampling</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_archives.html"><code>extract_inner_tuning_archives()</code></a></td>
<td>-</td>
<td>Extracts inner archives from nested resampling</td>
</tr>
</tbody>
</table>
</div>
<section id="resources" class="level3 unnumbered unlisted page-columns page-full"><h3 class="unnumbered unlisted anchored" data-anchor-id="resources">Resources</h3>
<div class="page-columns page-full"><p>The <a href="https://cheatsheets.mlr-org.com/mlr3tuning.pdf">mlr3tuning cheatsheet</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> summarizes the most important functions of mlr3tuning and the <a href="https://mlr-org.com/gallery.html#category:tuning">mlr3 gallery</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> features a collection of case studies and demonstrations about optimization, most notably learn how to:</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<a href="https://cheatsheets.mlr-org.com/mlr3tuning.pdf">https://cheatsheets.mlr-org.com/mlr3tuning.pdf</a></p></li><li id="fn2"><p><sup>2</sup>&nbsp;<a href="https://mlr-org.com/gallery.html#category:tuning">https://mlr-org.com/gallery.html#category:tuning</a></p></li></div></div>
<ul>
<li>Apply advanced methods in the <a href="https://mlr-org.com/gallery.html#category:practical_tuning_series">practical tuning series</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</li>
<li>Optimize an rpart classification tree with only a <a href="https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/">few lines of code</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>Tune an XGBoost model with <a href="https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/">early stopping</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</li>
<li>Quickly load and tune over search spaces that have been published in literature with <a href="https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/">mlr3tuningspaces</a><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</li>
</ul><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;<a href="https://mlr-org.com/gallery.html#category:practical_tuning_series">https://mlr-org.com/gallery.html#category:practical_tuning_series</a></p></li><li id="fn4"><p><sup>4</sup>&nbsp;<a href="https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/">https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/</a></p></li><li id="fn5"><p><sup>5</sup>&nbsp;<a href="https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/">https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/</a></p></li><li id="fn6"><p><sup>6</sup>&nbsp;<a href="https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/">https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/</a></p></li></div></section></section><section id="exercises" class="level2" data-number="4.7"><h2 data-number="4.7" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">4.7</span> Exercises</h2>
<ol type="1">
<li>Tune the <code>mtry</code>, <code>sample.fraction</code>, <code>num.trees</code> hyperparameters of a random forest model (<code>regr.ranger</code>) on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_mtcars.html"><code>Motor Trend</code></a> data set (<code>mtcars</code>). Use a simple random search with 50 evaluations and select a suitable batch size. Evaluate with a 3-fold cross-validation and the root mean squared error.</li>
<li>Evaluate the performance of the model created in Question 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling. Print the unbiased performance estimate of the model.</li>
<li>Tune and benchmark an XGBoost model against a logistic regression and determine which has the best Brier score. Use mlr3tuningspaces and nested resampling.</li>
</ol>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-bergstra2012" class="csl-entry" role="doc-biblioentry">
Bergstra, James, and Yoshua Bengio. 2012. <span>“Random Search for Hyper-Parameter Optimization.”</span> <em>Journal of Machine Learning Research</em> 13 (10): 281–305. <a href="http://jmlr.org/papers/v13/bergstra12a.html">http://jmlr.org/papers/v13/bergstra12a.html</a>.
</div>
<div id="ref-hpo_practical" class="csl-entry" role="doc-biblioentry">
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2021. <span>“Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges.”</span> <a href="https://doi.org/10.48550/ARXIV.2107.05847">https://doi.org/10.48550/ARXIV.2107.05847</a>.
</div>
<div id="ref-hpo_automl" class="csl-entry" role="doc-biblioentry">
Feurer, Matthias, and Frank Hutter. 2019. <span>“Hyperparameter Optimization.”</span> In <em>Automated Machine Learning: Methods, Systems, Challenges</em>, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 3–33. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5_1">https://doi.org/10.1007/978-3-030-05318-5_1</a>.
</div>
<div id="ref-hansen2011" class="csl-entry" role="doc-biblioentry">
Hansen, Nikolaus, and Anne Auger. 2011. <span>“CMA-ES: Evolution Strategies and Covariance Matrix Adaptation.”</span> In <em>Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation</em>, 991–1010.
</div>
<div id="ref-james_introduction_2014" class="csl-entry" role="doc-biblioentry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. <em>An Introduction to Statistical Learning: With Applications in r</em>. Springer Publishing Company, Incorporated.
</div>
<div id="ref-hpo_multi" class="csl-entry" role="doc-biblioentry">
Karl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. <span>“Multi-Objective Hyperparameter Optimization - an Overview.”</span> <a href="https://doi.org/10.48550/ARXIV.2206.07438">https://doi.org/10.48550/ARXIV.2206.07438</a>.
</div>
<div id="ref-li2017" class="csl-entry" role="doc-biblioentry">
Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2017. <span>“Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.”</span> <em>The Journal of Machine Learning Research</em> 18 (1): 6765–6816.
</div>
<div id="ref-lopez2016" class="csl-entry" role="doc-biblioentry">
López-Ibáñez, Manuel, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres, Mauro Birattari, and Thomas Stützle. 2016. <span>“The Irace Package: Iterated Racing for Automatic Algorithm Configuration.”</span> <em>Operations Research Perspectives</em> 3: 43–58.
</div>
<div id="ref-Simon2007" class="csl-entry" role="doc-biblioentry">
Simon, Richard. 2007. <span>“Resampling Strategies for Model Assessment and Selection.”</span> In <em>Fundamentals of Data Mining in Genomics and Proteomics</em>, edited by Werner Dubitzky, Martin Granzow, and Daniel Berrar, 173–86. <span>Boston, MA</span>: <span>Springer US</span>. <a href="https://doi.org/10.1007/978-0-387-47509-7_8">https://doi.org/10.1007/978-0-387-47509-7_8</a>.
</div>
<div id="ref-tsallis1996" class="csl-entry" role="doc-biblioentry">
Tsallis, Constantino, and Daniel A Stariolo. 1996. <span>“Generalized Simulated Annealing.”</span> <em>Physica A: Statistical Mechanics and Its Applications</em> 233 (1-2): 395–406.
</div>
<div id="ref-xiang2013" class="csl-entry" role="doc-biblioentry">
Xiang, Yang, Sylvain Gubian, Brian Suomela, and Julia Hoeng. 2013. <span>“Generalized Simulated Annealing for Global Optimization: The GenSA Package.”</span> <em>R J.</em> 5 (1): 13.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./performance.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resampling and Benchmarking</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./feature-selection.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature Selection</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb74" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Marc Becker</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-8115-0400</span></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="co">    email: marc.becker@stat.uni-muenchen.de</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Ludwig-Maximilians-Universität München</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Lennard Schneider</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0003-4152-5308</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="co">    email: lennart.schneider@stat.uni-muenchen.de</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Ludwig-Maximilians-Universität München</span></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="co">  Most machine learning algorithms are configurated by a set of hyperparameters.</span></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a><span class="co">  The goal of hyperparameter optimization is to find the optimal hyperparameter configuration of a machine learning algorithm for a given task.</span></span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a><span class="co">  This chapter presents an introduction to hyperparameter optimization in the mlr3 ecosystem.</span></span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a><span class="co">  As a practical example, we optimize the `cost` and `gamma`  hyperparameters of a support vector machine on the sonar task.</span></span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a><span class="co">  We introduce the tuning instance class that describes the tuning problem and the tuner class that wraps an optimization algorithm.</span></span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a><span class="co">  After running the optimization, we show how to analyze the results and fit a final model.</span></span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a><span class="co">  We also show how to run a multi-objective optimization with multiple measures.</span></span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a><span class="co">  Then we move on to more advanced topics like search space transformations, fallback learners and encapsulation.</span></span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a><span class="co">  Finally, we show how to use nested resampling to get an unbiased estimate of the performance of an optimized model.</span></span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hyperparameter Optimization {#sec-optimization}</span></span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-32"><a href="#cb74-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-33"><a href="#cb74-33" aria-hidden="true" tabindex="-1"></a>{{&lt; include _setup.qmd &gt;}}</span>
<span id="cb74-34"><a href="#cb74-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-35"><a href="#cb74-35" aria-hidden="true" tabindex="-1"></a>Machine learning algorithms usually include <span class="in">`r index("parameters")`</span> and <span class="in">`r define("hyperparameters")`</span>.</span>
<span id="cb74-36"><a href="#cb74-36" aria-hidden="true" tabindex="-1"></a>Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters.</span>
<span id="cb74-37"><a href="#cb74-37" aria-hidden="true" tabindex="-1"></a>In contrast, hyperparameters, are configured by the user and determine how the model will fit its parameters.</span>
<span id="cb74-38"><a href="#cb74-38" aria-hidden="true" tabindex="-1"></a>Examples include setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network.</span>
<span id="cb74-39"><a href="#cb74-39" aria-hidden="true" tabindex="-1"></a>Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms.</span>
<span id="cb74-40"><a href="#cb74-40" aria-hidden="true" tabindex="-1"></a>So in this chapter, we will demonstrate how to make this into more of a science.</span>
<span id="cb74-41"><a href="#cb74-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-42"><a href="#cb74-42" aria-hidden="true" tabindex="-1"></a>The goal of <span class="in">`r define("hyperparameter optimization", "HPO: Hyperparameter Optimization")`</span> (@sec-model-tuning) or model <span class="in">`r index("tuning")`</span> is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.</span>
<span id="cb74-43"><a href="#cb74-43" aria-hidden="true" tabindex="-1"></a>There is no closed-form mathematical representation (nor analytic gradient information) for model agnostic HPO, instead, we follow a numerical black-box optimization: an ML algorithm is configured with values chosen for one or more hyperparameters, this algorithm is then evaluated (optimally with a robust resampling method) and its performance measured, this is repeated with multiple configurations and the configuration with the best performance is selected.</span>
<span id="cb74-44"><a href="#cb74-44" aria-hidden="true" tabindex="-1"></a>We could think of finding the optimal configuration in the same way as selecting a model from a benchmark experiment, where in this case each model uses the same underlying algorithm but with different hyperparameter configurations.</span>
<span id="cb74-45"><a href="#cb74-45" aria-hidden="true" tabindex="-1"></a>For example, we could naively tune the number of trees in a random forest using basic mlr3 code:</span>
<span id="cb74-46"><a href="#cb74-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-49"><a href="#cb74-49" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb74-50"><a href="#cb74-50" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-naivetuning</span></span>
<span id="cb74-51"><a href="#cb74-51" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: In this code example we benchmark three random forest models with 1, 10, and 100 trees respectively, using 3-fold resampling, classification error loss, and tested on the simplified penguin dataset. The plot shows that the models with 10 and 100 trees are better performing across all three folds and 100 trees may be better than 10.</span></span>
<span id="cb74-52"><a href="#cb74-52" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Boxplots for each of the three configurations showing classification error over the three folds. The image shows the worst performance in the model with 1 tree and similar performance with 10 and 100 trees.</span></span>
<span id="cb74-53"><a href="#cb74-53" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(</span>
<span id="cb74-54"><a href="#cb74-54" aria-hidden="true" tabindex="-1"></a>  <span class="at">tasks =</span> <span class="fu">tsk</span>(<span class="st">"penguins_simple"</span>),</span>
<span id="cb74-55"><a href="#cb74-55" aria-hidden="true" tabindex="-1"></a>  <span class="at">learners =</span> <span class="fu">list</span>(</span>
<span id="cb74-56"><a href="#cb74-56" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">num.trees =</span> <span class="dv">1</span>, <span class="at">id =</span> <span class="st">"1 tree"</span>),</span>
<span id="cb74-57"><a href="#cb74-57" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">num.trees =</span> <span class="dv">10</span>, <span class="at">id =</span> <span class="st">"10 trees"</span>),</span>
<span id="cb74-58"><a href="#cb74-58" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">num.trees =</span> <span class="dv">100</span>, <span class="at">id =</span> <span class="st">"100 trees"</span>)),</span>
<span id="cb74-59"><a href="#cb74-59" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamplings =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb74-60"><a href="#cb74-60" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb74-61"><a href="#cb74-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-62"><a href="#cb74-62" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr)</span>
<span id="cb74-63"><a href="#cb74-63" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-64"><a href="#cb74-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-65"><a href="#cb74-65" aria-hidden="true" tabindex="-1"></a>Human trial-and-error (which is essentially what we are doing above), is time-consuming, often biased, error-prone, and computationally irreproducible.</span>
<span id="cb74-66"><a href="#cb74-66" aria-hidden="true" tabindex="-1"></a>Instead, many sophisticated HPO methods (@sec-tuner) (or 'tuners') have been developed over the last few decades for robust and efficient HPO.</span>
<span id="cb74-67"><a href="#cb74-67" aria-hidden="true" tabindex="-1"></a>Most HPO methods are iterative and propose different configurations until some termination criterion is met, at which point the optimal configuration is then returned (@fig-optimization-loop).</span>
<span id="cb74-68"><a href="#cb74-68" aria-hidden="true" tabindex="-1"></a>Popular, modern examples are given by algorithms based on evolutionary algorithms or Bayesian optimization methods.</span>
<span id="cb74-69"><a href="#cb74-69" aria-hidden="true" tabindex="-1"></a>Recent HPO methods often also make use of evaluating a configuration at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and training data can always be subsampled to only include a smaller fraction of all available data.</span>
<span id="cb74-70"><a href="#cb74-70" aria-hidden="true" tabindex="-1"></a>The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising).</span>
<span id="cb74-71"><a href="#cb74-71" aria-hidden="true" tabindex="-1"></a>Another interesting direction of HPO is to optimize multiple metrics (@sec-multi-metrics-tuning) simultaneously, e.g., minimizing the generalization error along with the size of the model.</span>
<span id="cb74-72"><a href="#cb74-72" aria-hidden="true" tabindex="-1"></a>This gives rise to multi-objective HPO.</span>
<span id="cb74-73"><a href="#cb74-73" aria-hidden="true" tabindex="-1"></a>For more details on HPO in general, the reader is referred to @hpo_practical and @hpo_automl.</span>
<span id="cb74-74"><a href="#cb74-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-77"><a href="#cb74-77" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb74-78"><a href="#cb74-78" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-optimization-loop</span></span>
<span id="cb74-79"><a href="#cb74-79" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Representation of the hyperparameter optimization loop in mlr3tuning. Blue - Hyperparameter optimization loop. Purple - Objects of the tuning instance supplied by the user. Blue-Green - Internally created objects of the tuning instance. Green - Optimization Algorithm.</span></span>
<span id="cb74-80"><a href="#cb74-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb74-81"><a href="#cb74-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-82"><a href="#cb74-82" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/hpo_loop.png"</span>)</span>
<span id="cb74-83"><a href="#cb74-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-84"><a href="#cb74-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-85"><a href="#cb74-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Tuning {#sec-model-tuning}</span></span>
<span id="cb74-86"><a href="#cb74-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-87"><a href="#cb74-87" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3tuning`</span>\index{mlr3tuning} is the hyperparameter optimization package of the mlr3 ecosystem.</span>
<span id="cb74-88"><a href="#cb74-88" aria-hidden="true" tabindex="-1"></a>At the heart of the package (and indeed any optimization problem) are the R6 classes</span>
<span id="cb74-89"><a href="#cb74-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-90"><a href="#cb74-90" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("TuningInstanceSingleCrit")`</span> and <span class="in">`r ref("TuningInstanceMultiCrit")`</span>, which are used to construct a tuning 'instance' which describes the optimization problem and stores the results; and</span>
<span id="cb74-91"><a href="#cb74-91" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("Tuner")`</span> which is used to get and set optimization algorithms.</span>
<span id="cb74-92"><a href="#cb74-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-93"><a href="#cb74-93" aria-hidden="true" tabindex="-1"></a>In this section, we will cover these classes as well as other supporting functions and classes.</span>
<span id="cb74-94"><a href="#cb74-94" aria-hidden="true" tabindex="-1"></a>Throughout this section, we will look at optimizing a <span class="in">`r index("support vector machine")`</span> (SVM) on the <span class="in">`r ref("mlr_tasks_sonar", text = "sonar")`</span> data set as a running example.</span>
<span id="cb74-95"><a href="#cb74-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-96"><a href="#cb74-96" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learner and Search Space {#sec-learner-search-space}</span></span>
<span id="cb74-97"><a href="#cb74-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-98"><a href="#cb74-98" aria-hidden="true" tabindex="-1"></a>We begin by constructing a support vector machine from the <span class="in">`r ref_pkg("e1071")`</span> with a radial kernel and specify we want to tune this using <span class="in">`"C-classification"`</span> (the alternative is <span class="in">`"nu-classification"`</span>, which has the same underlying algorithm but with a <span class="in">`nu`</span> parameter to tune over <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span> instead of <span class="in">`cost`</span> over [0, $\infty$)).</span>
<span id="cb74-99"><a href="#cb74-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-100"><a href="#cb74-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-003}</span></span>
<span id="cb74-101"><a href="#cb74-101" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>, <span class="at">type =</span> <span class="st">"C-classification"</span>, <span class="at">kernel =</span> <span class="st">"radial"</span>)</span>
<span id="cb74-102"><a href="#cb74-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-103"><a href="#cb74-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-104"><a href="#cb74-104" aria-hidden="true" tabindex="-1"></a>Learner hyperparameter information is stored in the <span class="in">`$param_set`</span> field, including parameter name, class (e.g., discrete or numeric), levels it can be tuned over, tuning limits, and more.</span>
<span id="cb74-105"><a href="#cb74-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-106"><a href="#cb74-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-004}</span></span>
<span id="cb74-107"><a href="#cb74-107" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(learner<span class="sc">$</span>param_set)[, <span class="fu">list</span>(id, class, lower, upper, nlevels)]</span>
<span id="cb74-108"><a href="#cb74-108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-109"><a href="#cb74-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-110"><a href="#cb74-110" aria-hidden="true" tabindex="-1"></a>Note that <span class="in">`$param_set`</span> also displays non-tunable parameters.</span>
<span id="cb74-111"><a href="#cb74-111" aria-hidden="true" tabindex="-1"></a>Detailed information about parameters can be found in the help pages of the underlying implementation, for this example see <span class="in">`r ref("e1071::svm()")`</span>.</span>
<span id="cb74-112"><a href="#cb74-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-113"><a href="#cb74-113" aria-hidden="true" tabindex="-1"></a>Given infinite resources, we could tune every single hyperparameter, but in reality that is not possible so instead only a subset of hyperparameters can be tuned.</span>
<span id="cb74-114"><a href="#cb74-114" aria-hidden="true" tabindex="-1"></a>This subset is referred to as the <span class="in">`r define("search space")`</span> or <span class="in">`r index("tuning space")`</span>.</span>
<span id="cb74-115"><a href="#cb74-115" aria-hidden="true" tabindex="-1"></a>In this example we will tune the regularization and influence hyperparameters, <span class="in">`cost`</span> and <span class="in">`gamma`</span>.</span>
<span id="cb74-116"><a href="#cb74-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-117"><a href="#cb74-117" aria-hidden="true" tabindex="-1"></a>For numeric hyperparameters (we will explore others later) one must specify the bounds to tune over.</span>
<span id="cb74-118"><a href="#cb74-118" aria-hidden="true" tabindex="-1"></a>We do this by constructing a learner and using <span class="in">`r ref("to_tune()")`</span> to set the lower and upper limits for the parameters we want to tune.</span>
<span id="cb74-119"><a href="#cb74-119" aria-hidden="true" tabindex="-1"></a>This function allows us to construct a learner in the usual way but to leave the hyperparameters of interest to be unspecified within a set range.</span>
<span id="cb74-120"><a href="#cb74-120" aria-hidden="true" tabindex="-1"></a>This is best demonstrated by example:</span>
<span id="cb74-121"><a href="#cb74-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-122"><a href="#cb74-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-005}</span></span>
<span id="cb74-123"><a href="#cb74-123" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb74-124"><a href="#cb74-124" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb74-125"><a href="#cb74-125" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb74-126"><a href="#cb74-126" aria-hidden="true" tabindex="-1"></a>  <span class="at">type  =</span> <span class="st">"C-classification"</span>,</span>
<span id="cb74-127"><a href="#cb74-127" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span></span>
<span id="cb74-128"><a href="#cb74-128" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-129"><a href="#cb74-129" aria-hidden="true" tabindex="-1"></a>learner</span>
<span id="cb74-130"><a href="#cb74-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-131"><a href="#cb74-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-132"><a href="#cb74-132" aria-hidden="true" tabindex="-1"></a>Here we have constructed a classification SVM by setting the type to "C-classification", the kernel to "radial", and not fully specifying the <span class="in">`cost`</span> and <span class="in">`gamma`</span> hyperparameters but instead indicating that we will tune these parameters.</span>
<span id="cb74-133"><a href="#cb74-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-134"><a href="#cb74-134" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb74-135"><a href="#cb74-135" aria-hidden="true" tabindex="-1"></a>The <span class="in">`cost`</span> and <span class="in">`gamma`</span> hyperparameters are usually tuned on the logarithmic scale.</span>
<span id="cb74-136"><a href="#cb74-136" aria-hidden="true" tabindex="-1"></a>You can find out more in @sec-advanced-search-spaces.</span>
<span id="cb74-137"><a href="#cb74-137" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb74-138"><a href="#cb74-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-139"><a href="#cb74-139" aria-hidden="true" tabindex="-1"></a>Search spaces are usually chosen by experience.</span>
<span id="cb74-140"><a href="#cb74-140" aria-hidden="true" tabindex="-1"></a>In some cases these can be quite complex, @sec-paradox describes how to construct these.</span>
<span id="cb74-141"><a href="#cb74-141" aria-hidden="true" tabindex="-1"></a>@sec-tuning-spaces introduces the <span class="in">`r mlr3tuningspaces`</span> extension package which allows loading of search spaces that have been established in published scientific articles.</span>
<span id="cb74-142"><a href="#cb74-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-143"><a href="#cb74-143" aria-hidden="true" tabindex="-1"></a><span class="fu">### Terminator {#sec-terminator}</span></span>
<span id="cb74-144"><a href="#cb74-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-145"><a href="#cb74-145" aria-hidden="true" tabindex="-1"></a>Theoretically, a tuner could search an entire search space exhaustively, however practically this is not possible and mathematically this is impossible for continuous hyperparameters.</span>
<span id="cb74-146"><a href="#cb74-146" aria-hidden="true" tabindex="-1"></a>Therefore a core part of configuring tuning is to specify when to terminate the algorithm, this is also known as specifying the <span class="in">`r define("tuning budget")`</span>.</span>
<span id="cb74-147"><a href="#cb74-147" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3tuning`</span> includes many methods to specify when to terminate an algorithm, which are known as <span class="in">`r ref("Terminator", text = "Terminators")`</span>\index{Terminators}<span class="co">[</span><span class="ot">Terminators</span><span class="co">]</span>{.aside}.</span>
<span id="cb74-148"><a href="#cb74-148" aria-hidden="true" tabindex="-1"></a>Available terminators are listed in @tbl-terms.</span>
<span id="cb74-149"><a href="#cb74-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-150"><a href="#cb74-150" aria-hidden="true" tabindex="-1"></a>| Terminator | Function call and default parameters |</span>
<span id="cb74-151"><a href="#cb74-151" aria-hidden="true" tabindex="-1"></a>|----------  | ---- |</span>
<span id="cb74-152"><a href="#cb74-152" aria-hidden="true" tabindex="-1"></a>| Number of Evaluations | <span class="in">`trm("evals", n_evals = 500)`</span> |</span>
<span id="cb74-153"><a href="#cb74-153" aria-hidden="true" tabindex="-1"></a>| Run Time | <span class="in">`trm("run_time", secs = 100)`</span> |</span>
<span id="cb74-154"><a href="#cb74-154" aria-hidden="true" tabindex="-1"></a>| Performance Level | <span class="in">`trm("perf_reached", level = 0.1)`</span> |</span>
<span id="cb74-155"><a href="#cb74-155" aria-hidden="true" tabindex="-1"></a>| Stagnation | <span class="in">`trm("stagnation", iters = 5, threshold = 1e-5)`</span> |</span>
<span id="cb74-156"><a href="#cb74-156" aria-hidden="true" tabindex="-1"></a>| None | <span class="in">`trm("none")`</span> |</span>
<span id="cb74-157"><a href="#cb74-157" aria-hidden="true" tabindex="-1"></a>| Clock Time | <span class="in">`trm("clock_time", stop_time = "2022-11-06 08:42:53 CET"`</span> |</span>
<span id="cb74-158"><a href="#cb74-158" aria-hidden="true" tabindex="-1"></a>| Combo | <span class="in">`trm("combo", terminators = list(run_time_100, evals_200)`</span> |</span>
<span id="cb74-159"><a href="#cb74-159" aria-hidden="true" tabindex="-1"></a>: Terminators available in <span class="in">`r mlr3tuning`</span>, their function call and default parameters. {#tbl-terms}</span>
<span id="cb74-160"><a href="#cb74-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-161"><a href="#cb74-161" aria-hidden="true" tabindex="-1"></a>The most commonly used terminators are those that stop the tuning after a certain time  (<span class="in">`"run_time"`</span>) or the number of evaluations (<span class="in">`"evals"`</span>).</span>
<span id="cb74-162"><a href="#cb74-162" aria-hidden="true" tabindex="-1"></a>Choosing a runtime is often based on practical considerations and intuition.</span>
<span id="cb74-163"><a href="#cb74-163" aria-hidden="true" tabindex="-1"></a>Using a time limit can be important on clusters so that the tuning is finished before the account budget is exhausted.</span>
<span id="cb74-164"><a href="#cb74-164" aria-hidden="true" tabindex="-1"></a>The <span class="in">`"perf_reached"`</span> terminator stops the tuning when a certain performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model.</span>
<span id="cb74-165"><a href="#cb74-165" aria-hidden="true" tabindex="-1"></a>However, one needs to be careful using this terminator as if the level is set too optimistically, the tuning might never terminate.</span>
<span id="cb74-166"><a href="#cb74-166" aria-hidden="true" tabindex="-1"></a>The <span class="in">`"stagnation"`</span> terminator stops when no progress is made in a certain amount of iterations.</span>
<span id="cb74-167"><a href="#cb74-167" aria-hidden="true" tabindex="-1"></a>Note, this could result in the optimization being terminated too early if the search space is too complex.</span>
<span id="cb74-168"><a href="#cb74-168" aria-hidden="true" tabindex="-1"></a>We use <span class="in">`"none"`</span> when tuners, such as Grid Search and Hyperband, control the termination themselves.</span>
<span id="cb74-169"><a href="#cb74-169" aria-hidden="true" tabindex="-1"></a>Terminators can be freely combined with the <span class="in">`"combo"`</span> terminator, this is explored in the exercises at the end of this chapter.</span>
<span id="cb74-170"><a href="#cb74-170" aria-hidden="true" tabindex="-1"></a>A complete and always up-to-date list of terminators can be found on our website at <span class="co">[</span><span class="ot">https://mlr-org.com/terminators.html</span><span class="co">](https://mlr-org.com/terminators.html)</span>.</span>
<span id="cb74-171"><a href="#cb74-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-172"><a href="#cb74-172" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tuning Instance with `ti` {#sec-tuning-instance}</span></span>
<span id="cb74-173"><a href="#cb74-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-174"><a href="#cb74-174" aria-hidden="true" tabindex="-1"></a>A <span class="in">`r define("tuning instance")`</span> can be constructed manually (@sec-tuning-instance) with the <span class="in">`r ref("ti()")`</span> function or automated (@sec-simplified-tuning) with the <span class="in">`r ref("tune()")`</span> function.</span>
<span id="cb74-175"><a href="#cb74-175" aria-hidden="true" tabindex="-1"></a>We cover the manual approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of <span class="in">`r mlr3tuning`</span>.</span>
<span id="cb74-176"><a href="#cb74-176" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("ti")`</span> function constructs a tuning instance which collects together the information required to optimise a model.</span>
<span id="cb74-177"><a href="#cb74-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-178"><a href="#cb74-178" aria-hidden="true" tabindex="-1"></a>Now continuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the <span class="in">`r ref("ti()")`</span> function to create a <span class="in">`r ref("TuningInstanceSingleCrit")`</span> (note: supplying two measures to <span class="in">`ti()`</span> would result in <span class="in">`r ref("TuningInstanceMultiCrit")`</span> (@sec-multi-metrics-tuning)).</span>
<span id="cb74-179"><a href="#cb74-179" aria-hidden="true" tabindex="-1"></a>For this example we will use three-fold resampling and will optimise the classification error measure. Note that we use <span class="in">`trm("none")`</span> as we are using an exhaustive grid search.</span>
<span id="cb74-180"><a href="#cb74-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-181"><a href="#cb74-181" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-007}</span></span>
<span id="cb74-182"><a href="#cb74-182" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb74-183"><a href="#cb74-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-184"><a href="#cb74-184" aria-hidden="true" tabindex="-1"></a>measure <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb74-185"><a href="#cb74-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-186"><a href="#cb74-186" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb74-187"><a href="#cb74-187" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb74-188"><a href="#cb74-188" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb74-189"><a href="#cb74-189" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb74-190"><a href="#cb74-190" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb74-191"><a href="#cb74-191" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-192"><a href="#cb74-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-193"><a href="#cb74-193" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb74-194"><a href="#cb74-194" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb74-195"><a href="#cb74-195" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb74-196"><a href="#cb74-196" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb74-197"><a href="#cb74-197" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb74-198"><a href="#cb74-198" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb74-199"><a href="#cb74-199" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-200"><a href="#cb74-200" aria-hidden="true" tabindex="-1"></a>instance</span>
<span id="cb74-201"><a href="#cb74-201" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-202"><a href="#cb74-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-203"><a href="#cb74-203" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tuner {#sec-tuner}</span></span>
<span id="cb74-204"><a href="#cb74-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-205"><a href="#cb74-205" aria-hidden="true" tabindex="-1"></a>After we created the tuning problem, we can look at *how* to tune.</span>
<span id="cb74-206"><a href="#cb74-206" aria-hidden="true" tabindex="-1"></a>There are multiple <span class="in">`r ref("Tuner", "Tuners")`</span>\index{Tuners}<span class="co">[</span><span class="ot">Tuners</span><span class="co">]</span>{.aside} in <span class="in">`r mlr3tuning`</span>, which implement different HPO algorithms.</span>
<span id="cb74-207"><a href="#cb74-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-208"><a href="#cb74-208" aria-hidden="true" tabindex="-1"></a>| Tuner | Function call | Method |</span>
<span id="cb74-209"><a href="#cb74-209" aria-hidden="true" tabindex="-1"></a>|----------  | ---- | ----- |</span>
<span id="cb74-210"><a href="#cb74-210" aria-hidden="true" tabindex="-1"></a>| Random Search | <span class="in">`tnr("random_search")`</span> | Samples configurations from a uniform distribution randomly <span class="co">[</span><span class="ot">@bergstra2012</span><span class="co">]</span>. |</span>
<span id="cb74-211"><a href="#cb74-211" aria-hidden="true" tabindex="-1"></a>| Grid Search | <span class="in">`tnr("grid_search")`</span> | Discretizes the range of each configuration and exhaustively evaluates each combination. |</span>
<span id="cb74-212"><a href="#cb74-212" aria-hidden="true" tabindex="-1"></a>| Iterative Racing | <span class="in">`tnr("irace")`</span> | Races down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space <span class="co">[</span><span class="ot">@lopez2016</span><span class="co">]</span>. |</span>
<span id="cb74-213"><a href="#cb74-213" aria-hidden="true" tabindex="-1"></a>| Bayesian Optimization | <span class="in">`tnr("mbo")`</span> | Iterative algorithms that make use of a continuously updated surrogate model built for the objective function. By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency. |</span>
<span id="cb74-214"><a href="#cb74-214" aria-hidden="true" tabindex="-1"></a>| Hyperband | <span class="in">`tnr("hyperband")`</span> | Multi-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping <span class="co">[</span><span class="ot">@li2017</span><span class="co">]</span>. |</span>
<span id="cb74-215"><a href="#cb74-215" aria-hidden="true" tabindex="-1"></a>| Covariance Matrix Adaptation Evolution Strategy | <span class="in">`tnr("cmaes")`</span> | Evolution strategy algorithm with sampling from a multivariate Gaussian who is updated with the success of the previous population <span class="co">[</span><span class="ot">@hansen2011</span><span class="co">]</span>. |</span>
<span id="cb74-216"><a href="#cb74-216" aria-hidden="true" tabindex="-1"></a>| Generalized Simulated Annealing | <span class="in">`tnr("gensa")`</span> | Probabilistic algorithm for numeric search spaces <span class="co">[</span><span class="ot">@xiang2013; @tsallis1996</span><span class="co">]</span>. |</span>
<span id="cb74-217"><a href="#cb74-217" aria-hidden="true" tabindex="-1"></a>| Nonlinear Optimization | <span class="in">`tnr("nloptr")`</span> | Several nonlinear optimization algorithms for numeric search spaces. |</span>
<span id="cb74-218"><a href="#cb74-218" aria-hidden="true" tabindex="-1"></a>: Tuning algorithms available in <span class="in">`r mlr3tuning`</span>, their function call and the methodology. {#tbl-tuners}</span>
<span id="cb74-219"><a href="#cb74-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-220"><a href="#cb74-220" aria-hidden="true" tabindex="-1"></a>When selecting algorithms, grid search and random search are the most basic and are often selected first in initial experiments.</span>
<span id="cb74-221"><a href="#cb74-221" aria-hidden="true" tabindex="-1"></a>They are 'naive' algorithms in that they try new configurations whilst ignoring performance from previous attempts.</span>
<span id="cb74-222"><a href="#cb74-222" aria-hidden="true" tabindex="-1"></a>In contrast, more advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly.</span>
<span id="cb74-223"><a href="#cb74-223" aria-hidden="true" tabindex="-1"></a>Some advanced algorithms are included in extension packages, for example the package <span class="in">`r mlr3mbo`</span> implements Bayesian optimization (also called Model-Based Optimization)\index{MBO}, and <span class="in">`r mlr3hyperband`</span> implements algorithms of the <span class="in">`r index("hyperband")`</span> family.</span>
<span id="cb74-224"><a href="#cb74-224" aria-hidden="true" tabindex="-1"></a>A complete and up-to-date list of tuners can be found on the <span class="co">[</span><span class="ot">website</span><span class="co">](https://mlr-org.com/tuners.html)</span>.</span>
<span id="cb74-225"><a href="#cb74-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-226"><a href="#cb74-226" aria-hidden="true" tabindex="-1"></a>For our SVM example, we will use a simple grid search with a resolution of 5, which is the distinct values to try *per hyperparameter*.</span>
<span id="cb74-227"><a href="#cb74-227" aria-hidden="true" tabindex="-1"></a>For example for a search space $<span class="sc">\{</span>1, 2, 3, 4, 5, 6<span class="sc">\}</span>$ then a grid search with resolution 3 would pick three values evenly apart in this search space, i.e., $<span class="sc">\{</span>2, 4, 6<span class="sc">\}</span>$.</span>
<span id="cb74-228"><a href="#cb74-228" aria-hidden="true" tabindex="-1"></a>The <span class="in">`batch_size`</span> controls how many configurations are evaluated at the same time (see @sec-parallelization).</span>
<span id="cb74-229"><a href="#cb74-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-230"><a href="#cb74-230" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-008}</span></span>
<span id="cb74-231"><a href="#cb74-231" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>)</span>
<span id="cb74-232"><a href="#cb74-232" aria-hidden="true" tabindex="-1"></a>tuner</span>
<span id="cb74-233"><a href="#cb74-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-234"><a href="#cb74-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-235"><a href="#cb74-235" aria-hidden="true" tabindex="-1"></a>In our example we are tuning over two numeric parameters, <span class="in">`r ref("TunerGridSearch")`</span> will create an equidistant grid between the respective upper and lower bounds.</span>
<span id="cb74-236"><a href="#cb74-236" aria-hidden="true" tabindex="-1"></a>This means our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.</span>
<span id="cb74-237"><a href="#cb74-237" aria-hidden="true" tabindex="-1"></a>Each configuration is a distinct set of hyperparameter values that is used to construct a model from the chosen learner, which is fit to the chosen task (@fig-optimization-loop).</span>
<span id="cb74-238"><a href="#cb74-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-239"><a href="#cb74-239" aria-hidden="true" tabindex="-1"></a>All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (@sec-terminator) signals that the budget is exhausted.</span>
<span id="cb74-240"><a href="#cb74-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-241"><a href="#cb74-241" aria-hidden="true" tabindex="-1"></a>Just like learners, tuners also have parameters, known as <span class="in">`r define("control parameters")`</span>, which (as the name suggests) controls the behavior of the tuners.</span>
<span id="cb74-242"><a href="#cb74-242" aria-hidden="true" tabindex="-1"></a>Unlike learners, default values for control parameters usually give good results and these rarely need to be changed.</span>
<span id="cb74-243"><a href="#cb74-243" aria-hidden="true" tabindex="-1"></a>Control parameters are stored in the <span class="in">`$param_set`</span> field.</span>
<span id="cb74-244"><a href="#cb74-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-245"><a href="#cb74-245" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-009}</span></span>
<span id="cb74-246"><a href="#cb74-246" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span>param_set</span>
<span id="cb74-247"><a href="#cb74-247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-248"><a href="#cb74-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-249"><a href="#cb74-249" aria-hidden="true" tabindex="-1"></a><span class="fu">### Trigger the Tuning</span></span>
<span id="cb74-250"><a href="#cb74-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-251"><a href="#cb74-251" aria-hidden="true" tabindex="-1"></a>Now we have all our components, we are ready to start tuning! To do this we simply pass the constructed <span class="in">`r ref("TuningInstanceSingleCrit")`</span> to the <span class="in">`$optimize()`</span> method of the initialized <span class="in">`r ref("Tuner")`</span>.</span>
<span id="cb74-252"><a href="#cb74-252" aria-hidden="true" tabindex="-1"></a>The tuner then proceeds with the HPO loop we discussed at the beginning of the chapter (@fig-optimization-loop).</span>
<span id="cb74-253"><a href="#cb74-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-254"><a href="#cb74-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-010}</span></span>
<span id="cb74-255"><a href="#cb74-255" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb74-256"><a href="#cb74-256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-257"><a href="#cb74-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-258"><a href="#cb74-258" aria-hidden="true" tabindex="-1"></a>The optimizer returns the best hyperparameter configuration and the corresponding measured performance.</span>
<span id="cb74-259"><a href="#cb74-259" aria-hidden="true" tabindex="-1"></a>This information is also stored in <span class="in">`instance$result`</span>.</span>
<span id="cb74-260"><a href="#cb74-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-261"><a href="#cb74-261" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb74-262"><a href="#cb74-262" aria-hidden="true" tabindex="-1"></a>The column <span class="in">`x_domain`</span> contains transformed values and <span class="in">`learner_param_vals`</span> optional constants (none in this example).</span>
<span id="cb74-263"><a href="#cb74-263" aria-hidden="true" tabindex="-1"></a>See section @sec-advanced-search-spaces for more information.</span>
<span id="cb74-264"><a href="#cb74-264" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb74-265"><a href="#cb74-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-266"><a href="#cb74-266" aria-hidden="true" tabindex="-1"></a><span class="fu">### Quick Tuning with `tune` {#sec-simplified-tuning}</span></span>
<span id="cb74-267"><a href="#cb74-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-268"><a href="#cb74-268" aria-hidden="true" tabindex="-1"></a>In the previous section, we looked at creating a tuning instance manually using <span class="in">`r ref("ti()")`</span>, which offers more control over the tuning process.</span>
<span id="cb74-269"><a href="#cb74-269" aria-hidden="true" tabindex="-1"></a>However, you can also simplify this (albeit with slightly less control) using the <span class="in">`r ref("tune()")`</span> sugar function.</span>
<span id="cb74-270"><a href="#cb74-270" aria-hidden="true" tabindex="-1"></a>Internally this creates a <span class="in">`r ref("TuningInstanceSingleCrit")`</span>, starts the tuning and returns the result with the instance.</span>
<span id="cb74-271"><a href="#cb74-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-272"><a href="#cb74-272" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-012}</span></span>
<span id="cb74-273"><a href="#cb74-273" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb74-274"><a href="#cb74-274" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb74-275"><a href="#cb74-275" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>),</span>
<span id="cb74-276"><a href="#cb74-276" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb74-277"><a href="#cb74-277" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb74-278"><a href="#cb74-278" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-279"><a href="#cb74-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-280"><a href="#cb74-280" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb74-281"><a href="#cb74-281" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb74-282"><a href="#cb74-282" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb74-283"><a href="#cb74-283" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb74-284"><a href="#cb74-284" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb74-285"><a href="#cb74-285" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb74-286"><a href="#cb74-286" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-287"><a href="#cb74-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-288"><a href="#cb74-288" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result</span>
<span id="cb74-289"><a href="#cb74-289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-290"><a href="#cb74-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-291"><a href="#cb74-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### Analyzing the Result {#sec-analyzing-result}</span></span>
<span id="cb74-292"><a href="#cb74-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-293"><a href="#cb74-293" aria-hidden="true" tabindex="-1"></a>Whether you use <span class="in">`r ref("ti")`</span> or <span class="in">`r ref("tune")`</span> the output is the same and the 'archive' lists all evaluated hyperparameter configurations:</span>
<span id="cb74-294"><a href="#cb74-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-295"><a href="#cb74-295" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-013}</span></span>
<span id="cb74-296"><a href="#cb74-296" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, <span class="fu">list</span>(cost, gamma, classif.ce)]</span>
<span id="cb74-297"><a href="#cb74-297" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-298"><a href="#cb74-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-299"><a href="#cb74-299" aria-hidden="true" tabindex="-1"></a>Each row of the archive is a different evaluated configuration (there are 25 rows in total in the full <span class="in">`data.table`</span>).</span>
<span id="cb74-300"><a href="#cb74-300" aria-hidden="true" tabindex="-1"></a>The columns here show the tested configurations, the measure we optimize,</span>
<span id="cb74-301"><a href="#cb74-301" aria-hidden="true" tabindex="-1"></a>the completed configuration time stamp, and the total train and predict times.</span>
<span id="cb74-302"><a href="#cb74-302" aria-hidden="true" tabindex="-1"></a>If we only specify a single-objective criterium then the instance will return the configuration that optimizes this measure however we can manually inspect the archive to determine other important features.</span>
<span id="cb74-303"><a href="#cb74-303" aria-hidden="true" tabindex="-1"></a>For example, how long did the model take to run? Were there any errors in running?</span>
<span id="cb74-304"><a href="#cb74-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-305"><a href="#cb74-305" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-014}</span></span>
<span id="cb74-306"><a href="#cb74-306" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[,</span>
<span id="cb74-307"><a href="#cb74-307" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(timestamp, runtime_learners, errors, warnings)]</span>
<span id="cb74-308"><a href="#cb74-308" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-309"><a href="#cb74-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-310"><a href="#cb74-310" aria-hidden="true" tabindex="-1"></a>Now we see not only was our optimal configuration the best performing with respect to classification error, but also it had the fastest runtime.</span>
<span id="cb74-311"><a href="#cb74-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-312"><a href="#cb74-312" aria-hidden="true" tabindex="-1"></a>Another powerful feature of the instance is that we can score the internal <span class="in">`r ref("ResampleResult")`</span>s on a different performance measure, for example looking at false negative rate (FNR) and false positive rate (FPR) as well as classification error:</span>
<span id="cb74-313"><a href="#cb74-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-314"><a href="#cb74-314" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-015}</span></span>
<span id="cb74-315"><a href="#cb74-315" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive,</span>
<span id="cb74-316"><a href="#cb74-316" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.fpr"</span>, <span class="st">"classif.fnr"</span>)))[,</span>
<span id="cb74-317"><a href="#cb74-317" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(cost, gamma, classif.ce, classif.fpr, classif.fnr)]</span>
<span id="cb74-318"><a href="#cb74-318" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-319"><a href="#cb74-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-320"><a href="#cb74-320" aria-hidden="true" tabindex="-1"></a>Now we see our model is also the best performing with respect to FPR and FNR!</span>
<span id="cb74-321"><a href="#cb74-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-322"><a href="#cb74-322" aria-hidden="true" tabindex="-1"></a>You can view all the resamplings in a <span class="in">`r ref("BenchmarkResult")`</span> object with <span class="in">`instance$archive$benchmark_result`</span>.</span>
<span id="cb74-323"><a href="#cb74-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-324"><a href="#cb74-324" aria-hidden="true" tabindex="-1"></a>Finally, for more visually appealing results you can use <span class="in">`r mlr3viz`</span> (@fig-surface).</span>
<span id="cb74-325"><a href="#cb74-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-326"><a href="#cb74-326" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-016}</span></span>
<span id="cb74-327"><a href="#cb74-327" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-surface</span></span>
<span id="cb74-328"><a href="#cb74-328" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high `cost` values and `gamma` values around `exp(-5)` achieve the best performance.</span></span>
<span id="cb74-329"><a href="#cb74-329" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Heatmap showing model performance during HPO. y-axis is 'gamma' parameter between (-10,10) and x-axis is 'cost' parameter between (-10,10). The heatmap shows squares covering all points on the plot and circular points indicating configurations tried in our optimisation. The top-left quadrant is all yellow indicating poor performance when gamma is high and cost is low. The bottom-right is dark blue indicating good performance when cost is high and gamme is low.</span></span>
<span id="cb74-330"><a href="#cb74-330" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(instance, <span class="at">type =</span> <span class="st">"surface"</span>)</span>
<span id="cb74-331"><a href="#cb74-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-332"><a href="#cb74-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-333"><a href="#cb74-333" aria-hidden="true" tabindex="-1"></a><span class="fu">### Using a tuned model {#sec-final-model}</span></span>
<span id="cb74-334"><a href="#cb74-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-335"><a href="#cb74-335" aria-hidden="true" tabindex="-1"></a>Once the learner has been tuned we can start to use it like any other model in the mlr3 universe.</span>
<span id="cb74-336"><a href="#cb74-336" aria-hidden="true" tabindex="-1"></a>To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters with the optimal configurations:</span>
<span id="cb74-337"><a href="#cb74-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-338"><a href="#cb74-338" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-018}</span></span>
<span id="cb74-339"><a href="#cb74-339" aria-hidden="true" tabindex="-1"></a>svm_tuned <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>, <span class="at">id =</span> <span class="st">"SVM Tuned"</span>)</span>
<span id="cb74-340"><a href="#cb74-340" aria-hidden="true" tabindex="-1"></a>svm_tuned<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> instance<span class="sc">$</span>result_learner_param_vals</span>
<span id="cb74-341"><a href="#cb74-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-342"><a href="#cb74-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-343"><a href="#cb74-343" aria-hidden="true" tabindex="-1"></a>Now we can train the learner on the full dataset and we are ready to make predictions.</span>
<span id="cb74-344"><a href="#cb74-344" aria-hidden="true" tabindex="-1"></a>The trained model can then be used to predict new, external data:</span>
<span id="cb74-345"><a href="#cb74-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-346"><a href="#cb74-346" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-019}</span></span>
<span id="cb74-347"><a href="#cb74-347" aria-hidden="true" tabindex="-1"></a>svm_tuned<span class="sc">$</span><span class="fu">train</span>(<span class="fu">tsk</span>(<span class="st">"sonar"</span>))</span>
<span id="cb74-348"><a href="#cb74-348" aria-hidden="true" tabindex="-1"></a>svm_tuned<span class="sc">$</span>model</span>
<span id="cb74-349"><a href="#cb74-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-350"><a href="#cb74-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-351"><a href="#cb74-351" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb74-352"><a href="#cb74-352" aria-hidden="true" tabindex="-1"></a>A common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (<span class="in">`instance$result$classif.ce`</span>) as the model's performance.</span>
<span id="cb74-353"><a href="#cb74-353" aria-hidden="true" tabindex="-1"></a>However, doing so would lead to bias and therefore nested resampling is required (@sec-nested-resampling).</span>
<span id="cb74-354"><a href="#cb74-354" aria-hidden="true" tabindex="-1"></a>Therefore when tuning as above ensure that you do not make any statements about model performance without testing the model on more unseen data.</span>
<span id="cb74-355"><a href="#cb74-355" aria-hidden="true" tabindex="-1"></a>We will come back to this in more detail in @sec-autotuner.</span>
<span id="cb74-356"><a href="#cb74-356" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb74-357"><a href="#cb74-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-358"><a href="#cb74-358" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Tuning</span></span>
<span id="cb74-359"><a href="#cb74-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-360"><a href="#cb74-360" aria-hidden="true" tabindex="-1"></a><span class="fu">### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}</span></span>
<span id="cb74-361"><a href="#cb74-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-362"><a href="#cb74-362" aria-hidden="true" tabindex="-1"></a>So far, we have only looked at the case where no issues occur.</span>
<span id="cb74-363"><a href="#cb74-363" aria-hidden="true" tabindex="-1"></a>However, it often happens that learners with certain configurations do not converge, run out of memory, or terminate with an error.</span>
<span id="cb74-364"><a href="#cb74-364" aria-hidden="true" tabindex="-1"></a>We can protect the tuning process from failing learners with encapsulation.</span>
<span id="cb74-365"><a href="#cb74-365" aria-hidden="true" tabindex="-1"></a>The encapsulation separates the tuning from the training of the individual learner.</span>
<span id="cb74-366"><a href="#cb74-366" aria-hidden="true" tabindex="-1"></a>The encapsulation method is set in the learner.</span>
<span id="cb74-367"><a href="#cb74-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-368"><a href="#cb74-368" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-020}</span></span>
<span id="cb74-369"><a href="#cb74-369" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>encapsulate <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"evaluate"</span>, <span class="at">predict =</span> <span class="st">"evaluate"</span>)</span>
<span id="cb74-370"><a href="#cb74-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-371"><a href="#cb74-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-372"><a href="#cb74-372" aria-hidden="true" tabindex="-1"></a>The encapsulation can be set individually for training and predicting.</span>
<span id="cb74-373"><a href="#cb74-373" aria-hidden="true" tabindex="-1"></a>There are currently two options for encapsulating a learner.</span>
<span id="cb74-374"><a href="#cb74-374" aria-hidden="true" tabindex="-1"></a>The  <span class="in">`r ref_pkg("evaluate")`</span> package and the <span class="in">`r ref_pkg("callr")`</span> package.</span>
<span id="cb74-375"><a href="#cb74-375" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref_pkg("callr")`</span> package comes with more overhead because the encapsulation spawns a separate R process.</span>
<span id="cb74-376"><a href="#cb74-376" aria-hidden="true" tabindex="-1"></a>Both packages allow setting a timeout which is useful when a learner does not converge.</span>
<span id="cb74-377"><a href="#cb74-377" aria-hidden="true" tabindex="-1"></a>We set a timeout of 30 seconds.</span>
<span id="cb74-378"><a href="#cb74-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-379"><a href="#cb74-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-021}</span></span>
<span id="cb74-380"><a href="#cb74-380" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>timeout <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="dv">30</span>, <span class="at">predict =</span> <span class="dv">30</span>)</span>
<span id="cb74-381"><a href="#cb74-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-382"><a href="#cb74-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-383"><a href="#cb74-383" aria-hidden="true" tabindex="-1"></a>With encapsulation, exceptions and timeouts do not stop the tuning.</span>
<span id="cb74-384"><a href="#cb74-384" aria-hidden="true" tabindex="-1"></a>Instead, the error message is recorded and a fallback learner is fitted.</span>
<span id="cb74-385"><a href="#cb74-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-386"><a href="#cb74-386" aria-hidden="true" tabindex="-1"></a>Fallback learners allow scoring a result when no model was fitted during training.</span>
<span id="cb74-387"><a href="#cb74-387" aria-hidden="true" tabindex="-1"></a>A common approach is to predict a weak baseline e.g. predicting the mean of the data or just the majority class.</span>
<span id="cb74-388"><a href="#cb74-388" aria-hidden="true" tabindex="-1"></a>See @sec-fallback-learner for more detailed information.</span>
<span id="cb74-389"><a href="#cb74-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-390"><a href="#cb74-390" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("mlr_learners_classif.featureless", "featureless learner")`</span> predicts the most frequent label.</span>
<span id="cb74-391"><a href="#cb74-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-392"><a href="#cb74-392" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-022}</span></span>
<span id="cb74-393"><a href="#cb74-393" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>fallback <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.featureless"</span>)</span>
<span id="cb74-394"><a href="#cb74-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-395"><a href="#cb74-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-396"><a href="#cb74-396" aria-hidden="true" tabindex="-1"></a>Errors and warnings that occurred during tuning are stored in the archive.</span>
<span id="cb74-397"><a href="#cb74-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-398"><a href="#cb74-398" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-023}</span></span>
<span id="cb74-399"><a href="#cb74-399" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, <span class="fu">list</span>(cost, gamma, classif.ce, errors, warnings)]</span>
<span id="cb74-400"><a href="#cb74-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-401"><a href="#cb74-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-402"><a href="#cb74-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### Advanced Search Spaces {#sec-advanced-search-spaces}</span></span>
<span id="cb74-403"><a href="#cb74-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-404"><a href="#cb74-404" aria-hidden="true" tabindex="-1"></a>Usually, the <span class="in">`cost`</span> and <span class="in">`gamma`</span> hyperparameters are tuned on the logarithmic scale which means the optimization algorithm searches in $<span class="co">[</span><span class="ot">log(1e-5), log(1e5)</span><span class="co">]</span>$ but transforms the selected configuration with <span class="in">`exp()`</span> before passing to the learner.</span>
<span id="cb74-405"><a href="#cb74-405" aria-hidden="true" tabindex="-1"></a>Using the log transformation emphasizes smaller values but can also result in large values.</span>
<span id="cb74-406"><a href="#cb74-406" aria-hidden="true" tabindex="-1"></a>The code below demonstrates this more clearly.</span>
<span id="cb74-407"><a href="#cb74-407" aria-hidden="true" tabindex="-1"></a>The histograms show how the algorithm searches within a narrow range but exponentiating then results in the majority of points being relatively small but a few being very large.</span>
<span id="cb74-408"><a href="#cb74-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-409"><a href="#cb74-409" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-005gen}</span></span>
<span id="cb74-410"><a href="#cb74-410" aria-hidden="true" tabindex="-1"></a>cost <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">1000</span>, <span class="fu">log</span>(<span class="fl">1e-5</span>), <span class="fu">log</span>(<span class="fl">1e5</span>))</span>
<span id="cb74-411"><a href="#cb74-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-412"><a href="#cb74-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-415"><a href="#cb74-415" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb74-416"><a href="#cb74-416" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb74-417"><a href="#cb74-417" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-logscale</span></span>
<span id="cb74-418"><a href="#cb74-418" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Histogram of sampled `cost` values.</span></span>
<span id="cb74-419"><a href="#cb74-419" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb74-420"><a href="#cb74-420" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "`cost` values sampled by the optimization algorithm."</span></span>
<span id="cb74-421"><a href="#cb74-421" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "`exp(cost)` values seen by the learner."</span></span>
<span id="cb74-422"><a href="#cb74-422" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb74-423"><a href="#cb74-423" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb74-424"><a href="#cb74-424" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb74-425"><a href="#cb74-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-426"><a href="#cb74-426" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">cost =</span> cost)</span>
<span id="cb74-427"><a href="#cb74-427" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> cost)) <span class="sc">+</span></span>
<span id="cb74-428"><a href="#cb74-428" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb74-429"><a href="#cb74-429" aria-hidden="true" tabindex="-1"></a>    <span class="at">bins =</span> <span class="dv">15</span>,</span>
<span id="cb74-430"><a href="#cb74-430" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">1</span>, <span class="at">begin =</span> <span class="fl">0.5</span>),</span>
<span id="cb74-431"><a href="#cb74-431" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb74-432"><a href="#cb74-432" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb74-433"><a href="#cb74-433" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb74-434"><a href="#cb74-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-435"><a href="#cb74-435" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">cost =</span> <span class="fu">exp</span>(cost))</span>
<span id="cb74-436"><a href="#cb74-436" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> cost)) <span class="sc">+</span></span>
<span id="cb74-437"><a href="#cb74-437" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb74-438"><a href="#cb74-438" aria-hidden="true" tabindex="-1"></a>    <span class="at">bins =</span> <span class="dv">15</span>,</span>
<span id="cb74-439"><a href="#cb74-439" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">1</span>, <span class="at">begin =</span> <span class="fl">0.5</span>),</span>
<span id="cb74-440"><a href="#cb74-440" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb74-441"><a href="#cb74-441" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb74-442"><a href="#cb74-442" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb74-443"><a href="#cb74-443" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-444"><a href="#cb74-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-445"><a href="#cb74-445" aria-hidden="true" tabindex="-1"></a>To add the <span class="in">`exp()`</span> transformation to a hyperparameter, we pass <span class="in">`logscale = TRUE`</span> to <span class="in">`to_tune()`</span>.</span>
<span id="cb74-446"><a href="#cb74-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-449"><a href="#cb74-449" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb74-450"><a href="#cb74-450" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb74-451"><a href="#cb74-451" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb74-452"><a href="#cb74-452" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb74-453"><a href="#cb74-453" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb74-454"><a href="#cb74-454" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb74-455"><a href="#cb74-455" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-456"><a href="#cb74-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-457"><a href="#cb74-457" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb74-458"><a href="#cb74-458" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb74-459"><a href="#cb74-459" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb74-460"><a href="#cb74-460" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb74-461"><a href="#cb74-461" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb74-462"><a href="#cb74-462" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb74-463"><a href="#cb74-463" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-464"><a href="#cb74-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-465"><a href="#cb74-465" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result</span>
<span id="cb74-466"><a href="#cb74-466" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-467"><a href="#cb74-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-468"><a href="#cb74-468" aria-hidden="true" tabindex="-1"></a>The column <span class="in">`x_domain`</span> contains the hyperparameter values after the transformation i.e. <span class="in">`exp(5.76)`</span> and <span class="in">`exp(-5.76)`</span>:</span>
<span id="cb74-469"><a href="#cb74-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-470"><a href="#cb74-470" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-011}</span></span>
<span id="cb74-471"><a href="#cb74-471" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result<span class="sc">$</span>x_domain</span>
<span id="cb74-472"><a href="#cb74-472" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-473"><a href="#cb74-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-474"><a href="#cb74-474" aria-hidden="true" tabindex="-1"></a><span class="fu">### Search Spaces Collection {#sec-tuning-spaces}</span></span>
<span id="cb74-475"><a href="#cb74-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-476"><a href="#cb74-476" aria-hidden="true" tabindex="-1"></a>Selected search spaces can require a lot of background knowledge or expertise.</span>
<span id="cb74-477"><a href="#cb74-477" aria-hidden="true" tabindex="-1"></a>The package <span class="in">`r ref_pkg("mlr3tuningspaces")`</span> tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms.</span>
<span id="cb74-478"><a href="#cb74-478" aria-hidden="true" tabindex="-1"></a>These search spaces should be applicable to a wide range of data sets, however, they may need to be adapted in specific situations.</span>
<span id="cb74-479"><a href="#cb74-479" aria-hidden="true" tabindex="-1"></a>The search spaces are stored in the dictionary <span class="in">`r ref("mlr_tuning_spaces")`</span>.</span>
<span id="cb74-480"><a href="#cb74-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-481"><a href="#cb74-481" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-024}</span></span>
<span id="cb74-482"><a href="#cb74-482" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_tuning_spaces)</span>
<span id="cb74-483"><a href="#cb74-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-484"><a href="#cb74-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-485"><a href="#cb74-485" aria-hidden="true" tabindex="-1"></a>The tuning spaces are named according to the scheme <span class="in">`{learner-id}.{publication}`</span>.</span>
<span id="cb74-486"><a href="#cb74-486" aria-hidden="true" tabindex="-1"></a>The sugar function <span class="in">`r ref("lts()")`</span> is used to retrieve a <span class="in">`r ref("TuningSpace")`</span>.</span>
<span id="cb74-487"><a href="#cb74-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-488"><a href="#cb74-488" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-025}</span></span>
<span id="cb74-489"><a href="#cb74-489" aria-hidden="true" tabindex="-1"></a><span class="fu">lts</span>(<span class="st">"classif.rpart.default"</span>)</span>
<span id="cb74-490"><a href="#cb74-490" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-491"><a href="#cb74-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-492"><a href="#cb74-492" aria-hidden="true" tabindex="-1"></a>A tuning space can be passed to <span class="in">`ti()`</span> as the <span class="in">`search_space`</span>.</span>
<span id="cb74-493"><a href="#cb74-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-494"><a href="#cb74-494" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-026}</span></span>
<span id="cb74-495"><a href="#cb74-495" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb74-496"><a href="#cb74-496" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb74-497"><a href="#cb74-497" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb74-498"><a href="#cb74-498" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb74-499"><a href="#cb74-499" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb74-500"><a href="#cb74-500" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>),</span>
<span id="cb74-501"><a href="#cb74-501" aria-hidden="true" tabindex="-1"></a>  <span class="at">search_space =</span> <span class="fu">lts</span>(<span class="st">"classif.rpart.rbv2"</span>)</span>
<span id="cb74-502"><a href="#cb74-502" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-503"><a href="#cb74-503" aria-hidden="true" tabindex="-1"></a>instance</span>
<span id="cb74-504"><a href="#cb74-504" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-505"><a href="#cb74-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-506"><a href="#cb74-506" aria-hidden="true" tabindex="-1"></a>Alternatively, we can explicitly set the search space of a learner with <span class="in">`r ref("TuneToken", "TuneTokens")`</span></span>
<span id="cb74-507"><a href="#cb74-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-508"><a href="#cb74-508" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-027}</span></span>
<span id="cb74-509"><a href="#cb74-509" aria-hidden="true" tabindex="-1"></a>vals <span class="ot">=</span> <span class="fu">lts</span>(<span class="st">"classif.rpart.default"</span>)<span class="sc">$</span>values</span>
<span id="cb74-510"><a href="#cb74-510" aria-hidden="true" tabindex="-1"></a>vals</span>
<span id="cb74-511"><a href="#cb74-511" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb74-512"><a href="#cb74-512" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(<span class="at">.values =</span> vals)</span>
<span id="cb74-513"><a href="#cb74-513" aria-hidden="true" tabindex="-1"></a>learner</span>
<span id="cb74-514"><a href="#cb74-514" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-515"><a href="#cb74-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-516"><a href="#cb74-516" aria-hidden="true" tabindex="-1"></a>When passing a learner to <span class="in">`r ref("lts()")`</span>, the default search space from the @hpo_practical article is applied.</span>
<span id="cb74-517"><a href="#cb74-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-518"><a href="#cb74-518" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-028}</span></span>
<span id="cb74-519"><a href="#cb74-519" aria-hidden="true" tabindex="-1"></a><span class="fu">lts</span>(<span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span>
<span id="cb74-520"><a href="#cb74-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-521"><a href="#cb74-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-522"><a href="#cb74-522" aria-hidden="true" tabindex="-1"></a>It is possible to simply overwrite a predefined tuning space in construction, for example here we change the range of the <span class="in">`nrounds`</span> hyperparameter in XGBoost:</span>
<span id="cb74-523"><a href="#cb74-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-524"><a href="#cb74-524" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-029}</span></span>
<span id="cb74-525"><a href="#cb74-525" aria-hidden="true" tabindex="-1"></a><span class="fu">lts</span>(<span class="st">"classif.xgboost.rbv2"</span>, <span class="at">nrounds =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">1024</span>))</span>
<span id="cb74-526"><a href="#cb74-526" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-527"><a href="#cb74-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-528"><a href="#cb74-528" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Objective Tuning {#sec-multi-metrics-tuning}</span></span>
<span id="cb74-529"><a href="#cb74-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-530"><a href="#cb74-530" aria-hidden="true" tabindex="-1"></a>So far we have considered optimizing a model with respect to one metric but multi-metric, or <span class="in">`r define("multi-objective optimization")`</span> is also possible.</span>
<span id="cb74-531"><a href="#cb74-531" aria-hidden="true" tabindex="-1"></a>A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive and false negative predictions.</span>
<span id="cb74-532"><a href="#cb74-532" aria-hidden="true" tabindex="-1"></a>In a more complex example, consider the problem of deploying a classifier in a healthcare setting, there is clearly an ethical argument to tune the model to make the best possible predictions, however in machine learning this can often lead to models that are harder to interpret (think about deep neural networks!).</span>
<span id="cb74-533"><a href="#cb74-533" aria-hidden="true" tabindex="-1"></a>In this case, we may be interested in minimizing *both*  classification error (for example) and complexity.</span>
<span id="cb74-534"><a href="#cb74-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-535"><a href="#cb74-535" aria-hidden="true" tabindex="-1"></a>In general, when optimizing multiple metrics, these will be in competition (if they were not we would only need to optimize with respect to one of them!) and so no single configuration exists that optimizes all metrics.</span>
<span id="cb74-536"><a href="#cb74-536" aria-hidden="true" tabindex="-1"></a>Focus is therefore given to the concept of <span class="in">`r index("Pareto optimality")`</span>.</span>
<span id="cb74-537"><a href="#cb74-537" aria-hidden="true" tabindex="-1"></a>One hyperparameter configuration is said to <span class="in">`r index("Pareto-dominate")`</span> another one if the resulting model is equal or better in all metrics and strictly better in at least one metric.</span>
<span id="cb74-538"><a href="#cb74-538" aria-hidden="true" tabindex="-1"></a>All configurations that are not Pareto-dominated are referred to as Pareto efficient and the set of all these configurations is referred to as the <span class="in">`r define("Pareto front")`</span> (@fig-pareto).</span>
<span id="cb74-539"><a href="#cb74-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-540"><a href="#cb74-540" aria-hidden="true" tabindex="-1"></a>The goal of multi-objective HPO is to approximate the true, unknown Pareto front.</span>
<span id="cb74-541"><a href="#cb74-541" aria-hidden="true" tabindex="-1"></a>More methodological details on multi-objective HPO can be found in @hpo_multi.</span>
<span id="cb74-542"><a href="#cb74-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-543"><a href="#cb74-543" aria-hidden="true" tabindex="-1"></a>We will now demonstrate multi-objective HPO by tuning a decision tree on the <span class="in">`r ref("mlr_tasks_spam", "Spam")`</span> data set with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).</span>
<span id="cb74-544"><a href="#cb74-544" aria-hidden="true" tabindex="-1"></a>We will tune</span>
<span id="cb74-545"><a href="#cb74-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-546"><a href="#cb74-546" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The complexity hyperparameter <span class="in">`cp`</span> that controls when the learner considers introducing another branch.</span>
<span id="cb74-547"><a href="#cb74-547" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The <span class="in">`minsplit`</span> hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.</span>
<span id="cb74-548"><a href="#cb74-548" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The <span class="in">`maxdepth`</span> hyperparameter that limits the depth of the tree.</span>
<span id="cb74-549"><a href="#cb74-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-550"><a href="#cb74-550" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-031}</span></span>
<span id="cb74-551"><a href="#cb74-551" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>,</span>
<span id="cb74-552"><a href="#cb74-552" aria-hidden="true" tabindex="-1"></a>  <span class="at">cp =</span> <span class="fu">to_tune</span>(<span class="fl">1e-04</span>, <span class="fl">1e-1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb74-553"><a href="#cb74-553" aria-hidden="true" tabindex="-1"></a>  <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">128</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb74-554"><a href="#cb74-554" aria-hidden="true" tabindex="-1"></a>  <span class="at">maxdepth =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb74-555"><a href="#cb74-555" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-556"><a href="#cb74-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-557"><a href="#cb74-557" aria-hidden="true" tabindex="-1"></a>measures <span class="ot">=</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span>))</span>
<span id="cb74-558"><a href="#cb74-558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-559"><a href="#cb74-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-560"><a href="#cb74-560" aria-hidden="true" tabindex="-1"></a>Note that as we tune with respect to multiple measures, the function <span class="in">`ti`</span> creates a <span class="in">`r ref("TuningInstanceMultiCrit")`</span> instead of a <span class="in">`r ref("TuningInstanceSingleCrit")`</span>.</span>
<span id="cb74-561"><a href="#cb74-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-562"><a href="#cb74-562" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-033}</span></span>
<span id="cb74-563"><a href="#cb74-563" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb74-564"><a href="#cb74-564" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"spam"</span>),</span>
<span id="cb74-565"><a href="#cb74-565" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb74-566"><a href="#cb74-566" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb74-567"><a href="#cb74-567" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> measures,</span>
<span id="cb74-568"><a href="#cb74-568" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>),</span>
<span id="cb74-569"><a href="#cb74-569" aria-hidden="true" tabindex="-1"></a>  <span class="at">store_models =</span> <span class="cn">TRUE</span>  <span class="co"># required to inspect selected_features</span></span>
<span id="cb74-570"><a href="#cb74-570" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-571"><a href="#cb74-571" aria-hidden="true" tabindex="-1"></a>instance</span>
<span id="cb74-572"><a href="#cb74-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-573"><a href="#cb74-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-574"><a href="#cb74-574" aria-hidden="true" tabindex="-1"></a>As before we will then select and run a tuning algorithm, here we use random search:</span>
<span id="cb74-575"><a href="#cb74-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-576"><a href="#cb74-576" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-034,output=FALSE}</span></span>
<span id="cb74-577"><a href="#cb74-577" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">20</span>)</span>
<span id="cb74-578"><a href="#cb74-578" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb74-579"><a href="#cb74-579" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-580"><a href="#cb74-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-581"><a href="#cb74-581" aria-hidden="true" tabindex="-1"></a>Finally, we inspect the best-performing configurations, i.e., the Pareto set.</span>
<span id="cb74-582"><a href="#cb74-582" aria-hidden="true" tabindex="-1"></a>And then inspect the estimated Pareto set and visualize the estimated Pareto front:</span>
<span id="cb74-583"><a href="#cb74-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-584"><a href="#cb74-584" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-035}</span></span>
<span id="cb74-585"><a href="#cb74-585" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()[, <span class="fu">list</span>(cp, minsplit, maxdepth, classif.ce, selected_features)]</span>
<span id="cb74-586"><a href="#cb74-586" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-587"><a href="#cb74-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-588"><a href="#cb74-588" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-036}</span></span>
<span id="cb74-589"><a href="#cb74-589" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pareto</span></span>
<span id="cb74-590"><a href="#cb74-590" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pareto front of selected features and classification error. Black dots represent tested configurations, each red dot individually represents a Pareto-optimal configuration and all red dots together represent the Pareto front.</span></span>
<span id="cb74-591"><a href="#cb74-591" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Black dots represent simulated tested configurations of selected_features vs. classif.ce and red dots and a red line along the bottom-left of the plot shows the Pareto front.</span></span>
<span id="cb74-592"><a href="#cb74-592" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb74-593"><a href="#cb74-593" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb74-594"><a href="#cb74-594" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb74-595"><a href="#cb74-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-596"><a href="#cb74-596" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(), <span class="fu">aes</span>(<span class="at">x =</span> selected_features, <span class="at">y =</span> classif.ce)) <span class="sc">+</span></span>
<span id="cb74-597"><a href="#cb74-597" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(</span>
<span id="cb74-598"><a href="#cb74-598" aria-hidden="true" tabindex="-1"></a>    <span class="at">direction =</span> <span class="st">"vh"</span>,</span>
<span id="cb74-599"><a href="#cb74-599" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">viridis</span>(<span class="dv">1</span>, <span class="at">begin =</span> <span class="fl">0.5</span>),</span>
<span id="cb74-600"><a href="#cb74-600" aria-hidden="true" tabindex="-1"></a>    <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb74-601"><a href="#cb74-601" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb74-602"><a href="#cb74-602" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb74-603"><a href="#cb74-603" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb74-604"><a href="#cb74-604" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">1</span>, <span class="at">begin =</span> <span class="fl">0.33</span>),</span>
<span id="cb74-605"><a href="#cb74-605" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb74-606"><a href="#cb74-606" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb74-607"><a href="#cb74-607" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb74-608"><a href="#cb74-608" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive),</span>
<span id="cb74-609"><a href="#cb74-609" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb74-610"><a href="#cb74-610" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb74-611"><a href="#cb74-611" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">1</span>, <span class="at">begin =</span> <span class="fl">0.66</span>),</span>
<span id="cb74-612"><a href="#cb74-612" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb74-613"><a href="#cb74-613" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb74-614"><a href="#cb74-614" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb74-615"><a href="#cb74-615" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-616"><a href="#cb74-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-617"><a href="#cb74-617" aria-hidden="true" tabindex="-1"></a><span class="fu">## Automated Tuning with `AutoTuner` {#sec-autotuner}</span></span>
<span id="cb74-618"><a href="#cb74-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-619"><a href="#cb74-619" aria-hidden="true" tabindex="-1"></a>One of the most powerful classes in mlr3 is the <span class="in">`r ref("AutoTuner")`</span>.</span>
<span id="cb74-620"><a href="#cb74-620" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("AutoTuner")`</span> wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters -- this allows transparent tuning of any learner, without the need to extract information on the best hyperparameter settings at the end.</span>
<span id="cb74-621"><a href="#cb74-621" aria-hidden="true" tabindex="-1"></a>As the <span class="in">`r ref("AutoTuner")`</span> itself inherits from the <span class="in">`r ref("Learner")`</span> base class, it can be used like any other learner!</span>
<span id="cb74-622"><a href="#cb74-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-623"><a href="#cb74-623" aria-hidden="true" tabindex="-1"></a>Let us see this in practice.</span>
<span id="cb74-624"><a href="#cb74-624" aria-hidden="true" tabindex="-1"></a>We will run the exact same example as above but this time using the <span class="in">`r ref("AutoTuner")`</span> for automated tuning:</span>
<span id="cb74-625"><a href="#cb74-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-626"><a href="#cb74-626" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-038}</span></span>
<span id="cb74-627"><a href="#cb74-627" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb74-628"><a href="#cb74-628" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb74-629"><a href="#cb74-629" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb74-630"><a href="#cb74-630" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb74-631"><a href="#cb74-631" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb74-632"><a href="#cb74-632" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-633"><a href="#cb74-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-634"><a href="#cb74-634" aria-hidden="true" tabindex="-1"></a>at <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb74-635"><a href="#cb74-635" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>, <span class="at">batch_size =</span> <span class="dv">5</span>),</span>
<span id="cb74-636"><a href="#cb74-636" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb74-637"><a href="#cb74-637" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb74-638"><a href="#cb74-638" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb74-639"><a href="#cb74-639" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-640"><a href="#cb74-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-641"><a href="#cb74-641" aria-hidden="true" tabindex="-1"></a>at</span>
<span id="cb74-642"><a href="#cb74-642" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-643"><a href="#cb74-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-644"><a href="#cb74-644" aria-hidden="true" tabindex="-1"></a>We can now use this like any other learner, calling the <span class="in">`$train()`</span> and <span class="in">`$predict()`</span> methods.</span>
<span id="cb74-645"><a href="#cb74-645" aria-hidden="true" tabindex="-1"></a>The key difference to a normal learner, is that calling <span class="in">`$train()`</span> also tunes the model.</span>
<span id="cb74-646"><a href="#cb74-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-647"><a href="#cb74-647" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-039}</span></span>
<span id="cb74-648"><a href="#cb74-648" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb74-649"><a href="#cb74-649" aria-hidden="true" tabindex="-1"></a>split <span class="ot">=</span> <span class="fu">partition</span>(task)</span>
<span id="cb74-650"><a href="#cb74-650" aria-hidden="true" tabindex="-1"></a>at<span class="sc">$</span><span class="fu">train</span>(task, <span class="at">row_ids =</span> split<span class="sc">$</span>train)</span>
<span id="cb74-651"><a href="#cb74-651" aria-hidden="true" tabindex="-1"></a>at<span class="sc">$</span><span class="fu">predict</span>(task, <span class="at">row_ids =</span> split<span class="sc">$</span>test)<span class="sc">$</span><span class="fu">score</span>()</span>
<span id="cb74-652"><a href="#cb74-652" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb74-653"><a href="#cb74-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-654"><a href="#cb74-654" aria-hidden="true" tabindex="-1"></a>We could also pass the <span class="in">`r ref("AutoTuner")`</span> to <span class="in">`r ref("resample()")`</span> and <span class="in">`r ref("benchmark()")`</span>, which would result in a nested resampling (@sec-nested-resampling), discussed next.</span>
<span id="cb74-655"><a href="#cb74-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-656"><a href="#cb74-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-657"><a href="#cb74-657" aria-hidden="true" tabindex="-1"></a><span class="fu">## Nested Resampling {#sec-nested-resampling}</span></span>
<span id="cb74-658"><a href="#cb74-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-659"><a href="#cb74-659" aria-hidden="true" tabindex="-1"></a>Hyperparameter optimization generally requires an additional layer or resampling to prevent bias in tuning.</span>
<span id="cb74-660"><a href="#cb74-660" aria-hidden="true" tabindex="-1"></a>If the same data is used for determining the optimal configuration and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased <span class="co">[</span><span class="ot">@Simon2007</span><span class="co">]</span>.</span>
<span id="cb74-661"><a href="#cb74-661" aria-hidden="true" tabindex="-1"></a>This is analogous to <span class="in">`r index("optimism of the training error")`</span> described in <span class="co">[</span><span class="ot">@james_introduction_2014</span><span class="co">]</span>, which occurs when training error is taken as an estimate of out-of-sample performance.</span>
<span id="cb74-662"><a href="#cb74-662" aria-hidden="true" tabindex="-1"></a>This bias is represented in @fig-bad-tuning which shows an algorithm being tuned on data that has been split intro training and testing data, and then the same data is used to estimate the model performance after selecting the best configuration after HPO.</span>
<span id="cb74-663"><a href="#cb74-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-664"><a href="#cb74-664" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid wrong-tuning}</span></span>
<span id="cb74-665"><a href="#cb74-665" aria-hidden="true" tabindex="-1"></a>%%| label: fig<span class="ot">-b</span>ad<span class="ot">-t</span>uning</span>
<span id="cb74-666"><a href="#cb74-666" aria-hidden="true" tabindex="-1"></a>%%| fig<span class="ot">-c</span>ap: Illustration of biased tuning. An algorithm is tuned by training on the <span class="ot">`</span><span class="st">Train</span><span class="ot">`</span> dataset <span class="ot">and</span> then the optimal configuration is selected by evaluation on the <span class="ot">`</span><span class="st">Test</span><span class="ot">`</span> data. The model<span class="ot">'</span><span class="ss">s performance is then evaluated with the optimal configuration on the same data.</span></span>
<span id="cb74-667"><a href="#cb74-667" aria-hidden="true" tabindex="-1"></a><span class="ss">%%| alt-text: Flowchart representing biased tuning. The diagram shows input </span><span class="ot">'</span>Search Space<span class="ot">'</span><span class="ss"> passed into </span><span class="ot">'</span>Algorithm<span class="ot">'</span><span class="ss"> which then has a black arrow to </span><span class="ot">'</span>Train<span class="ot">'</span><span class="ss">,then a black arrow to </span><span class="ot">'</span>Test<span class="ot">'</span><span class="ss">, then a black arrow to </span><span class="ot">'</span>Optimal configuration<span class="ot">'</span><span class="ss">. This has a red arrow back to </span><span class="ot">'</span>Algorithm<span class="ot">'</span><span class="ss"> then to </span><span class="ot">'</span>Train<span class="ot">'</span><span class="ss"> then to </span><span class="ot">'</span>Test<span class="ot">'</span><span class="ss"> then to </span><span class="ot">'</span>Performance<span class="ot">'</span><span class="ss">. The diagram clearly shows the reuse of the same training and testing data.</span></span>
<span id="cb74-668"><a href="#cb74-668" aria-hidden="true" tabindex="-1"></a><span class="ss">%%{init: { </span><span class="ot">'</span>flowchart<span class="ot">'</span><span class="ss">: {</span><span class="ot">'</span>rankSpacing<span class="ot">'</span><span class="ss">: 25}}}%%</span></span>
<span id="cb74-669"><a href="#cb74-669" aria-hidden="true" tabindex="-1"></a><span class="ss">flowchart LR</span></span>
<span id="cb74-670"><a href="#cb74-670" aria-hidden="true" tabindex="-1"></a><span class="ss">    search[(Search Space)]</span></span>
<span id="cb74-671"><a href="#cb74-671" aria-hidden="true" tabindex="-1"></a><span class="ss">    opt[(Optimal&lt;br&gt;configuration)]</span></span>
<span id="cb74-672"><a href="#cb74-672" aria-hidden="true" tabindex="-1"></a><span class="ss">    train[(Train)]</span></span>
<span id="cb74-673"><a href="#cb74-673" aria-hidden="true" tabindex="-1"></a><span class="ss">    test[(Test)]</span></span>
<span id="cb74-674"><a href="#cb74-674" aria-hidden="true" tabindex="-1"></a><span class="ss">    alg[/Algorithm/]</span></span>
<span id="cb74-675"><a href="#cb74-675" aria-hidden="true" tabindex="-1"></a><span class="ss">    search --&gt; alg</span></span>
<span id="cb74-676"><a href="#cb74-676" aria-hidden="true" tabindex="-1"></a><span class="ss">    alg --&gt; train</span></span>
<span id="cb74-677"><a href="#cb74-677" aria-hidden="true" tabindex="-1"></a><span class="ss">    alg --&gt; train</span></span>
<span id="cb74-678"><a href="#cb74-678" aria-hidden="true" tabindex="-1"></a><span class="ss">    train --&gt; test</span></span>
<span id="cb74-679"><a href="#cb74-679" aria-hidden="true" tabindex="-1"></a><span class="ss">    train --&gt; test</span></span>
<span id="cb74-680"><a href="#cb74-680" aria-hidden="true" tabindex="-1"></a><span class="ss">    test --&gt; opt</span></span>
<span id="cb74-681"><a href="#cb74-681" aria-hidden="true" tabindex="-1"></a><span class="ss">    test --&gt; perf(Performance)</span></span>
<span id="cb74-682"><a href="#cb74-682" aria-hidden="true" tabindex="-1"></a><span class="ss">    opt --&gt; alg</span></span>
<span id="cb74-683"><a href="#cb74-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-684"><a href="#cb74-684" aria-hidden="true" tabindex="-1"></a><span class="ss">    linkStyle 1,3,6,7 stroke-width:2px, stroke:red;</span></span>
<span id="cb74-685"><a href="#cb74-685" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
<span id="cb74-686"><a href="#cb74-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-687"><a href="#cb74-687" aria-hidden="true" tabindex="-1"></a><span class="ss">`r define("Nested resampling")` separates model optimization from the process of estimating the performance of the model by adding an additional layer of resampling, i.e., whilst model performance is estimated using a resampling method in the </span><span class="ot">'</span>usual way<span class="ot">'</span><span class="ss">, tuning is then performed by resampling the resampled data (@fig-nested-resampling).</span></span>
<span id="cb74-688"><a href="#cb74-688" aria-hidden="true" tabindex="-1"></a><span class="ss">For more details and a formal introduction to nested resampling the reader is referred to @hpo_practical.</span></span>
<span id="cb74-689"><a href="#cb74-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-690"><a href="#cb74-690" aria-hidden="true" tabindex="-1"></a><span class="ss">A common confusion is how and when to use nested resampling.</span></span>
<span id="cb74-691"><a href="#cb74-691" aria-hidden="true" tabindex="-1"></a><span class="ss">In the rest of this section we will answer the </span><span class="ot">'</span>how<span class="ot">'</span><span class="ss"> question but first the </span><span class="ot">'</span>when<span class="ot">'</span><span class="ss">.</span></span>
<span id="cb74-692"><a href="#cb74-692" aria-hidden="true" tabindex="-1"></a><span class="ss">A common mistake is to confuse nested resampling for model evaluation and comparison, with tuning for model deployment.</span></span>
<span id="cb74-693"><a href="#cb74-693" aria-hidden="true" tabindex="-1"></a><span class="ss">To put it differently, nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset, it is *not* a procedure to select optimal hyperparameters.</span></span>
<span id="cb74-694"><a href="#cb74-694" aria-hidden="true" tabindex="-1"></a><span class="ss">Nested resampling produces many hyperparameter configurations which should not be used to construct a final model [@Simon2007].</span></span>
<span id="cb74-695"><a href="#cb74-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-696"><a href="#cb74-696" aria-hidden="true" tabindex="-1"></a><span class="ss">```{r optimization-037}</span></span>
<span id="cb74-697"><a href="#cb74-697" aria-hidden="true" tabindex="-1"></a><span class="ss">#| label: fig-nested-resampling</span></span>
<span id="cb74-698"><a href="#cb74-698" aria-hidden="true" tabindex="-1"></a><span class="ss">#| fig-cap: An illustration of nested resampling. The green blocks represent 3-fold coss-validation for the outer resampling for model evaluation and the blue and gray blocks represent 4-fold cross-validation for the inner resampling for HPO.</span></span>
<span id="cb74-699"><a href="#cb74-699" aria-hidden="true" tabindex="-1"></a><span class="ss">#| fig-alt: The image shows three rows of blocks in light and dark green representing three-fold cross-validation for the outer resampling. Below the dark green blocks are four further rows of blue and gray blocks representing four-fold cross-validation for the inner resampling.</span></span>
<span id="cb74-700"><a href="#cb74-700" aria-hidden="true" tabindex="-1"></a><span class="ss">#| echo: false</span></span>
<span id="cb74-701"><a href="#cb74-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-702"><a href="#cb74-702" aria-hidden="true" tabindex="-1"></a><span class="ss">knitr::include_graphics("Figures/nested_resampling.png")</span></span>
<span id="cb74-703"><a href="#cb74-703" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
<span id="cb74-704"><a href="#cb74-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-705"><a href="#cb74-705" aria-hidden="true" tabindex="-1"></a><span class="ss">In words this process runs as follows:</span></span>
<span id="cb74-706"><a href="#cb74-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-707"><a href="#cb74-707" aria-hidden="true" tabindex="-1"></a><span class="ss">1. `r index("Outer resampling")` -- Instantiate 3-fold cross-validation to create different testing and training data sets.</span></span>
<span id="cb74-708"><a href="#cb74-708" aria-hidden="true" tabindex="-1"></a><span class="ss">1. `r index("Inner resampling")` -- Within the training data instantiate 4-fold cross-validation to create different inner testing and training data sets.</span></span>
<span id="cb74-709"><a href="#cb74-709" aria-hidden="true" tabindex="-1"></a><span class="ss">1. HPO -- Tune the hyperparameters using the inner data splits (blue and gray blocks).</span></span>
<span id="cb74-710"><a href="#cb74-710" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Training -- Fit the learner on the outer training data set using the optimal hyperparameter configuration obtained from the inner resampling (dark green blocks).</span></span>
<span id="cb74-711"><a href="#cb74-711" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Evaluation -- Evaluate the performance of the learner on the outer testing data (light green blocks).</span></span>
<span id="cb74-712"><a href="#cb74-712" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Cross-validation -- Repeat (2)-(5) for each of the three folds.</span></span>
<span id="cb74-713"><a href="#cb74-713" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Aggregation -- Take the sample mean of the three performance values for an unbiased performance estimate.</span></span>
<span id="cb74-714"><a href="#cb74-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-715"><a href="#cb74-715" aria-hidden="true" tabindex="-1"></a><span class="ss">That is enough theory for now, let us take a look at how this works in mlr3.</span></span>
<span id="cb74-716"><a href="#cb74-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-717"><a href="#cb74-717" aria-hidden="true" tabindex="-1"></a><span class="ss">### Nested Resampling with `AutoTuner`</span></span>
<span id="cb74-718"><a href="#cb74-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-719"><a href="#cb74-719" aria-hidden="true" tabindex="-1"></a><span class="ss">Nested resampling in mlr3 becomes quite simple with the `AutoTuner` (@sec-autotuner).</span></span>
<span id="cb74-720"><a href="#cb74-720" aria-hidden="true" tabindex="-1"></a><span class="ss"> We simply specify the inner-resampling and tuning setup with the `AutoTuner` and then pass this to `r ref("resample()")` or `r ref("benchmark()")`.</span></span>
<span id="cb74-721"><a href="#cb74-721" aria-hidden="true" tabindex="-1"></a><span class="ss"> Continuing with our previous example we will use the auto-tuner to resample a support vector classifier with 3-fold cross-validation in the outer-resampling and 4-fold cross-validation in the inner resampling.</span></span>
<span id="cb74-722"><a href="#cb74-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-723"><a href="#cb74-723" aria-hidden="true" tabindex="-1"></a><span class="ss">```{r optimization-040}</span></span>
<span id="cb74-724"><a href="#cb74-724" aria-hidden="true" tabindex="-1"></a><span class="ss">learner = lrn("classif.svm",</span></span>
<span id="cb74-725"><a href="#cb74-725" aria-hidden="true" tabindex="-1"></a><span class="ss">  cost  = to_tune(1e-5, 1e5, logscale = TRUE),</span></span>
<span id="cb74-726"><a href="#cb74-726" aria-hidden="true" tabindex="-1"></a><span class="ss">  gamma = to_tune(1e-5, 1e5, logscale = TRUE),</span></span>
<span id="cb74-727"><a href="#cb74-727" aria-hidden="true" tabindex="-1"></a><span class="ss">  kernel = "radial",</span></span>
<span id="cb74-728"><a href="#cb74-728" aria-hidden="true" tabindex="-1"></a><span class="ss">  type = "C-classification"</span></span>
<span id="cb74-729"><a href="#cb74-729" aria-hidden="true" tabindex="-1"></a><span class="ss">)</span></span>
<span id="cb74-730"><a href="#cb74-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-731"><a href="#cb74-731" aria-hidden="true" tabindex="-1"></a><span class="ss">at = auto_tuner(</span></span>
<span id="cb74-732"><a href="#cb74-732" aria-hidden="true" tabindex="-1"></a><span class="ss">  method = tnr("grid_search", resolution = 5, batch_size = 5),</span></span>
<span id="cb74-733"><a href="#cb74-733" aria-hidden="true" tabindex="-1"></a><span class="ss">  learner = learner,</span></span>
<span id="cb74-734"><a href="#cb74-734" aria-hidden="true" tabindex="-1"></a><span class="ss">  resampling = rsmp("cv", folds = 4),</span></span>
<span id="cb74-735"><a href="#cb74-735" aria-hidden="true" tabindex="-1"></a><span class="ss">  measure = msr("classif.ce"),</span></span>
<span id="cb74-736"><a href="#cb74-736" aria-hidden="true" tabindex="-1"></a><span class="ss">  term_evals = 20,</span></span>
<span id="cb74-737"><a href="#cb74-737" aria-hidden="true" tabindex="-1"></a><span class="ss">)</span></span>
<span id="cb74-738"><a href="#cb74-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-739"><a href="#cb74-739" aria-hidden="true" tabindex="-1"></a><span class="ss">task = tsk("sonar")</span></span>
<span id="cb74-740"><a href="#cb74-740" aria-hidden="true" tabindex="-1"></a><span class="ss">outer_resampling = rsmp("cv", folds = 3)</span></span>
<span id="cb74-741"><a href="#cb74-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-742"><a href="#cb74-742" aria-hidden="true" tabindex="-1"></a><span class="ss">rr = resample(task, at, outer_resampling, store_models = TRUE)</span></span>
<span id="cb74-743"><a href="#cb74-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-744"><a href="#cb74-744" aria-hidden="true" tabindex="-1"></a><span class="ss">rr</span></span>
<span id="cb74-745"><a href="#cb74-745" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
<span id="cb74-746"><a href="#cb74-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-747"><a href="#cb74-747" aria-hidden="true" tabindex="-1"></a><span class="ss">Note we set `store_models = TRUE` so that the `r ref("AutoTuner")` models are stored to investigate the inner tuning.</span></span>
<span id="cb74-748"><a href="#cb74-748" aria-hidden="true" tabindex="-1"></a><span class="ss">In this example, we utilized the same resampling strategy (K-fold cross-validation) but the mlr3 infrastructure is not limited to this, you can freely combine different inner and outer resampling strategies as you choose.</span></span>
<span id="cb74-749"><a href="#cb74-749" aria-hidden="true" tabindex="-1"></a><span class="ss">You can also mix-and-match parallelization methods for controlling the process (@sec-nested-resampling-parallelization).</span></span>
<span id="cb74-750"><a href="#cb74-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-751"><a href="#cb74-751" aria-hidden="true" tabindex="-1"></a><span class="ss">There are some special functions for nested resampling available in addition to the methods described in @sec-resampling.</span></span>
<span id="cb74-752"><a href="#cb74-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-753"><a href="#cb74-753" aria-hidden="true" tabindex="-1"></a><span class="ss">The `r ref("extract_inner_tuning_results()")` and `r ref("extract_inner_tuning_archives()")` functions return the optimal configurations (across all outer folds) and full tuning archives respectively.</span></span>
<span id="cb74-754"><a href="#cb74-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-755"><a href="#cb74-755" aria-hidden="true" tabindex="-1"></a><span class="ss">```{r optimization-042}</span></span>
<span id="cb74-756"><a href="#cb74-756" aria-hidden="true" tabindex="-1"></a><span class="ss">extract_inner_tuning_results(rr)[,</span></span>
<span id="cb74-757"><a href="#cb74-757" aria-hidden="true" tabindex="-1"></a><span class="ss">  list(iteration, cost, gamma, classif.ce)]</span></span>
<span id="cb74-758"><a href="#cb74-758" aria-hidden="true" tabindex="-1"></a><span class="ss">extract_inner_tuning_archives(rr)[,</span></span>
<span id="cb74-759"><a href="#cb74-759" aria-hidden="true" tabindex="-1"></a><span class="ss">  list(iteration, cost, gamma, classif.ce)]</span></span>
<span id="cb74-760"><a href="#cb74-760" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
<span id="cb74-761"><a href="#cb74-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-762"><a href="#cb74-762" aria-hidden="true" tabindex="-1"></a><span class="ss">From the optimal results, we observe a trend toward larger `cost` and smaller `gamma` values.</span></span>
<span id="cb74-763"><a href="#cb74-763" aria-hidden="true" tabindex="-1"></a><span class="ss">However, as we discussed earlier, these values should not be used to fit a final model as the selected hyperparameters might differ greatly between the resampling iterations.</span></span>
<span id="cb74-764"><a href="#cb74-764" aria-hidden="true" tabindex="-1"></a><span class="ss">On the one hand, this could be due to the optimization algorithm used, for example, with simple algorithms like random search, we do not expect stability of hyperparameters. On the other hand, more advanced methods like irace converge to an optimal hyperparameter configuration.</span></span>
<span id="cb74-765"><a href="#cb74-765" aria-hidden="true" tabindex="-1"></a><span class="ss">Another reason for instability in hyperparameters could be due to small data sets and/or a low number of resampling iterations (i.e., the usual small data high variance problem).</span></span>
<span id="cb74-766"><a href="#cb74-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-767"><a href="#cb74-767" aria-hidden="true" tabindex="-1"></a><span class="ss">### Performance comparison</span></span>
<span id="cb74-768"><a href="#cb74-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-769"><a href="#cb74-769" aria-hidden="true" tabindex="-1"></a><span class="ss">Finally, we will compare the predictive performances estimated on the outer resampling to the inner resampling to gain an understanding of model overfitting and general performance.</span></span>
<span id="cb74-770"><a href="#cb74-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-771"><a href="#cb74-771" aria-hidden="true" tabindex="-1"></a><span class="ss">```{r optimization-043}</span></span>
<span id="cb74-772"><a href="#cb74-772" aria-hidden="true" tabindex="-1"></a><span class="ss">extract_inner_tuning_results(rr)[,</span></span>
<span id="cb74-773"><a href="#cb74-773" aria-hidden="true" tabindex="-1"></a><span class="ss">  list(iteration, cost, gamma, classif.ce)]</span></span>
<span id="cb74-774"><a href="#cb74-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-775"><a href="#cb74-775" aria-hidden="true" tabindex="-1"></a><span class="ss">rr$score()[,</span></span>
<span id="cb74-776"><a href="#cb74-776" aria-hidden="true" tabindex="-1"></a><span class="ss">  list(iteration, classif.ce)]</span></span>
<span id="cb74-777"><a href="#cb74-777" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
<span id="cb74-778"><a href="#cb74-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-779"><a href="#cb74-779" aria-hidden="true" tabindex="-1"></a><span class="ss">Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.</span></span>
<span id="cb74-780"><a href="#cb74-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-781"><a href="#cb74-781" aria-hidden="true" tabindex="-1"></a><span class="ss">It is therefore important to ensure that the performance of a tuned model is *always* reported as the aggregated performance of all outer resampling iterations, which is an unbiased estimate of future model performance.</span></span>
<span id="cb74-782"><a href="#cb74-782" aria-hidden="true" tabindex="-1"></a><span class="ss">Note here we use the term *unbiased* to refer only to the statistical procedure of the performance estimation.</span></span>
<span id="cb74-783"><a href="#cb74-783" aria-hidden="true" tabindex="-1"></a><span class="ss">The underlying prediction of the model could still be biased e.g. due to a bias in the data set.</span></span>
<span id="cb74-784"><a href="#cb74-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-785"><a href="#cb74-785" aria-hidden="true" tabindex="-1"></a><span class="ss">```{r optimization-045}</span></span>
<span id="cb74-786"><a href="#cb74-786" aria-hidden="true" tabindex="-1"></a><span class="ss">rr$aggregate()</span></span>
<span id="cb74-787"><a href="#cb74-787" aria-hidden="true" tabindex="-1"></a><span class="ss">```</span></span>
<span id="cb74-788"><a href="#cb74-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-789"><a href="#cb74-789" aria-hidden="true" tabindex="-1"></a><span class="ss">As a final note, nested resampling is computationally expensive, as a simple example using five outer folds and three inner folds with a grid search of resolution 5 used to tune 2 parameters, results in 5*3*5*5 = 375 iterations of model training/testing. In practice, you may often see closer to three folds used in inner resampling or even holdout, or if you have the resources then we recommend parallelization (@sec-parallelization).</span></span>
<span id="cb74-790"><a href="#cb74-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-791"><a href="#cb74-791" aria-hidden="true" tabindex="-1"></a><span class="ss">## Conclusion</span></span>
<span id="cb74-792"><a href="#cb74-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-793"><a href="#cb74-793" aria-hidden="true" tabindex="-1"></a><span class="ss">In this chapter, we learned how to optimize a model using tuning instances, about different tuners and terminators, how to make use of the automated methods for quicker implementation in larger experiments, and the importance of nested resampling.</span></span>
<span id="cb74-794"><a href="#cb74-794" aria-hidden="true" tabindex="-1"></a><span class="ss">The most important functions and classes we learned about are in @tbl-api-optimization alongside their R6 classes.</span></span>
<span id="cb74-795"><a href="#cb74-795" aria-hidden="true" tabindex="-1"></a><span class="ss">If you are interested in learning more about the underlying R6 classes to gain finer control of these methods, then take a look at the online API.</span></span>
<span id="cb74-796"><a href="#cb74-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-797"><a href="#cb74-797" aria-hidden="true" tabindex="-1"></a><span class="ss">| S3 function | R6 Class | Summary |</span></span>
<span id="cb74-798"><a href="#cb74-798" aria-hidden="true" tabindex="-1"></a><span class="ss">| ------------------- | -------- | -------------------- |</span></span>
<span id="cb74-799"><a href="#cb74-799" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("tnr()")`   | `r ref("Tuner")` | Determines an optimisation algorithm |</span></span>
<span id="cb74-800"><a href="#cb74-800" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("trm()")` | `r ref("Terminator")` | Controls when to terminate the tuning algorithm |</span></span>
<span id="cb74-801"><a href="#cb74-801" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("ti()")` | `r ref("TuningInstanceSingleCrit")` or `r ref("TuningInstanceMultiCrit")` | Stores tuning settings and save results |</span></span>
<span id="cb74-802"><a href="#cb74-802" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("paradox::to_tune()")` | `r ref("paradox::TuneToken")` | Sets which parameters in a learner to tune and over what search space |</span></span>
<span id="cb74-803"><a href="#cb74-803" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("auto_tuner()")` | `r ref("AutoTuner")` | Automates the tuning process |</span></span>
<span id="cb74-804"><a href="#cb74-804" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("extract_inner_tuning_results()")`  | -                    | Extracts inner results from nested resampling |</span></span>
<span id="cb74-805"><a href="#cb74-805" aria-hidden="true" tabindex="-1"></a><span class="ss">| `r ref("extract_inner_tuning_archives()")` | -                    | Extracts inner archives from nested resampling |</span></span>
<span id="cb74-806"><a href="#cb74-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-807"><a href="#cb74-807" aria-hidden="true" tabindex="-1"></a><span class="ss">:Core S3 </span><span class="ot">'</span>sugar<span class="ot">'</span><span class="ss"> functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-optimization}</span></span>
<span id="cb74-808"><a href="#cb74-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-809"><a href="#cb74-809" aria-hidden="true" tabindex="-1"></a><span class="ss">### Resources{.unnumbered .unlisted}</span></span>
<span id="cb74-810"><a href="#cb74-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-811"><a href="#cb74-811" aria-hidden="true" tabindex="-1"></a><span class="ss">The `r link("https://cheatsheets.mlr-org.com/mlr3tuning.pdf", "mlr3tuning cheatsheet")` summarizes the most important functions of mlr3tuning and the `r link("https://mlr-org.com/gallery.html#category:tuning", "mlr3 gallery")` features a collection of case studies and demonstrations about optimization, most notably learn how to:</span></span>
<span id="cb74-812"><a href="#cb74-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-813"><a href="#cb74-813" aria-hidden="true" tabindex="-1"></a><span class="ss">  - Apply advanced methods in the `r link("https://mlr-org.com/gallery.html#category:practical_tuning_series", "practical tuning series")`.</span></span>
<span id="cb74-814"><a href="#cb74-814" aria-hidden="true" tabindex="-1"></a><span class="ss">  - Optimize an rpart classification tree with only a `r link("https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/", "few lines of code")`.</span></span>
<span id="cb74-815"><a href="#cb74-815" aria-hidden="true" tabindex="-1"></a><span class="ss">  - Tune an XGBoost model with `r link("https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/", "early stopping")`.</span></span>
<span id="cb74-816"><a href="#cb74-816" aria-hidden="true" tabindex="-1"></a><span class="ss">  - Quickly load and tune over search spaces that have been published in literature with `r link("https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/", "mlr3tuningspaces")`.</span></span>
<span id="cb74-817"><a href="#cb74-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-818"><a href="#cb74-818" aria-hidden="true" tabindex="-1"></a><span class="ss">## Exercises</span></span>
<span id="cb74-819"><a href="#cb74-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-820"><a href="#cb74-820" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).</span></span>
<span id="cb74-821"><a href="#cb74-821" aria-hidden="true" tabindex="-1"></a><span class="ss">Use a simple random search with 50 evaluations and select a suitable batch size.</span></span>
<span id="cb74-822"><a href="#cb74-822" aria-hidden="true" tabindex="-1"></a><span class="ss">Evaluate with a 3-fold cross-validation and the root mean squared error.</span></span>
<span id="cb74-823"><a href="#cb74-823" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Evaluate the performance of the model created in Question 1 with nested resampling.</span></span>
<span id="cb74-824"><a href="#cb74-824" aria-hidden="true" tabindex="-1"></a><span class="ss">Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.</span></span>
<span id="cb74-825"><a href="#cb74-825" aria-hidden="true" tabindex="-1"></a><span class="ss">Print the unbiased performance estimate of the model.</span></span>
<span id="cb74-826"><a href="#cb74-826" aria-hidden="true" tabindex="-1"></a><span class="ss">1. Tune and benchmark an XGBoost model against a logistic regression and determine which has the best Brier score.</span></span>
<span id="cb74-827"><a href="#cb74-827" aria-hidden="true" tabindex="-1"></a><span class="ss">Use mlr3tuningspaces and nested resampling.</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licenced under <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.</div>   
      <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Written with <i class="bi bi-heart-fill"></i> for #rstats, ML and FOSS by the mlr-org team.</div>
  </div>
</footer>


<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>