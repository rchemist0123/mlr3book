[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Flexible and Robust Machine Learning Using mlr3 in R",
    "section": "",
    "text": "Getting Started\n\nEditors\nMichel Lang, Raphael Sonabend, Lars Kotthoff, Bernd Bischl\n\n\nContributing authors\n\nMarc Becker\nPrzemysław Biecek\nMartin Binder\nBernd Bischl\nLukas Burk\nGiuseppe Casalicchio\nSebastian Fischer\nNatalie Foss\nLars Kotthoff\nMichel Lang\nFlorian Pfisterer\nDamir Pulatov\nLennart Schneider\nPatrick Schratz\nRaphael Sonabend\nMarvin Wright\n\nWelcome to the Machine Learning in R universe. This is the electronic version of the upcoming book Flexible and Robust Machine Learning Using mlr3 in R. This book will teach you about the mlr3 universe of packages, from some machine learning methodology to implementations of complex algorithmic pipelines. We will cover how to use the mlr3 family of packages for data processing, fitting and training of machine learning models, tuning and hyperparameter optimization, feature selection, pipelines, data preprocessing, and model interpretability. In addition we will look at how our interface works beyond classification and regression settings to other fields including survival analysis, clustering, and more. Finally we will demonstrate how you can contribute to our universe by creating packages, learners, measures, pipelines, and other features.\nWe hope you enjoy reading our book and always welcome comments and feedback. If you notice any mistakes in the book we would appreciate if you could open an issue in the mlr3book issue tracker. All content in this book is licenced under CC BY-NC 4.0."
  },
  {
    "objectID": "preface.html#howtouse",
    "href": "preface.html#howtouse",
    "title": "Preface",
    "section": "How to use this book",
    "text": "How to use this book\nThe mlr3 ecosystem is the result of many years of methodological and applied research and improving the design and implementation of the packages over the years. This book describes the resulting features of the mlr3verse and discusses best practices for ML, technical implementation details, extension guidelines, and in-depth considerations for optimizing ML. It is suitable for a wide range of readers and levels of ML expertise.\nChapter 1, Chapter 2, and Chapter 3 cover the basics of mlr3. These chapters are essential to understanding the core infrastrucure of ML in mlr3. We recommend that all readers study these chapters to become familiar with basic mlr3 terminology, syntax, and style. Chapter 4, Chapter 5, and Chapter 6 contain more advanced implementation details and some ML theory. Chapter 8 delves into detail on domain-specific methods that are implemented in our extension packages. Readers may choose to selectively read sections in this chapter depending on your use cases (i.e., if you have domain-specific problems to tackle), or to use these as introductions to new domains to explore. Chapter 9 contains technical implementation details that are essential reading for advanced users who require parallelisation, custom error handling, and fine control over hyperparameters and large databases. Chapter 10 discusses packages that can be integrated with mlr3 to provide model-agnostic interpretability methods. Finally, anyone who would like to contribute to our ecosystem should read Chapter 11.\nOf course, you can also read the book cover to cover from start to finish. We have marked any section that contains complex technical information with an exclamation mark (!). You may wish to skip these sections if you are only interested in basic functionality. Similarly, we have marked sections that are optional, such as parts that are more methodological focused and do not discuss the software implementation, with an asterisk (*). Readers that are interested in the more technical detail will likely want to pay attention to the tables at the end of each chapter that show the relationship between our S3 ‘sugar’ functions and the underlying R6 classes; this is explained in more detail in Chapter 1.\nThis book tries to follow the Diátaxis framework1 for documentation and so we include tutorials, how-to guides, API references, and explanations. This means that the conclusion of each chapter includes a short reference to the core functions learnt in the chapter, links to relevant posts in the mlr3gallery2, and a few exercises that will cover content introduced in the chapter. You can find the solutions to these exercises in Appendix A.1 https://diataxis.fr/2 https://mlr-org.com/gallery.html\nFinally, if you want to reproduce any of the results in this book, note that the random seed is set as the chapter number and the sessionInfo() printed in Appendix E."
  },
  {
    "objectID": "preface.html#installguide",
    "href": "preface.html#installguide",
    "title": "Preface",
    "section": "Installation guidelines",
    "text": "Installation guidelines\nAll packages in the mlr3 ecosystem can be installed from GitHub and R-universe; the majority (but not all) packages can also be installed from CRAN. We recommend adding the mlr-org R-universe3 to your R options so that you can install all packages with install.packages() without having to worry which package repository it comes from. To do this, install the usethis package and run the following:3 R-universe is an alternative package repository to CRAN. The bit of code below tells R to look at both R-universe and CRAN when trying to install packages. R will always install the latest version of a package.\n\nusethis::edit_r_profile()\n\nIn the file that opens add or change the repos argument in options so it looks something like this (you might need to add the full code block below or just edit the existing options function).\n\noptions(repos = c(\n  mlrorg = \"https://mlr-org.r-universe.dev\",\n  CRAN = \"https://cloud.r-project.org/\"\n))\n\nSave the file, restart your R session, and you are ready to go!\n\ninstall.packages(\"mlr3verse\")\n\nIf you want latest development versions of any of our packages, run\n\nremotes::install_github(\"mlr-org/{pkg}\")\n\nwith {pkg} replaced with the name of the package you want to install. You can see an up-to-date list of all our extension packages at https://github.com/mlr-org/mlr3/wiki/Extension-Packages."
  },
  {
    "objectID": "preface.html#community-links",
    "href": "preface.html#community-links",
    "title": "Preface",
    "section": "Community links",
    "text": "Community links\nThe mlr community is open to all and we welcome everybody, from those completely new to ML and R to advanced coders and professional data scientists. You can reach us on our Mattermost4.4 https://lmmisld-lmu-stats-slds.srv.mwn.de/signup_email?id=6n7n67tdh7d4bnfxydqomjqspo\nFor case studies and how-to guides, check out the mlr3gallery5 for extended practical blog posts. For updates on mlr you might find our blog6 a useful point of reference.5 https://mlr-org.com/gallery.html6 https://mlr-org.com/blog.html\nWe appreciate all contributions, whether they are bug reports, feature requests, or pull requests that fix bugs or extend functionality. Each of our GitHub repositories includes issues and pull request templates to ensure we can help you as much as possible to get started. Please make sure you read our code of conduct7 and contribution guidelines8. With so many packages in our universe it may be hard to keep track of where to open issues. As a general rule:7 https://github.com/mlr-org/mlr3/blob/main/.github/CODE_OF_CONDUCT.md8 https://github.com/mlr-org/mlr3/blob/main/CONTRIBUTING.md\n\nIf you have a question about using any part of the mlr3 ecosystem, ask on StackOverflow and use the tag #mlr3 – one of our team will answer you there. Be sure to include a reproducible example (reprex) and if we think you found a bug then we will refer you to the relevant GitHub repository.\nBug reports or pull requests about core functionality (train, predict, etc.) should be opened in the mlr3 GitHub repository.\nBug reports or pull requests about learners should be opened in the mlr3extralearners GitHub repository.\nBug reports or pull requests about measures should be opened in the mlr3measures GitHub repository.\nBug reports or pull requests about domain specific functionality should be opened in the GitHub repository of the respective package (see Chapter 1).\n\nDo not worry about opening an issue in the wrong place, we will transfer it to the right one!"
  },
  {
    "objectID": "preface.html#citeus",
    "href": "preface.html#citeus",
    "title": "Preface",
    "section": "Citation info",
    "text": "Citation info\nEvery package in the mlr3verse has its own citation details that can be found on the respective GitHub repository.\nTo reference this book please use:\nBecker M, Binder M, Bischl B, Foss N, Kotthoff L, Lang M, Pfisterer F,\nReich N G, Richter J, Schratz P, Sonabend R, Pulatov D.\n2023. \"Preface\". https://mlr3book.mlr-org.com.\n@misc{\n    title = Preface\n    author = {Marc Becker, Martin Binder, Bernd Bischl, Natalie Foss,\n    Lars Kotthoff, Michel Lang, Florian Pfisterer, Nicholas G. Reich,\n    Jakob Richter, Patrick Schratz, Raphael Sonabend, Damir Pulatov},\n    url = {https://mlr3book.mlr-org.com},\n    year = {2023}\n}\nTo reference the mlr3 package, please cite our JOSS paper:\nLang M, Binder M, Richter J, Schratz P, Pfisterer F, Coors S, Au Q,\nCasalicchio G, Kotthoff L, Bischl B (2019). “mlr3: A modern object-oriented\nmachine learning framework in R.” Journal of Open Source Software.\ndoi: 10.21105/joss.01903.\n\n@Article{mlr3,\n  title = {{mlr3}: A modern object-oriented machine learning framework in {R}},\n  author = {Michel Lang and Martin Binder and Jakob Richter and Patrick Schratz and\n  Florian Pfisterer and Stefan Coors and Quay Au and Giuseppe Casalicchio and\n  Lars Kotthoff and Bernd Bischl},\n  journal = {Journal of Open Source Software},\n  year = {2019},\n  month = {dec},\n  doi = {10.21105/joss.01903},\n  url = {https://joss.theoj.org/papers/10.21105/joss.01903},\n}"
  },
  {
    "objectID": "preface.html#styleguide",
    "href": "preface.html#styleguide",
    "title": "Preface",
    "section": "mlr3book style guide",
    "text": "mlr3book style guide\nThroughout this book we will use our own style guide that can be found in the mlr3 wiki9. Below are the most important style choices relevant to the book.9 https://github.com/mlr-org/mlr3/wiki/Style-Guide\n\nWe always use = instead of <- for assignment.\nClass names are in UpperCamelCase\nFunction and method names are in lower_snake_case\nWhen referencing functions, we will only include the package prefix (e.g., pkg::function) for functions outside the mlr3 universe or when there may be ambiguity about in which package the function lives. Note you can use environment(function) to see which namespace a function is loaded from.\n\nWe denote packages, fields, methods, and functions as follows:\n\n\npackage - With link (if online) to package CRAN, R-Universe, or GitHub page\n\npackage::function() (for functions outside the mlr-org ecosystem)\n\nfunction() (for functions inside the mlr-org ecosystem) - With link to function documentation page\n\n$field for fields (data encapsulated in a R6 class)\n\n$method() for methods (functions encapsulated in a R6 class)"
  },
  {
    "objectID": "intro.html#target-audience",
    "href": "intro.html#target-audience",
    "title": "1  Introduction and Overview",
    "section": "\n1.1 Target audience",
    "text": "1.1 Target audience\nWe assume that users of mlr3 have taken an introductory machine learning course or have the equivalent expertise and some basic experience with R. A background in computer science or statistics is beneficial for understanding the advanced functionality described in the later chapters of this book, but not required. (James et al. 2014) provides a comprehensive introduction for those new to machine learning.\nmlr3 provides a domain-specific language for machine learning in R that allows to do everything from simple exercises to complex projects. We target both practitioners who want to quickly apply machine learning algorithms and researchers who want to implement, benchmark, and compare their new methods in a structured environment."
  },
  {
    "objectID": "intro.html#from-mlr-to-mlr3",
    "href": "intro.html#from-mlr-to-mlr3",
    "title": "1  Introduction and Overview",
    "section": "\n1.2 From mlr to mlr3",
    "text": "1.2 From mlr to mlr3\nThe mlr package (Bischl et al. 2016) was first released to CRAN4 in 2013, with the core design and architecture dating back much further. Over time, the addition of many features has led to a considerably more complex design that made it harder to build, maintain, and extend than we had hoped for. In hindsight, we saw that some design and architecture choices in mlr made it difficult to support new features, in particular with respect to machine learning pipelines. Furthermore, the R ecosystem and helpful packages such as data.table have undergone major changes after the initial design of mlr.4 https://cran.r-project.org\nIt would have been impossible to integrate all of these changes into the original design of mlr. Instead, we decided to start working on a reimplementation in 2018, which resulted in the first release of mlr3 on CRAN in July 2019.\nThe new design and the integration of further and newly-developed R packages (especially R6, future, and data.table) makes mlr3 much easier to use, maintain, and in many regards more efficient than its predecessor mlr. The packages in the ecosystem are less tightly coupled, making them easier to maintain and easier to develop, especially very specialized packages."
  },
  {
    "objectID": "intro.html#design-principles",
    "href": "intro.html#design-principles",
    "title": "1  Introduction and Overview",
    "section": "\n1.3 Design principles",
    "text": "1.3 Design principles\n\n\n\n\n\n\nSome readers may want to skip this section of the book.\n\n\n\n\n\n\nWe follow these general design principles in the mlr3 package and mlr3verse ecosystem.\n\n\nSeparation of computation and presentation. Most packages of the mlr3 ecosystem focus on processing and transforming data, applying machine learning algorithms, and computing results. Our core packages do not provide graphical user interfaces (GUIs) because their dependencies would make installation unnecessarily complex, especially on headless servers. For the same reason, visualizations of data and results are provided in the extra package mlr3viz, which avoids dependencies on ggplot2. mlr3shiny provides an interface for some basic machine learning tasks using the shiny package.\n\nObject-oriented programming (OOP). Embrace R6 for a clean, object-oriented design, object state-changes, and reference semantics.\n\nTabular data. Embrace data.table for its top-notch computation performance as well as tabular data as a data structure which can be easily processed further.\n\nUnify input and output data formats. This considerably simplifies the API and allows easy selection and “split-apply-combine” (aggregation) operations. We combine data.table and R6 to place references to non-atomic and compound objects in tables and make heavy use of list columns.\n\nDefensive programming and type safety. All user input is checked with checkmate (Lang 2017). We document return types, and avoid mechanisms popular in base R which “simplify” the result unpredictably (e.g., sapply() or the drop argument for indexing data.frames). And we have extensive unit tests!\n\nLight on dependencies. One of the main maintenance burdens for mlr was to keep up with changing learner interfaces and behavior of the many packages it depended on. We require far fewer packages in mlr3 to make installation and maintenance easier. We still provide the same functionality, but it is split into more packages that have fewer dependencies individually. As mentioned above, this is particularly the case for all visualization functionality, which is contained in a separate package to avoid unnecessary dependencies in all other packages."
  },
  {
    "objectID": "intro.html#package-ecosystem",
    "href": "intro.html#package-ecosystem",
    "title": "1  Introduction and Overview",
    "section": "\n1.4 Package ecosystem",
    "text": "1.4 Package ecosystem\nmlr3 depends on the following popular and well-established packages that are not developed by core members of the mlr3 team:\n\n\nR6: The class system predominantly used in mlr3.\n\ndata.table: High-performance extension of R’s data.frame.\n\ndigest: Cryptographic hash functions.\n\nuuid: Generation of universally unique identifiers.\n\nlgr: Highly configurable logging library.\n\nmlbench and palmerpenguins: More machine learning data sets.\n\nevaluate: For capturing output, warnings, and exceptions (Section 9.2).\n\nfuture / future.apply: For parallelization (Section 9.1).\n\nThe mlr3 package itself provides the base functionality that the rest of ecosystem (mlr3verse) relies on and the fundamental building blocks for machine learning. Figure 1.1 shows the packages in the mlr3verse that extend mlr3 with capabilities for preprocessing, pipelining, visualizations, additional learners, additional task types, and more.\n\n\n\n\nFigure 1.1: Overview of the mlr3 ecosystem, the mlr3verse.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA complete list with links to the repositories for the respective packages can be found on our package overview page5.5 https://mlr-org.com/ecosystem.html\n\n\nWe build on R6 for object orientation and data.table to store and operate on tabular data. Both are core to mlr3; we briefly introduce both packages for beginners. While in-depth expertise with these packages is not necessary, a basic understanding is required to work effectively with mlr3."
  },
  {
    "objectID": "intro.html#sec-r6",
    "href": "intro.html#sec-r6",
    "title": "1  Introduction and Overview",
    "section": "\n1.5 Quick R6 introduction for beginners",
    "text": "1.5 Quick R6 introduction for beginners\nR6 is one of R’s more recent paradigm for object-oriented programming (OOP). It addresses shortcomings of earlier OO implementations in R, such as S3, which we used in mlr. If you have done any object-oriented programming before, R6 should feel familiar. We focus on the parts of R6 that you need to know to use mlr3.\nObjects are created by calling the constructor of an R6::R6Class() object, specifically the initialization method $new(). For example, foo = Foo$new(bar = 1) creates a new object of class Foo, setting the bar argument of the constructor to the value 1.\nObjects have mutable state that is encapsulated in their fields, which can be accessed through the dollar operator. We can access the bar value in the foo variable from above through foo$bar and set its value by assigning the field, e.g. foo$bar = 2.\nIn addition to fields, objects expose methods that allow to inspect the object’s state, retrieve information, or perform an action that changes the internal state of the object. For example, the $train() method of a learner changes the internal state of the learner by building and storing a model, which can then be used to make predictions.\nObjects can have public and private fields and methods. The public fields and methods define the API to interact with the object. Private methods are only relevant for you if you want to extend mlr3, e.g. with new learners.\nTechnically, R6 objects are environments, and as such have reference semantics. For example, foo2 = foo does not create a copy of foo in foo2, but another reference to the same actual object. Setting foo$bar = 3 will also change foo2$bar to 3 and vice versa.\nTo copy an object, use the $clone() method and the deep = TRUE argument for nested objects, for example, foo2 = foo$clone(deep = TRUE).\n\n\n\n\n\n\nTip\n\n\n\nFor more details on R6, have a look at the excellent R6 vignettes6, especially the introduction7. For comprehensive R6 information, we refer to the R6 chapter from Advanced R8.6 https://r6.r-lib.org/7 https://r6.r-lib.org/articles/Introduction.html8 https://adv-r.hadley.nz/r6.html"
  },
  {
    "objectID": "intro.html#sec-data.table",
    "href": "intro.html#sec-data.table",
    "title": "1  Introduction and Overview",
    "section": "\n1.6 Quick data.table introduction for beginners",
    "text": "1.6 Quick data.table introduction for beginners\nThe package data.table implements a popular alternative to R’s data.frame(), i.e. an object to store tabular data. We decided to use data.table because it is blazingly fast and scales well to bigger data.\n\n\n\n\n\n\nNote\n\n\n\nMany mlr3 functions return data.tables which can conveniently be subsetted or combined with other outputs. If you do not like the syntax or are feeling more comfortable with other tools, base data.frames or tibble/dplyrs are just a single as.data.frame() or as_tibble() away.\n\n\nData tables are constructed with the data.table() function (whose interface is similar to data.frame()) or by converting an object with as.data.table().\n\nlibrary(\"data.table\")\ndt = data.table(x = 1:6, y = rep(letters[1:3], each = 2))\ndt\n\n   x y\n1: 1 a\n2: 2 a\n3: 3 b\n4: 4 b\n5: 5 c\n6: 6 c\n\n\ndata.tables can be used much like data.frames, but they do provide additional functionality that makes complex operations easier. For example, data can be summarized by groups with the [ operator:\n\ndt[, mean(x), by = \"y\"]\n\n   y  V1\n1: a 1.5\n2: b 3.5\n3: c 5.5\n\n\nThere is also extensive support for many kinds of database join operations (see e.g. this RPubs post by Ronald Stalder9) that make it easy to combine multiple data.tables in different ways.9 https://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html\n\n\n\n\n\n\nTip\n\n\n\nFor an in-depth introduction, we refer the reader to the excellent data.table introduction vignette10.10 https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html"
  },
  {
    "objectID": "intro.html#sec-mlr3-utilities",
    "href": "intro.html#sec-mlr3-utilities",
    "title": "1  Introduction and Overview",
    "section": "\n1.7 Essential mlr3 utilities",
    "text": "1.7 Essential mlr3 utilities\nSugar functions\nMost objects in mlr3 can be created through convenience functions called sugar functions. They provide shortcuts for common code idioms, reducing the amount of code a user has to write. We heavily use sugar functions throughout this book and give the equivalent “full form” for full detail. In most cases, the sugar functions will achieve what you want to do, and you only have to consider using the full R6 code if you program custom objects or extensions. For example lrn(\"regr.rpart\") is the sugar version of LearnerRegrRpart$new().\nDictionaries\nmlr3 uses dictionaries to store objects like learners or tasks. These are key-value stores that allow to associate a key with a value that can be an R6 object, much like paper dictionaries associate words with their definitions. Often, values in dictionaries are accessed through sugar functions that automatically use the applicable dictionary without the user having to specify it; only the key to be retrieved needs to be specified. Dictionaries are used to group relevant objects so that they can be listed and retrieved easily. For example, a learner can be retrieved directly from the mlr_learners dictionary using the key \"classif.featureless\" (mlr_learners$get(\"classif.featureless\")) and you can get an overview over all stored learners with as.data.table(mlr_learners).\nmlr3viz\nmlr3viz is the package for all plotting functionality in the mlr3 ecosystem. The package uses a common theme (ggplot2::theme_minimal()) so that all generated plots have a similar aesthetic. Under the hood, mlr3viz uses ggplot2. mlr3viz extends fortify and autoplotfor use with common mlr3 outputs including Prediction, Learner, and Benchmark objects (these objects will be introduced and covered in the next chapter). The most common use of mlr3viz is the autoplot() function, where the type of the object passed determines the type of the plot. Plot types are documented in the respective manual page that can be accessed through ?autoplot.X. For example, the documentation of plots for regression tasks can be found by running ?autoplot.TaskRegr.\n\n\n\n\n\n\nBischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in r. Springer Publishing Company, Incorporated.\n\n\nLang, Michel. 2017. “checkmate: Fast Argument Checks for Defensive R Programming.” The R Journal 9 (1): 437–45. https://doi.org/10.32614/RJ-2017-028.\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903.\n\n\nR Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/."
  },
  {
    "objectID": "basics.html#sec-tasks",
    "href": "basics.html#sec-tasks",
    "title": "2  Fundamentals",
    "section": "\n2.1 Tasks",
    "text": "2.1 Tasks\nTasks are objects that contain the (usually tabular) data and additional meta-data that define a machine learning problem. The meta-data contain, for example, the name of the target feature for supervised machine learning problems. This information is used automatically by operations that can be performed on a task so that for example the user does not have to specify the prediction target every time a model is trained.\n\n2.1.1 Built-in Tasks\nmlr3 includes a few predefined machine learning tasks in an R6 Dictionary named mlr_tasks.\n\nmlr_tasks\n\n<DictionaryTask> with 19 stored values\nKeys: bike_sharing, boston_housing, breast_cancer, german_credit, ilpd,\n  iris, kc_housing, moneyball, mtcars, optdigits, penguins,\n  penguins_simple, pima, sonar, spam, titanic, usarrests, wine, zoo\n\n\nTo get a task from the dictionary, use the tsk() function and assign the return value to a new variable. Here, we retrieve the mtcars regression task, which is provided by the package datasets:\n\ntask_mtcars = tsk(\"mtcars\")\ntask_mtcars\n\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\nTo get more information about a particular task, it is easiest to use the help() method that all mlr3-objects come with:\n\ntask_mtcars$help()\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are familiar with R’s help system (i.e. the help() and ? functions), this may seem confusing. task_mtcars is the variable that holds the mtcars task, not a function, and hence we cannot use help() or ?.\n\n\nAlternatively, the corresponding man page can be found under mlr_tasks_<id>, e.g.\n\nhelp(\"mlr_tasks_mtcars\")\n\nWe can also load the data separately and convert it to a task, without using the tsk() function that mlr3 provides. If the data we want to use does not come with mlr3, it has to be done this way.\nFor example, the data for mtcars is also available separately, as a data.frame() and not a task. mtcars contains characteristics for different types of cars, along with their fuel consumption. We want to predict the numeric target feature stored in column \"mpg\" (miles per gallon).\n\ndata(\"mtcars\", package = \"datasets\")\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\nWe create the regression task, i.e. we construct a new instance of the R6 class TaskRegr. An easy way to do this is to use the function as_task_regr() to convert our data.frame() to a regression task, specifying the target feature in an additional argument. Before we give the data to as_task_regr(), we can process it using the usual R functions, for example to select a subset of data.\n\nlibrary(\"mlr3\")\nmtcars_subset = subset(mtcars, select = c(\"mpg\", \"cyl\", \"disp\"))\n\ntask_mtcars = as_task_regr(mtcars_subset, target = \"mpg\", id = \"cars\")\ntask_mtcars\n\n<TaskRegr:cars> (32 x 3)\n* Target: mpg\n* Properties: -\n* Features (2):\n  - dbl (2): cyl, disp\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe task constructors as_task_regr() and as_task_classif() will check for non-ASCII characters in the column names of your data. As many ML models do not work properly with arbitrary UTF8 names, mlr3 defaults to throw an error if any of the column names contains either a non-ASCII character or does not comply with R’s variable naming scheme. We generally recommend converting names with make.names() first, but you can also set the option mlr3.allow_utf8_names to true to relax the check (but do not be surprised if a model fails).\n\n\nThe data can be any rectangular data format, e.g. a data.frame(), data.table(), or tibble(). Internally, the data is converted and stored in a DataBackend. The target argument specifies the prediction target column. The id argument is optional and specifies an identifier for the task that is used in plots and summaries. If no id is given provided, the deparsed name of the data will be used (an R way of turning data into strings).\nPrinting a task gives a short summary: it has 32 observations and 3 columns, of which mpg is the target and 2 are features stored in double-precision floating point format.\nWe can plot the task using the mlr3viz package, which gives a graphical summary of the distribution of the target and feature values:\n\nlibrary(\"mlr3viz\")\nautoplot(task_mtcars, type = \"pairs\")\n\n\n\nOverview of the mtcars dataset.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInstead of loading multiple extension packages individually, it is often more convenient to load the mlr3verse package instead. It makes the functions from most mlr3 packages that are used for common machine learning and data science tasks available.\n\n\n\n2.1.2 Retrieving Data\nThe Task object primarily represents a tabular dataset, combined with meta-data about which columns of that data should be used to predict which other columns in what way, and some more information about column data types.\nVarious fields can be used to retrieve meta-data about a task. The dimensions, for example, can be retrieved using $nrow and $ncol:\n\ntask_mtcars$nrow\n\n[1] 32\n\ntask_mtcars$ncol\n\n[1] 3\n\n\nThe names of the feature and target columns are stored in the $feature_names and $target_names slots, respectively. Here, “target” refers to the feature we want to predict and “feature” to the predictors for the task.\n\ntask_mtcars$feature_names\n\n[1] \"cyl\"  \"disp\"\n\ntask_mtcars$target_names\n\n[1] \"mpg\"\n\n\nWhile the columns of a task have unique character-valued names, their rows are identified by unique natural numbers, called row IDs. They can be accessed through the $row_ids slot:\n\nhead(task_mtcars$row_ids)\n\n[1] 1 2 3 4 5 6\n\n\nRow IDs are not used as features when predicting; they are meta-data that allows to access individual observations.\n\n\n\n\n\n\nWarning\n\n\n\nAlthough the row IDs are typically just the sequence from 1 to nrow(data), they are only guaranteed to be unique natural numbers. It is possible that they do not start at 1, that they are not increasing by 1 each, or that they are not even in increasing order. This allows to transparently operate on real database management systems, where uniqueness is the only requirement for primary keys.\n\n\nThe data contained in a task can be accessed through $data(), which returns a data.table object. It has optional rows and cols arguments to specify subsets of the data to retrieve. When a database backend is used, this avoids loading unnecessary data into memory, making it more efficient than retrieving the entire data first and then subsetting it using [<rows>, <cols>].\n\ntask_mtcars$data()\n\n     mpg cyl  disp\n 1: 21.0   6 160.0\n 2: 21.0   6 160.0\n 3: 22.8   4 108.0\n 4: 21.4   6 258.0\n 5: 18.7   8 360.0\n---               \n28: 30.4   4  95.1\n29: 15.8   8 351.0\n30: 19.7   6 145.0\n31: 15.0   8 301.0\n32: 21.4   4 121.0\n\n# retrieve data for rows with IDs 1, 5, and 10 and column \"mpg\"\ntask_mtcars$data(rows = c(1, 5, 10), cols = \"mpg\")\n\n    mpg\n1: 21.0\n2: 18.7\n3: 19.2\n\n\nA shortcut to extract all data from a task is to simply convert it to a data.table:\n\n# show summary of all data\nsummary(as.data.table(task_mtcars))\n\n      mpg             cyl             disp      \n Min.   :10.40   Min.   :4.000   Min.   : 71.1  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8  \n Median :19.20   Median :6.000   Median :196.3  \n Mean   :20.09   Mean   :6.188   Mean   :230.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0  \n\n\n\n2.1.3 Task Mutators\nIt is often necessary to create tasks that encompass subsets of other tasks’ data, for example to manually create train-test-splits, or to fit models on a subset of given features. Restricting tasks to a given set of features can be done by calling $select() with the desired feature names. Restriction to rows (observations) is done with $filter() with the row IDs.\n\ntask_mtcars_small = tsk(\"mtcars\") # initialize with the full task\ntask_mtcars_small$select(c(\"am\", \"carb\")) # keep only these features\ntask_mtcars_small$filter(2:4) # keep only these rows\ntask_mtcars_small$data()\n\n    mpg am carb\n1: 21.0  1    4\n2: 22.8  1    1\n3: 21.4  0    1\n\n\nThese methods are so-called mutators; they modify the given Task in place. If you want to have an unmodified version of the task, you need to use the $clone() method to create a copy first.\n\ntask_mtcars_smaller = task_mtcars_small$clone()\ntask_mtcars_smaller$filter(2)\ntask_mtcars_smaller$data()\n\n   mpg am carb\n1:  21  1    4\n\ntask_mtcars_small$data()  # the original task is unmodified\n\n    mpg am carb\n1: 21.0  1    4\n2: 22.8  1    1\n3: 21.4  0    1\n\n\nNote also how the last call to $filter(2) did not select the second row of task_mtcars_small, but the row with ID 2, which is the first row of task_mtcars_small.\n\n\n\n\n\n\nTip\n\n\n\nIf you need to work with row numbers instead of row IDs, you can work on the vector of row IDs:\n\n# keep the 2nd row:\nkeep = task_mtcars_small$row_ids[2] # extracts ID of 2nd row\ntask_mtcars_smaller$filter(keep)\n\n\n\nThe methods above allow to subset the data; the methods $rbind() and $cbind() allow to add extra rows and columns to a task.\n\ntask_mtcars_smaller$rbind( # add another row\n  data.frame(mpg = 23, am = 0, carb = 3)\n)\ntask_mtcars_smaller$data()\n\n   mpg am carb\n1:  21  1    4\n2:  23  0    3\n\n\n\n2.1.4 Roles (Rows and Columns)\n\n\n\n\n\n\nSome readers may want to skip this section of the book.\n\n\n\n\n\n\nWe have seen that certain columns are designated as “targets” and “features” during task creation; mlr3 calls this “roles”. Target refers to the column(s) we want to predict and features are the predictors (also called co-variates or descriptors) for the target. Besides these two, there are other possible roles for columns. The roles affect the behavior of the task for different operations.\nThe task_mtcars_small task, for example, has the following column roles:\n\ntask_mtcars_small$col_roles\n\n$feature\n[1] \"am\"   \"carb\"\n\n$target\n[1] \"mpg\"\n\n$name\n[1] \"model\"\n\n$order\ncharacter(0)\n\n$stratum\ncharacter(0)\n\n$group\ncharacter(0)\n\n$weight\ncharacter(0)\n\n\nAs you can see, there are additional column roles; the interested reader is referred to the documentation of Task for more detail. We can list all supported column roles by printing the names of the field $col_roles:\n\n# supported column roles, see ?Task\nnames(task_mtcars_small$col_roles)\n\n[1] \"feature\" \"target\"  \"name\"    \"order\"   \"stratum\" \"group\"   \"weight\" \n\n\nColumns can have multiple roles. It is also possible for a column to have no role at all, in which case they are ignored. This is, in fact, how $select() and $filter() operate: They unassign the \"feature\" (for columns) or \"use\" (for rows) role without modifying the data which is stored in an immutable backend:\n\ntask_mtcars_small$backend\n\n<DataBackendDataTable> (32x13)\n             model  mpg cyl disp  hp drat    wt  qsec vs am gear carb ..row_id\n         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4        1\n     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4        2\n        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1        3\n    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1        4\n Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2        5\n           Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1        6\n[...] (26 rows omitted)\n\n\nThere are two main ways to manipulate the column roles of a Task:\n\nUse the Task method $set_col_roles() (recommended).\nDirectly modify the field $col_roles, which is a named list of vectors of column names. Each vector in this list corresponds to a column role, and the column names contained in that vector have the corresponding role.\n\nJust as $select()/$filter(), these are in-place operations, i.e. the task object itself is modified. To retain an unmodified version of a task, use $clone().\nChanging the column or row roles, whether through $select()/$filter() or directly, does not change the underlying data, it just updates the view on it. Because the underlying data are still there (and accessible through $backend), we can add the \"cyl\" column back into the task by setting its column role to \"feature\".\n\ntask_mtcars_small$set_col_roles(\"cyl\", roles = \"feature\")\ntask_mtcars_small$feature_names  # cyl is now a feature again\n\n[1] \"am\"   \"carb\" \"cyl\" \n\ntask_mtcars_small$data()\n\n    mpg am carb cyl\n1: 21.0  1    4   6\n2: 22.8  1    1   4\n3: 21.4  0    1   6\n\n\nJust like columns, it is also possible to assign different roles to rows. Rows can have two different roles:\n\nRole use: Rows that are generally available for training (although they may also be used for the test set). This role is the default role. The $filter() call changes this role, in the same way that $select() changes the \"feature\" column role.\nRole validation: Rows that are not used for training. Rows that have missing values in the target column during task creation are automatically set to the validation role.\n\nThere are several reasons to hold some observations back or treat them differently:\n\nIt is often good practice to validate the final model on an external validation set to identify overfitting.\nSome observations may be unlabeled in the original data, e.g. in competitions like Kaggle1.\n\n1 https://www.kaggle.com/These observations cannot be used for training a model, but can be used for getting predictions from a trained model."
  },
  {
    "objectID": "basics.html#sec-learners",
    "href": "basics.html#sec-learners",
    "title": "2  Fundamentals",
    "section": "\n2.2 Learners",
    "text": "2.2 Learners\nObjects of class Learner provide a unified interface to many popular machine learning algorithms in R. They are available through the mlr_learners dictionary. The list of learners supported in the base package mlr3 is deliberately small to avoid dependencies; support for additional learners is provided by the mlr3learners and mlr3extralearners packages.\nLearners encapsulate methods to train a model and make predictions using it given a Task and provide meta-data about the learners. The base class of each learner is Learner.\nTo retrieve a Learner from the mlr_learners dictionary, use the function lrn():\n\nlearner_rpart = lrn(\"regr.rpart\")\n\nEach learner provides the following meta-data:\n\n\n$feature_types: the type of features the learner can deal with.\n\n$packages: the packages required to train a model with this learner and make predictions.\n\n$properties: additional properties and capabilities. For example, a learner has the property “missings” if it is able to handle missing feature values, and “importance” if it computes and allows to extract data on the relative importance of the features.\n\n$predict_types: possible prediction types. For example, a regression learner can predict numerical values (“response”) and may be able to predict the standard error of a prediction (“se”).\n\nThis information can be queried through these slots, or seen at a glance when printing the learner:\n\nlearner_rpart\n\n<LearnerRegrRpart:regr.rpart>: Regression Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response]\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, selected_features, weights\n\n\nAll learners work in two stages:\n\n\nTraining: A training task (features and target data) is passed to the learner’s $train() function which trains and stores a model, i.e. the learned relationship of the features to the target.\n\nPrediction: New data, usually a different partition of the original dataset, is passed to the $predict() method of the trained learner. The model trained in the first step is used to predict the target values, e.g. the numerical value for regression problems.\n\n\n\n\n\n\n\nWarning\n\n\n\nA learner that has not been trained cannot make predictions and will throw an error if $predict() is called on it.\n\n\n\n\n\n\nFigure 2.2: Overview of the different stages of a learner.\n\n\n\n\n\n2.2.1 Training the learner\nWe train the model by giving a task to the learner. It is a good idea to hold back some data from the training process use to assess the quality of the predictions made by the trained model. The partition() function randomly splits the task into two disjoint sets: a training set (67% of the total data, the default) and a test set (33% of the total data, the data not part of the training set).\n\nsplits = partition(task_mtcars)\nsplits\n\n$train\n [1]  1  2  3  4  5  8 25 30 32  6 11 13 17 22 23 24 29 31 19 20 28\n\n$test\n [1]  9 10 21 27  7 12 14 15 16 18 26\n\n\nWe learn a regression tree by calling the $train() method of the learner, specifying the task and the part of it to use for training (splits$train). This operation adds the learned model to the existing Learner object. We can now access the stored model via the field $model.\n\nlearner_rpart$train(task_mtcars, splits$train)\nlearner_rpart$model\n\nn= 21 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 21 617.38670 20.33333  \n  2) disp>=153.35 14  88.36857 17.42857 *\n  3) disp< 153.35 7 174.63710 26.14286 *\n\n\nWe see that the learner has identified features in the task that are predictive of the class (mpg) and uses them to partition observations in the tree. The textual representation of the model depends on the type of learner. For more information on this particular type of model and how it is printed, see rpart::print.rpart().\nThe model seems rather simplistic, using only a single feature and a single set of branches. Each learner has hyperparameters that control its behavior and allow to influence the way a model is learned. Setting hyperparameters to values appropriate for a given machine learning task is crucial for good predictive performance. The field param_set stores a description of the hyperparameters the learner has, their ranges, defaults, and current values:\n\nlearner_rpart$param_set\n\n<ParamSet>\n                id    class lower upper nlevels        default value\n 1:             cp ParamDbl     0     1     Inf           0.01      \n 2:     keep_model ParamLgl    NA    NA       2          FALSE      \n 3:     maxcompete ParamInt     0   Inf     Inf              4      \n 4:       maxdepth ParamInt     1    30      30             30      \n 5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n 6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n 7:       minsplit ParamInt     1   Inf     Inf             20      \n 8: surrogatestyle ParamInt     0     1       2              0      \n 9:   usesurrogate ParamInt     0     2       3              2      \n10:           xval ParamInt     0   Inf     Inf             10     0\n\n\nThe set of current hyperparameter values is stored in the values field of the param_set field. You can access and change the current hyperparameter values by accessing this field, which stores a named list:\n\nlearner_rpart$param_set$values\n\n$xval\n[1] 0\n\nlearner_rpart$param_set$values$minsplit = 10\nlearner_rpart$param_set$values\n\n$xval\n[1] 0\n\n$minsplit\n[1] 10\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is possible to assign all hyperparameters in one go by assigning a named list to $values: learner$param_set$values = list(minsplit = 10, ...). This operation removes all previously-set hyperparameters.\n\n\nThe lrn() function also accepts additional arguments to update hyperparameters or set fields of the learner when constructing it:\n\nlearner_rpart = lrn(\"regr.rpart\", minsplit = 10)\nlearner_rpart$param_set$values\n\n$xval\n[1] 0\n\n$minsplit\n[1] 10\n\n\n\nlearner_rpart$train(task_mtcars, splits$train)\nlearner_rpart$model\n\nn= 21 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 21 617.386700 20.33333  \n  2) disp>=101.55 18 167.562800 18.46111  \n    4) cyl>=7 9  30.075560 16.07778 *\n    5) cyl< 7 9  35.242220 20.84444 *\n  3) disp< 101.55 3   8.166667 31.56667 *\n\n\nWith the changed hyperparameters, we have a more complex (and more reasonable) model.\n\n\n\n\n\n\nNote\n\n\n\nDetails on the hyperparameters of our rpart learner can be found at rpart::rpart.control(). Hyperparameters in general are discussed in more detail in the section on Hyperparameter Tuning.\n\n\n\n2.2.2 Predicting\nAfter the model has been created, we can now use it to make predictions. We can give the test partition to the $predict() function:\n\npredictions = learner_rpart$predict(task_mtcars, splits$test)\npredictions\n\n<PredictionRegr> for 11 observations:\n    row_ids truth response\n          9  22.8 20.84444\n         10  19.2 20.84444\n         21  21.5 20.84444\n---                       \n         16  10.4 16.07778\n         18  32.4 31.56667\n         26  27.3 31.56667\n\n\nThe $predict() method returns a Prediction object, in this case a PredictionRegr for predicting a numeric quantity. The “truth” column contains the ground truth data, which was not given to the model to get a prediction. The “response” column contains the value predicted by the model, allowing for easy comparison with the ground truth data.\nWe can also use separate data to make predictions, which can be part of a separate task or simply a separate data.frame:\n\nmtcars_new = data.frame(cyl = c(5, 6),\n                        disp = c(100, 120),\n                        hp = c(100, 150),\n                        drat = c(4, 3.9),\n                        wt = c(3.8, 4.1),\n                        qsec = c(18, 19.5),\n                        vs = c(1, 0),\n                        am = c(1, 1),\n                        gear = c(6, 4),\n                        carb = c(3, 5))\nmtcars_new\n\n  cyl disp  hp drat  wt qsec vs am gear carb\n1   5  100 100  4.0 3.8 18.0  1  1    6    3\n2   6  120 150  3.9 4.1 19.5  0  1    4    5\n\n\nThe learner does not need to know more meta-data about this data to make predictions, as this was given when training the model. We can use the $predict_newdata() method to make predictions for our separate dataset:\n\npredictions = learner_rpart$predict_newdata(mtcars_new)\npredictions\n\n<PredictionRegr> for 2 observations:\n row_ids truth response\n       1    NA 31.56667\n       2    NA 20.84444\n\n\nNote that the “truth” column is now NA, as we did not give the ground truth data.\nWe can also access the predictions directly:\n\npredictions$response\n\n[1] 31.56667 20.84444\n\n\nSimilar to plotting tasks, mlr3viz provides an autoplot() method for Prediction objects.\n\nlibrary(\"mlr3viz\")\npredictions = learner_rpart$predict(task_mtcars, splits$test)\nautoplot(predictions)\n\n\n\nComparing predicted and ground truth values for the mtcars dataset.\n\n\n\n\n\n2.2.3 Changing the Prediction Type\nRegression learners default to predicting a numeric quantity. However, many regression models can also give you bounds on the prediction by providing the standard error. To predict these standard errors, the predict_type field of a LearnerRegr must be changed from “response” (the default) to “se” before training. The rpart learner we used above does not support predicting standard errors, so we use the lm linear model instead, from the mlr3learners package:\n\nlibrary(mlr3learners)\n\nlearner_lm = lrn(\"regr.lm\")\nlearner_lm$predict_type = \"se\"\nlearner_lm$train(task_mtcars, splits$train)\npredictions = learner_lm$predict(task_mtcars, splits$test)\npredictions\n\n<PredictionRegr> for 11 observations:\n    row_ids truth response        se\n          9  22.8 26.35691 1.4217357\n         10  19.2 21.30664 0.9926847\n         21  21.5 26.44570 1.2694455\n---                                 \n         16  10.4 15.11714 2.1783078\n         18  32.4 26.62327 1.1895495\n         26  27.3 26.62199 1.1888914\n\n\n\n\n\n\n\n\nTip\n\n\n\nSection 2.6.1 shows how to list learners that support the standard error prediction type.\n\n\nThe prediction object now contains the standard error for the predictions.\n\npredictions\n\n<PredictionRegr> for 11 observations:\n    row_ids truth response        se\n          9  22.8 26.35691 1.4217357\n         10  19.2 21.30664 0.9926847\n         21  21.5 26.44570 1.2694455\n---                                 \n         16  10.4 15.11714 2.1783078\n         18  32.4 26.62327 1.1895495\n         26  27.3 26.62199 1.1888914"
  },
  {
    "objectID": "basics.html#sec-eval",
    "href": "basics.html#sec-eval",
    "title": "2  Fundamentals",
    "section": "\n2.3 Evaluation",
    "text": "2.3 Evaluation\nAn important step of modeling is evaluating the performance of the trained model. We have seen how to inspect the model and plot its predictions above, but a more rigorous way that allows to compare different types of models more easily is to compute a performance measure. mlr3 offers many performance measures, which can be created with the msr() function. Measures are stored in the dictionary mlr_measures, and a measure has to be supported by mlr3 to be used, just like learners. For example, we can list all measures that are available for regression tasks:\n\nmlr_measures$keys(\"regr\")\n\n [1] \"regr.bias\"  \"regr.ktau\"  \"regr.mae\"   \"regr.mape\"  \"regr.maxae\"\n [6] \"regr.medae\" \"regr.medse\" \"regr.mse\"   \"regr.msle\"  \"regr.pbias\"\n[11] \"regr.rae\"   \"regr.rmse\"  \"regr.rmsle\" \"regr.rrse\"  \"regr.rse\"  \n[16] \"regr.rsq\"   \"regr.sae\"   \"regr.smape\" \"regr.srho\"  \"regr.sse\"  \n\n\nMeasure objects can be created with a single performance measure (msr()) or multiple (msrs()):\n\nmeasure = msr(\"regr.rmse\")\nmeasures = msrs(c(\"regr.rmse\", \"regr.sse\"))\n\nAt the core of all performance measures is a quantification of the difference between the predicted value and the ground truth value (except for unsupervised tasks, which we will discuss later). This means that in order to assess performance, we usually need the ground truth data – observations for which we do not know the true value cannot be used to assess the quality of the predictions of a model. This is why we make predictions on the data the model did not use during training (the test set).\nAs we have seen above, mlr3’s Prediction objects contain both predictions and ground truth. The Measure objects define how prediction and ground truth are compared, and how differences between them are quantified. We choose root mean squared error (regr.rmse) as our performance measure for this example. Once the measure is created, we can pass it to the $score() method of the Prediction object to quantify the predictive performance of our model.\n\nmeasure = msr(\"regr.rmse\")\nmeasure\n\n<MeasureRegrSimple:regr.rmse>: Root Mean Squared Error\n* Packages: mlr3, mlr3measures\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n\npredictions$score(measure)\n\nregr.rmse \n 3.328844 \n\n\n\n\n\n\n\n\nNote\n\n\n\n$score() can be called without a measure; in this case the default measure for the type of task is used. Regression defaults to mean squared error (regr.mse).\n\n\nIt is possible to calculate multiple measures at the same time by passing multiple measures to $score(). For example, to compute both root mean squared error regr.rmse and mean squared error regr.mse:\n\nmeasures = msrs(c(\"regr.rmse\", \"regr.mse\"))\npredictions$score(measures)\n\nregr.rmse  regr.mse \n 3.328844 11.081203 \n\n\nmlr3 also provides measures that do not quantify the quality of the predictions of a model, but other information we may be interested in, for example the time it took to train the model and make predictions:\n\nmeasures = msrs(c(\"time_train\", \"time_predict\"))\npredictions$score(measures, learner = learner_lm)\n\n  time_train time_predict \n       0.003        0.003 \n\n\nNote that these measures require a trained learner in addition to the predictions.\nSome measures have hyperparameters themselves, for example selected_features. This measure gives information on the features the model used and is only supported by learners that have the “selected_features” property. It requires a task and a learner in addition to the predictions. The lm model does not support showing selected features; we use the rpart learner again and the full mtcars task.\n\ntask_mtcars = tsk(\"mtcars\")\nsplits = partition(task_mtcars)\nlearner_rpart = lrn(\"regr.rpart\", minsplit = 10)\n\nlearner_rpart$train(task_mtcars, splits$train)\npredictions = learner_rpart$predict(task_mtcars, splits$test)\nmeasure = msr(\"selected_features\")\npredictions$score(measure, task = task_mtcars, learner = learner_rpart)\n\nselected_features \n                2 \n\n\nThe hyperparameter of the measure specifies whether the number of selected features should be normalized by the total number of features. The default is FALSE, giving the absolute number of features that, in this case, the trained decision tree uses. We can change the hyperparameter in the same way as for learners, for example:\n\nmeasure = msr(\"selected_features\", normalize = TRUE)\npredictions$score(measure, task = task_mtcars, learner = learner_rpart)\n\nselected_features \n              0.2 \n\n\nWe have now seen the basic building blocks of mlr3 – creating and partitioning a task, instantiating a learner and setting its hyperparameters, training a model and inspecting it, making predictions, and assessing the quality of the model with a performance measure. So far, we have focused on regression, where we want to predict a numeric quantity. The rest of this chapter looks at other task types. The general procedure it the same, but some details are different."
  },
  {
    "objectID": "basics.html#sec-classif",
    "href": "basics.html#sec-classif",
    "title": "2  Fundamentals",
    "section": "\n2.4 Classification",
    "text": "2.4 Classification\nClassification predicts a discrete, categorical target instead of the continuous numeric quantity for regression. The models that learn to classify data are different from regression models, and regression learners are not applicable for classification problems (although for some learners, there are both regression and classification versions). mlr3 distinguishes between the different tasks and learner types through different R6 classes and different prefixes – regression learners and measures start with regr., whereas classification learners and measures start with classif..\n\n2.4.1 Classification Tasks\nThe mlr_tasks dictionary that comes with mlr3 contains several classification tasks (TaskClassif). We can show only the classification tasks by converting the dictionary to a data.table and filtering on the task_type:\n\nas.data.table(mlr_tasks)[task_type == \"classif\"]\n\n                key                                     label task_type nrow\n 1:   breast_cancer                   Wisconsin Breast Cancer   classif  683\n 2:   german_credit                             German Credit   classif 1000\n 3:            ilpd                 Indian Liver Patient Data   classif  583\n 4:            iris                              Iris Flowers   classif  150\n 5:       optdigits Optical Recognition of Handwritten Digits   classif 5620\n 6:        penguins                           Palmer Penguins   classif  344\n 7: penguins_simple                Simplified Palmer Penguins   classif  333\n 8:            pima                      Pima Indian Diabetes   classif  768\n 9:           sonar                    Sonar: Mines vs. Rocks   classif  208\n10:            spam                         HP Spam Detection   classif 4601\n11:         titanic                                   Titanic   classif 1309\n12:            wine                              Wine Regions   classif  178\n13:             zoo                               Zoo Animals   classif  101\n9 variables not shown: [ncol, properties, lgl, int, dbl, chr, fct, ord, pxc]\n\n\nWe will use the penguins dataset as a running example:\n\ntask_penguins = tsk(\"penguins\")\ntask_penguins\n\n<TaskClassif:penguins> (344 x 8): Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (7):\n  - int (3): body_mass, flipper_length, year\n  - dbl (2): bill_depth, bill_length\n  - fct (2): island, sex\n\n\nJust like for regression tasks, printing it gives an overview of the task, including the number of observations and features, and their types.\nThe target variable, species, is of type factor and has the following three classes or levels:\n\nunique(task_penguins$data(cols = \"species\"))\n\n     species\n1:    Adelie\n2:    Gentoo\n3: Chinstrap\n\n\nClassification tasks (TaskClassif) can also be plotted using autoplot(). Apart from the “pairs” plot type that we show here, “target” and “duo” are available. We refer the interested reader to the documentation of mlr3viz::autoplot.TaskClassif for an explanation of the other options. To keep the plot readable, we select only the first two features of the dataset.\n\nlibrary(\"mlr3viz\")\n\ntask_penguins_small = task_penguins$clone()\ntask_penguins_small$select(head(task_penguins_small$feature_names, 2))\nautoplot(task_penguins_small, type = \"pairs\")\n\n\n\nOverview of the penguins dataset.\n\n\n\n\n\n2.4.2 Classification Learners\nClassification learners (LearnerClassif) are a different R6 class than regression learners (LearnerRegr), but also inherit from the base class Learner. We can instantiate a classification learner in the same way as a regression learner, by retrieving it from the mlr_learners dictionary using lrn(). Note the classif. prefix to denote that we want a learner that classifies observations:\n\nlearner_rpart = lrn(\"classif.rpart\")\nlearner_rpart\n\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\nJust like regression learners, classification learners have hyperparameters we can set to change their behavior, and printing the learner object gives some basic information about it. Training a model and making predictions works in the same way as for regression:\n\nsplits = partition(task_penguins)\nlearner_rpart$train(task_penguins, splits$train)\nlearner_rpart$model\n\nn= 231 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 231 129 Adelie (0.441558442 0.199134199 0.359307359)  \n  2) flipper_length< 206.5 143  43 Adelie (0.699300699 0.293706294 0.006993007)  \n    4) bill_length< 44.2 101   3 Adelie (0.970297030 0.029702970 0.000000000) *\n    5) bill_length>=44.2 42   3 Chinstrap (0.047619048 0.928571429 0.023809524) *\n  3) flipper_length>=206.5 88   6 Gentoo (0.022727273 0.045454545 0.931818182)  \n    6) bill_depth>=17.2 7   3 Chinstrap (0.285714286 0.571428571 0.142857143) *\n    7) bill_depth< 17.2 81   0 Gentoo (0.000000000 0.000000000 1.000000000) *\n\npredictions = learner_rpart$predict(task_penguins, splits$test)\npredictions\n\n<PredictionClassif> for 113 observations:\n    row_ids     truth  response\n          2    Adelie    Adelie\n          3    Adelie    Adelie\n         10    Adelie    Adelie\n---                            \n        332 Chinstrap Chinstrap\n        335 Chinstrap Chinstrap\n        341 Chinstrap    Adelie\n\n\nJust like predictions of regression models, we can plot classification predictions with autoplot():\n\nlibrary(\"mlr3viz\")\nautoplot(predictions)\n\n\n\nComparing predicted and ground truth values for the penguins dataset.\n\n\n\n\n\n2.4.2.1 Changing the Prediction Type\nClassification problems support two types of predictions: the default “response”, i.e. the class label, and “prob”, which gives the probability for each class label. Not all learners support predicting probabilities.\nThe prediction type for a learner can be changed by setting $predict_type. After retraining the learner, all predictions have class probabilities (one for each class) in addition to the response, which is the class with the highest probability:\n\nlearner_rpart$predict_type = \"prob\"\nlearner_rpart$train(task_penguins, splits$train)\npredictions = learner_rpart$predict(task_penguins, splits$test)\npredictions\n\n<PredictionClassif> for 113 observations:\n    row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n          2    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n          3    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n         10    Adelie    Adelie  0.97029703     0.02970297  0.00000000\n---                                                                   \n        332 Chinstrap Chinstrap  0.04761905     0.92857143  0.02380952\n        335 Chinstrap Chinstrap  0.04761905     0.92857143  0.02380952\n        341 Chinstrap    Adelie  0.97029703     0.02970297  0.00000000\n\n\n\n\n\n\n\n\nTip\n\n\n\nSection 2.6.1 shows how to list learners that support the probability prediction type.\n\n\n\n2.4.3 Classification Evaluation\nEvaluation measures for classification problems that are supported by mlr3 can be found in the mlr_measures dictionary:\n\nmlr_measures$keys(\"classif\")\n\n [1] \"classif.acc\"         \"classif.auc\"         \"classif.bacc\"       \n [4] \"classif.bbrier\"      \"classif.ce\"          \"classif.costs\"      \n [7] \"classif.dor\"         \"classif.fbeta\"       \"classif.fdr\"        \n[10] \"classif.fn\"          \"classif.fnr\"         \"classif.fomr\"       \n[13] \"classif.fp\"          \"classif.fpr\"         \"classif.logloss\"    \n[16] \"classif.mauc_au1p\"   \"classif.mauc_au1u\"   \"classif.mauc_aunp\"  \n[19] \"classif.mauc_aunu\"   \"classif.mbrier\"      \"classif.mcc\"        \n[22] \"classif.npv\"         \"classif.ppv\"         \"classif.prauc\"      \n[25] \"classif.precision\"   \"classif.recall\"      \"classif.sensitivity\"\n[28] \"classif.specificity\" \"classif.tn\"          \"classif.tnr\"        \n[31] \"classif.tp\"          \"classif.tpr\"        \n\n\nSome of these measures require the predictition type to be “prob” (e.g. classif.auc). As the default is “response”, using those measures requires to change the prediction type, as shown above. You can check what prediction type a measure requires by looking at $predict_type.\n\nmeasure = msr(\"classif.acc\")\nmeasure$predict_type\n\n[1] \"response\"\n\n\nOnce we have created a classification measure, we can give it to the $score() method to compute its value for a given PredictionClassif object:\n\npredictions$score(measure)\n\nclassif.acc \n  0.9557522 \n\n\n\n2.4.3.1 Confusion Matrix\nA popular way to show the quality of prediction of a classification model is a confusion matrix. It gives a quick overview of what observations are misclassified, and how they are misclassified. The rows in a confusion matrix are the predicted class and the columns are the true class. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified. More information on Wikipedia2.2 https://en.wikipedia.org/wiki/Confusion_matrix\nmlr3 supports confusion matrices through the $confusion property of the PredictionClassif object:\n\npredictions$confusion\n\n           truth\nresponse    Adelie Chinstrap Gentoo\n  Adelie        49         3      0\n  Chinstrap      1        19      1\n  Gentoo         0         0     40\n\n\nIn this case, our classifier does fairly well classifying the penguins.\n\n2.4.4 Binary Classification and Positive Classes\nClassification problems with a two-class target are called binary classification tasks. Binary Classification is special in the sense that one of these classes is denoted positive and the other one negative. You can specify the positive class for a classification task object during task creation. If not explicitly set during construction, the positive class defaults to the first level of the target feature.\n\n# during construction\ndata(\"Sonar\", package = \"mlbench\")\ntask_sonar = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\n\n# switch positive class to level 'M'\ntask_sonar$positive = \"M\"\n\n\n2.4.5 Thresholding\nModels trained on binary classification tasks that predict the probability for the positive class usually use a simple rule to determine the predicted class label – if the probability is more than 50%, predict the positive label; otherwise, predict the negative label. In some cases, you may want to adjust this threshold, for example, if the classes are very unbalanced (i.e., one is much more prevalent than the other). For example, in the “german_credit” dataset, the credit risk is good for far more observations.\nTraining a classifier on this data overpredicts the majority class, i.e. the more prevalent class is more likely to be predicted for any given observation.\n\ntask_credit = tsk(\"german_credit\")\nsplits = partition(task_credit)\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner$train(task_credit)\npredictions = learner$predict(task_credit)\nautoplot(predictions)\n\n\n\nComparing predicted and ground truth values for the german_credit dataset.\n\n\n\n\nChanging the prediction threshold allows to address this without having to adjust the hyperparameters of the learner or retrain the model.\n\npredictions$set_threshold(0.7)\nautoplot(predictions)\n\n\n\nComparing predicted and ground truth values for the german_credit dataset with adjusted threshold.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThresholds can be tuned automatically with respect to prediction performance with the mlr3pipelines package using PipeOpTuneThreshold. This is covered in Chapter 6.\n\n\nThresholding For Multiple Classes\nFor classification tasks with more than two classes you can also adjust the prediction threshold, which is 0.5 for each class by default. Thresholds work slightly differently with multiple classes:\n\nThe probability for a data point is divided by each class threshold resulting in n ratios for n classes.\nThe highest ratio is selected (ties are random by default).\n\nLowering the threshold for a class means that it is more likely to be predicted and raising it has the opposite effect. The zoo dataset illustrates this concept nicely.\nWhen trained normally some classes are not predicted at all:\n\ntask = tsk(\"zoo\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner$train(task)\npreds = learner$predict(task)\n\nautoplot(preds)\n\n\n\nComparing predicted and ground truth values for the zoo dataset.\n\n\n\n\nThe classes amphibian and insect are never predicted. On the other hand, the classes mollusc and reptile are predicted more often than they appear in the truth data. We can address this by lowering the threshold for amphibian and insect. $set_threshold() can be given a named list to set the threshold for all classes at once:\n\n# c(\"mammal\", \"bird\", \"reptile\", \"fish\", \"amphibian\", \"insect\", \"mollusc.et.al\")\nnew_thresh = c(0.5, 0.5, 0.5, 0.5, 0.4, 0.4, 0.5)\nnames(new_thresh) = task$class_names\n\nautoplot(preds$set_threshold(new_thresh))\n\n\n\nComparing predicted and ground truth values for the zoo dataset with adjusted thresholds.\n\n\n\n\nWe can again see that adjusting the thresholds results in better predictive performance, without having to retrain a model."
  },
  {
    "objectID": "basics.html#sec-task-types-add",
    "href": "basics.html#sec-task-types-add",
    "title": "2  Fundamentals",
    "section": "\n2.5 Additional Task Types",
    "text": "2.5 Additional Task Types\nIn addition to regression and classification, mlr3 supports more types of tasks:\n\nClustering (mlr3cluster::TaskClust in package mlr3cluster): An unsupervised task to identify similar groups within the feature space.\nSurvival (mlr3proba::TaskSurv in package mlr3proba): The target is the (right-censored) time to an event.\nDensity (mlr3proba::TaskDens in package mlr3proba): An unsupervised task to estimate the undetectable underlying probability distribution, based on observed data (as a numeric vector or a one-column matrix-like object).\n\nOther task types that are less common are described in Chapter 8."
  },
  {
    "objectID": "basics.html#sec-lrns-add",
    "href": "basics.html#sec-lrns-add",
    "title": "2  Fundamentals",
    "section": "\n2.6 Additional Learners",
    "text": "2.6 Additional Learners\nAs mentioned above, mlr3 supports many learners. They can be accessed through three packages: the mlr3 package, the mlr3learners package, and the mlr3extralearners package.\nThe list of learners included in the mlr3 package is dliberately small to avoid large sets of dependencies for this core package:\n\nFeatureless classifier classif.featureless: Simple baseline classification learner. Predicts the label that is most frequent in the training set. It can be used as a “fallback learner” to make predictions if another, more sophisticated, learner fails for some reason.\nFeatureless regressor regr.featureless: Simple baseline regression learner. Predicts the mean of the target values in the training set.\nRpart decision tree learner classif.rpart: Tree learner from rpart.\nRpart regression tree learner regr.rpart: Tree learner from rpart.\n\nThe mlr3learners package contains cherry-picked implementations of the most popular machine learning methods:\n\nLinear (regr.lm) and logistic (classif.log_reg) regression.\nPenalized Generalized Linear Models (regr.glmnet, classif.glmnet), possibly with built-in optimization of the penalization parameter (regr.cv_glmnet, classif.cv_glmnet).\n(Kernelized) \\(k\\)-Nearest Neighbors regression (regr.kknn) and classification (classif.kknn).\nKriging / Gaussian Process Regression (regr.km).\nLinear (classif.lda) and Quadratic (classif.qda) Discriminant Analysis.\nNaïve Bayes Classification (classif.naive_bayes).\nSupport-Vector machines (regr.svm, classif.svm).\nGradient Boosting (regr.xgboost, classif.xgboost).\nRandom Forests for regression and classification (regr.ranger, classif.ranger).\n\nA complete list of supported learners across all mlr3 packages is hosted on our website3.3 https://mlr-org.com/learners.html\nThe dictionary mlr_learners contains the supported learners and changes as packages are loaded. At the time of writing, mlr3 supports six learners, mlr3learners 21 learners, mlr3extralearners 88 learners, mlr3proba five learners, and mlr3cluster 19 learners.\n\n2.6.1 Listing Learners\nYou can list all learners by converting the mlr_learners dictionary into a data.table:\n\nas.data.table(mlr_learners)\n\n                     key                       label task_type\n  1:  classif.AdaBoostM1           Adaptive Boosting   classif\n  2:         classif.C50            Tree-based Model   classif\n  3:         classif.IBk           Nearest Neighbour   classif\n  4:         classif.J48            Tree-based Model   classif\n  5:        classif.JRip Propositional Rule Learner.   classif\n ---                                                          \n136: surv.priority_lasso              Priority Lasso      surv\n137:         surv.ranger               Random Forest      surv\n138:          surv.rfsrc               Random Forest      surv\n139:            surv.svm      Support Vector Machine      surv\n140:        surv.xgboost           Gradient Boosting      surv\n4 variables not shown: [feature_types, packages, properties, predict_types]\n\n\nThe resulting data.table contains a lot of meta-data that is useful for identifying learners that have particular properties. For example, we can list all learners that support regression problems:\n\nas.data.table(mlr_learners)[task_type == \"regr\"]\n\n              key                                     label task_type\n 1:      regr.IBk                       K-nearest neighbour      regr\n 2:  regr.M5Rules                      Rule-based Algorithm      regr\n 3:    regr.abess Fast Best Subset Selection for Regression      regr\n 4:     regr.bart        Bayesian Additive Regression Trees      regr\n 5: regr.catboost                         Gradient Boosting      regr\n---                                                                  \n35:    regr.rpart                           Regression Tree      regr\n36:      regr.rsm                    Response Surface Model      regr\n37:      regr.rvm                  Relevance Vector Machine      regr\n38:      regr.svm                                      <NA>      regr\n39:  regr.xgboost                                      <NA>      regr\n4 variables not shown: [feature_types, packages, properties, predict_types]\n\n\nWe can check multiple conditions, to for example find all learners that support regression problems and can predict standard errors:\n\nas.data.table(mlr_learners)[task_type == \"regr\" &\n    sapply(predict_types, function(x) \"se\" %in% x)]\n\n                key                                    label task_type\n1:       regr.debug             Debug Learner for Regression      regr\n2:       regr.earth Multivariate Adaptive Regression Splines      regr\n3: regr.featureless           Featureless Regression Learner      regr\n4:         regr.gam    Generalized Additive Regression Model      regr\n5:         regr.glm            Generalized Linear Regression      regr\n6:          regr.km                                     <NA>      regr\n7:          regr.lm                                     <NA>      regr\n8:         regr.mob       Model-based Recursive Partitioning      regr\n9:      regr.ranger                                     <NA>      regr\n4 variables not shown: [feature_types, packages, properties, predict_types]\n\n\nOr we can list all learners that support classification problems and missing feature values:\n\nas.data.table(mlr_learners)[task_type == \"classif\" &\n    sapply(properties, function(x) \"missings\" %in% x)]\n\n                         key                              label task_type\n 1:              classif.C50                   Tree-based Model   classif\n 2:              classif.J48                   Tree-based Model   classif\n 3:             classif.PART                   Tree-based Model   classif\n 4:         classif.catboost                  Gradient Boosting   classif\n 5:            classif.debug   Debug Learner for Classification   classif\n 6:      classif.featureless Featureless Classification Learner   classif\n 7:              classif.gbm                  Gradient Boosting   classif\n 8: classif.imbalanced_rfsrc           Imbalanced Random Forest   classif\n 9:         classif.lightgbm                  Gradient Boosting   classif\n10:            classif.rfsrc                      Random Forest   classif\n11:            classif.rpart                Classification Tree   classif\n12:          classif.xgboost                               <NA>   classif\n4 variables not shown: [feature_types, packages, properties, predict_types]"
  },
  {
    "objectID": "basics.html#exercises",
    "href": "basics.html#exercises",
    "title": "2  Fundamentals",
    "section": "\n2.7 Exercises",
    "text": "2.7 Exercises\n\nUsing the Sonar dataset, measure the classification error (classif.ce) of a classification tree model (classif.rpart) trained with default hyperparameters on 80% of the data and tested on the remaining 20%.\nGive the true positive, false positive, true negative, and false negative rates of the predictions made by the model in problem 1.\nChange the threshold of the model from problem 1 such that the false positive rate is lower than the false negative rate. Give a reason why you might do this.\n\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nHastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Springer New York. https://doi.org/10.1007/978-0-387-21606-5.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in r. Springer Publishing Company, Incorporated."
  },
  {
    "objectID": "performance.html#quick-start",
    "href": "performance.html#quick-start",
    "title": "3  Resampling and Benchmarking",
    "section": "\n3.1 Quick Start",
    "text": "3.1 Quick Start\nIn the previous chapter, we have applied the holdout method by manually partitioning the data contained in a Task object into a single training set (to train the model) and a single test set (to estimate the generalization performance). As a quick start into resampling and benchmarking with the mlr3 package, we show a short example of how to do this with the resample() and benchmark() convenience functions. Specifically, we show how to estimate the generalization performance of a learner on a given task by the holdout method using resample() and how to usebenchmark() to compare two learners on a task.\nWe first define the corresponding Task and Learner objects used throughout this chapter as follows:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\n\n\n\n\nThe code example below shows how to apply holdout (specified using rsmp(\"holdout\")) on the penguins task to estimate classification accuracy of a decision tree from the rpart package:\n\nresampling = rsmp(\"holdout\")\nrr = resample(task = task, learner = learner, resampling = resampling)\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9391304 \n\n\nThe benchmark() function internally uses the resample() function to estimate the performance based on a resampling strategy. For illustration, we show a minimal code example that compares the classification accuracy of the decision tree against a featureless learner which always predicts the majority class:\n\nlrns = c(learner, lrn(\"classif.featureless\"))\nd = benchmark_grid(task = task, learner = lrns, resampling = resampling)\nbmr = benchmark(design = d)\nacc = bmr$aggregate(msr(\"classif.acc\"))\nacc[, .(task_id, learner_id, classif.acc)]\n\n    task_id          learner_id classif.acc\n1: penguins       classif.rpart   0.9565217\n2: penguins classif.featureless   0.4695652\n\n\nFurther details on resampling and benchmarking can be found in the subsequent sections Section 3.2 and Section 3.3."
  },
  {
    "objectID": "performance.html#sec-resampling",
    "href": "performance.html#sec-resampling",
    "title": "3  Resampling and Benchmarking",
    "section": "\n3.2 Resampling Strategies",
    "text": "3.2 Resampling Strategies\nExisting resampling strategies differ in how they partition the available data into training and test set, and a comprehensive overview can be found in Japkowicz and Shah (2011). For example, the \\(k\\)-fold cross-validation method randomly partitions the data into \\(k\\) subsets, called folds (see Figure 3.2). Then \\(k\\) models are trained on training data consisting of \\(k-1\\) of the folds, with the remaining fold being used as test data exactly once in each of the \\(k\\) iterations. The \\(k\\) performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate. Several variations of cross-validation exist, including repeated \\(k\\)-fold cross-validation where the entire process illustrated in Figure 3.2 is repeated multiple times, and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.\nOther well-known resampling strategies include subsampling and bootstrapping. Subsampling — also known as repeated holdout — repeats the holdout method and creates multiple train-test splits, taking into account the ratio of observations to be included in the training sets. Bootstrapping creates training sets by randomly drawing observations from all available data with replacement. Some observations in the training sets may appear more than once, while the other observations that do not appear at all are used as test set. The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment.  Properties and pitfalls of different resampling techniques have been widely studied and discussed in the literature, see e.g., Bengio and Grandvalet (2003), Molinaro, Simon, and Pfeiffer (2005), Kim (2009), Bischl et al. (2012).\n\n\n\n\n\nFigure 3.2: Illustration of a 3-fold cross-validation.\n\n\n\n\nIn mlr3, many resampling strategies have already been implemented so that users do not have to implement them from scratch, which can be tedious and error-prone. In this section, we cover how to use mlr3 to\n\n\nquery implemented resampling strategies,\n\nconstruct resampling objects for a selected resampling strategy,\n\ninstantiate the train-test splits of a resampling object on a given task, and\n\nexecute the selected resampling strategy on a learning algorithm to obtain resampling results.\n\n\n3.2.1 Query\nAll implemented resampling strategies can be queried by looking at the mlr_resamplings dictionary. The dictionary contains several common resampling strategies, including holdout, (repeated) cross-validation, bootstrap, and subsampling. Passing the dictionary to the as.data.table function provides a more structured output with additional information:\n\nas.data.table(mlr_resamplings)\n\n           key                         label        params iters\n1:   bootstrap                     Bootstrap ratio,repeats    30\n2:      custom                 Custom Splits                  NA\n3:   custom_cv Custom Split Cross-Validation                  NA\n4:          cv              Cross-Validation         folds    10\n5:     holdout                       Holdout         ratio     1\n6:    insample           Insample Resampling                   1\n7:         loo                 Leave-One-Out                  NA\n8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n9: subsampling                   Subsampling ratio,repeats    30\n\n\nFor example, the column params shows the parameters of each resampling strategy (e.g., the train-test splitting ratio or the number of repeats) and the column iters shows the default value for the number of performed resampling iterations (i.e., the number of model fits).\n\n3.2.2 Construction\nOnce we have decided on a resampling strategy, we have to construct a Resampling object via the function rsmp() which will define the resampling strategy we want to employ. For example, to construct a Resampling object for holdout, we use the value of the key column from the mlr_resamplings dictionary and pass it to the convenience function rsmp():\n\nresampling = rsmp(\"holdout\")\nprint(resampling)\n\n<ResamplingHoldout>: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.6667\n\n\nBy default, the holdout method will use 2/3 of the data as training set and 1/3 as test set. We can adjust this by specifying the ratio parameter for holdout either during construction or by updating the ratio parameter afterwards. For example, we construct a Resampling object for holdout with a 80:20 split (see first line in the code below) then update to 50:50 (see second line in the code below):\n\nresampling = rsmp(\"holdout\", ratio = 0.8)\nresampling$param_set$values = list(ratio = 0.5)\n\nHoldout only estimates the generalization performance using a single test set. To obtain a more reliable performance estimate by making use of all available data, we may use other resampling strategies. For example, we could also set up a 10-fold cross-validation via\n\nresampling = rsmp(\"cv\", folds = 10)\n\nBy default, the $is_instantiated field of a Resampling object constructed as shown above is set to FALSE. This means that the resampling strategy is not yet applied to a task, i.e., the train-test splits are not contained in the Resampling object. \n\n\n\n3.2.3 Instantiation\n\n\n\nTo generate the train-test splits for a given task, we need to instantiate a resampling strategy by calling the $instantiate() method of the previously constructed Resampling object on a Task. This will manifest a fixed partition and store the row indices for the training and test sets directly in the Resampling object. We can access these rows via the $train_set() and $test_set() methods:\n\nresampling = rsmp(\"holdout\", ratio = 0.8)\nresampling$instantiate(task)\ntrain_ids = resampling$train_set(1)\ntest_ids = resampling$test_set(1)\nstr(train_ids)\n\n int [1:275] 2 3 4 5 6 7 8 9 10 11 ...\n\nstr(test_ids)\n\n int [1:69] 1 12 16 22 26 28 30 32 37 38 ...\n\n\nInstantiation is especially relevant is when the aim is to fairly compare multiple learners. Here, it is crucial to use the same train-test splits to obtain comparable results. That is, we need to ensure that all learners to be compared use the same training data to build a model and that they use the same test data to evaluate the model performance. This can be achieved using the same instantiated Resampling object for each learner or using the benchmark() function introduced in Section 3.3 which automatically instantiates the same train-test splits for each task.\n\n\n\n\n\n3.2.4 Execution\n\n\nCalling the function resample() on a task, learner, and constructed resampling object returns a ResampleResult object which contains all information needed to estimate the generalization performance. Specifically, the function will internally use the learner to train a model for each training set determined by the resampling strategy and store the model predictions of each test set:  \n\nresampling = rsmp(\"cv\", folds = 5)\nrr = resample(task, learner, resampling)\nprint(rr)\n\n<ResampleResult> of 5 iterations\n* Task: penguins\n* Learner: classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\nHere, we used 5-fold cross-validation as resampling strategy. The resulting ResampleResult object (stored as rr) provides various methods to access the stored information. The two most relevant methods for performance assessment are $score() and $aggregate().\nThe $score() method uses Measure objects to calculate the performance measure of each resampling iteration separately.  By default, it uses the model predictions and the corresponding ground truth values stored in the test set of the corresponding resampling iteration to calculate the performance measure. Alternatively, we can set the argument predict_sets = \"train\" within the $score() method to calculate the performance measure of each resampling iteration based on the training set instead of the test set. If we do not explicitly pass a Measure object to the $score() method, the classification error (classif.ce) and the mean squared error (regr.mse) are used as defaults for classification and regression tasks respectively. In the code example below, we explicitly use the classification accuracy (classif.acc) as performance measure and pass it to the $score() method to obtain the estimated performance of each resampling iteration separately:\n\nacc = rr$score(msr(\"classif.acc\"))\nacc[, .(iteration, classif.acc)]\n\n   iteration classif.acc\n1:         1   0.8985507\n2:         2   0.9855072\n3:         3   0.8695652\n4:         4   0.9420290\n5:         5   0.9558824\n\n\nSimilarly, we can pass Measure objects to the $aggregate() method to calculate an aggregated score across all resampling iterations. The type of aggregation is usually determined by the Measure object (see also the fields $average and $aggregator the in help page of Measure for more details). By default, the classification accuracy specified by msr(\"classif.acc\") uses the macro average, i.e., the accuracy is calculated in each resampling iteration separately and then averaged to obtain the macro-averaged performance estimate. Using the option average = \"micro\" combines the predictions of each resampling iteration into a single Prediction object and computes a micro-averaged performance estimate:\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9303069 \n\nrr$aggregate(msr(\"classif.acc\", average = \"micro\"))\n\nclassif.acc \n  0.9302326 \n\n\n\n\nThe aggregated score refers to the generalization performance of our selected learner on the given task estimated by the resampling strategy defined in the Resampling object. It can be useful to also look at the individual performance values of each resampling iteration (as returned by the $score() method) to see if one (or more) of the iterations lead to very different performance results.\n\n3.2.5 Inspect ResampleResult Objects\n\n\n\n\n\n\nSome readers may want to skip this section of the book.\n\n\n\n\n\n\nIn this section, we briefly show how to inspect some important fields and methods of a ResampleResult object.\nEach resampling iteration involves a training step and a prediction step. Learner-specific error or warning messages may occur at each of these two steps. If the learner passed to the resample() function runs in an encapsulated framework that allows logging (see the $encapsulate field of a Learner object), all potential warning or error messages will be stored in the $warnings and $errors fields of the ResampleResult object.\n\n\n\n\n\n\nA ResampleResult object internally stores Prediction objects produced by each intermediate model from each resampling iteration, as these predictions are required to calculate a performance measure. The list of Prediction objects of each resampling iteration can be extracted by the $predictions() method:\n\npred = rr$predictions()\nstr(pred)\n\nList of 5\n $ :Classes 'PredictionClassif', 'Prediction', 'R6' <PredictionClassif> \n $ :Classes 'PredictionClassif', 'Prediction', 'R6' <PredictionClassif> \n $ :Classes 'PredictionClassif', 'Prediction', 'R6' <PredictionClassif> \n $ :Classes 'PredictionClassif', 'Prediction', 'R6' <PredictionClassif> \n $ :Classes 'PredictionClassif', 'Prediction', 'R6' <PredictionClassif> \n\n\nThis allows to analyze the predictions of individual intermediate models or to manually compute a macro-averaged performance estimate. Instead, we can use the $prediction() method to extract a single Prediction object that combines the predictions of each intermediate model. The combined prediction object can be used to manually compute a micro-averaged performance estimate, for example:\n\npred = rr$prediction()\npred\n\n<PredictionClassif> for 344 observations:\n    row_ids     truth  response\n          7    Adelie    Adelie\n          9    Adelie    Adelie\n         11    Adelie    Adelie\n---                            \n        333 Chinstrap Chinstrap\n        338 Chinstrap Chinstrap\n        339 Chinstrap Chinstrap\n\npred$score(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9302326 \n\n\nBy default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the ResampleResult object and because only the predictions are required to calculate the performance measure. However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models.\nTo do so, we can configure the resample() function to keep the fitted intermediate models by setting the store_models argument to TRUE. Each model trained in a specific resampling iteration is then stored in the resulting ResampleResult object and can be accessed via $learners[[i]]$model, where i refers to the i-th resampling iteration:\n\nrr = resample(task, learner, resampling, store_models = TRUE)\nrr$learners[[1]]$model\n\nn= 275 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 275 152 Adelie (0.447272727 0.192727273 0.360000000)  \n  2) flipper_length< 206.5 171  50 Adelie (0.707602339 0.286549708 0.005847953)  \n    4) bill_length< 43.35 122   4 Adelie (0.967213115 0.032786885 0.000000000) *\n    5) bill_length>=43.35 49   4 Chinstrap (0.061224490 0.918367347 0.020408163) *\n  3) flipper_length>=206.5 104   6 Gentoo (0.019230769 0.038461538 0.942307692)  \n    6) bill_depth>=17.2 8   4 Chinstrap (0.250000000 0.500000000 0.250000000) *\n    7) bill_depth< 17.2 96   0 Gentoo (0.000000000 0.000000000 1.000000000) *\n\n\n\n\n3.2.6 Custom Resampling\n\n\n\n\n\n\nSome readers may want to skip this section of the book.\n\n\n\n\n\n\nSometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds or to implement a special splitting structure as required for time series. A custom resampling strategy can be constructed using rsmp(\"custom\"), where the row indices of the observations used for training and testing must be defined manually when instantiated in a task. In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the $train and $test fields.\n\nresampling = rsmp(\"custom\")\nresampling$instantiate(task,\n  train = list(c(1:50, 151:333)),\n  test = list(51:150)\n)\n\nThe resulting Resampling object can then be used like all other resampling strategies. To show that both sets contain the row indices we have defined, we can inspect the instantiated Resampling object:\n\nstr(resampling$train_set(1))\n\n int [1:233] 1 2 3 4 5 6 7 8 9 10 ...\n\nstr(resampling$test_set(1))\n\n int [1:100] 51 52 53 54 55 56 57 58 59 60 ...\n\n\nThe above is equivalent to a single custom train-test split analogous to the holdout strategy. A custom version of the cross-validation strategy can be constructed using rsmp(\"custom_cv\"). The important difference is that we now have to specify either a custom factor variable (using the f argument of the $instantiate() method) or a factor column (using the col argument of the $instantiate() method) from the data to determine the folds.\nIn the example below, we instantiate a custom 4-fold cross-validation strategy using a factor variable called folds that contains 4 equally sized levels to define the 4 folds, each with one quarter of the total size of the \"penguin\" task:\n\ncustom_cv = rsmp(\"custom_cv\")\nfolds = as.factor(rep(1:4, each = task$nrow/4))\ncustom_cv$instantiate(task, f = folds)\ncustom_cv\n\n<ResamplingCustomCV>: Custom Split Cross-Validation\n* Iterations: 4\n* Instantiated: TRUE\n* Parameters: list()\n\n\n\n3.2.7 Resampling with Stratification and Grouping\n\n\n\n\n\n\nThis section of the book might be complex for some readers.\n\n\n\n\n\n\nIn mlr3, we can assign a special role to a feature contained in the data by configuring the corresponding $col_roles field of a Task. The two relevant column roles that will affect behavior of a resampling strategy are \"group\" or \"stratum\", whose meaning is explained in detail below.\nIn some cases, it is desirable to keep observations together when the data is split into corresponding training and test sets, especially when a set of observations naturally belong to a group (e.g., when the data contains repeated measurements of individuals). When observations belong to groups, we want to ensure that all observations of the same group belong to either the training set or the test set.\nIn mlr3, the column role \"group\" allows to specify the column in the data that defines the group structure of the observations (see also the help page of Resampling for more information on the column role \"group\"). The column role can be specified by assigning a feature to the $col_roles$group field which will then determine the group structure. The following code uses 3-fold cross-validation and the feature year of the penguins task to determine the grouping. Since the feature year contains only three distinct values (i.e., 2007, 2008, and 2009), the corresponding test sets consist of observations from only one year:\n\n\n\n\ntask_grp = tsk(\"penguins\")\ntask_grp$col_roles$group = \"year\"\nr = rsmp(\"cv\", folds = 3)\nr$instantiate(task_grp)\n\ntable(task_grp$data(cols = \"year\"))\n\nyear\n2007 2008 2009 \n 110  114  120 \n\ntable(task_grp$data(rows = r$test_set(1), cols = \"year\"))\n\nyear\n2009 \n 120 \n\ntable(task_grp$data(rows = r$test_set(2), cols = \"year\"))\n\nyear\n2008 \n 114 \n\ntable(task_grp$data(rows = r$test_set(3), cols = \"year\"))\n\nyear\n2007 \n 110 \n\n\n\n\n\n\nAnother column role available in mlr3 is \"stratum\", which implements stratified sampling. Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations. This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration.  The $col_roles$stratum field of a Task can be set to one or multiple features (including the target in case of classification tasks). In case of multiple features, each combination of the values of all stratification features will form a strata. For example, the target column species of the penguins task is imbalanced:\n\nprop.table(table(task$data(cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4418605 0.1976744 0.3604651 \n\n\nWithout specifying a \"stratum\" column role, the species column will have quite different class distributions across the corresponding test sets of a 3-fold cross-validation strategy:\n\nr = rsmp(\"cv\", folds = 3)\nr$instantiate(task)\nprop.table(table(task$data(rows = r$test_set(1), cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4869565 0.2000000 0.3130435 \n\nprop.table(table(task$data(rows = r$test_set(2), cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4086957 0.1913043 0.4000000 \n\nprop.table(table(task$data(rows = r$test_set(3), cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4298246 0.2017544 0.3684211 \n\n\nThe code below uses species as \"stratum\" column role to illustrate that the distribution of species in each test set will closely match the original distribution:\n\ntask_str = tsk(\"penguins\")\ntask_str$col_roles$stratum = \"species\"\nr = rsmp(\"cv\", folds = 3)\nr$instantiate(task_str)\n\nprop.table(table(task_str$data(rows = r$test_set(1), cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4396552 0.1982759 0.3620690 \n\nprop.table(table(task_str$data(rows = r$test_set(2), cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4434783 0.2000000 0.3565217 \n\nprop.table(table(task_str$data(rows = r$test_set(3), cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n0.4424779 0.1946903 0.3628319 \n\n\nRather than assigning the $col_roles$stratum directly, it is also possible to use the $set_col_roles() method to add or remove columns to specific roles incrementally:\n\ntask_str$set_col_roles(\"species\", remove_from = \"stratum\")\ntask_str$col_roles$stratum\n\ncharacter(0)\n\ntask_str$set_col_roles(\"species\", add_to = \"stratum\")\ntask_str$col_roles$stratum\n\n[1] \"species\"\n\n\nWe can further inspect the current stratification via the $strata field, which returns a data.table of the number of observations (N) and row indices (row_id) of each stratum. Since we stratified by the species column, we expect to see the same class frequencies as when we tabulate the task by the species column:\n\ntask_str$strata\n\n     N                      row_id\n1: 152             1,2,3,4,5,6,...\n2: 124 153,154,155,156,157,158,...\n3:  68 277,278,279,280,281,282,...\n\ntable(task$data(cols = \"species\"))\n\nspecies\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nShould we add another stratification column, the $strata field will show the same values as when we cross-tabulate the two variables of the task:\n\ntask_str$set_col_roles(\"year\", add_to = \"stratum\")\n\ntask_str$strata\n\n    N                      row_id\n1: 50             1,2,3,4,5,6,...\n2: 50       51,52,53,54,55,56,...\n3: 52 101,102,103,104,105,106,...\n4: 34 153,154,155,156,157,158,...\n5: 46 187,188,189,190,191,192,...\n6: 44 233,234,235,236,237,238,...\n7: 26 277,278,279,280,281,282,...\n8: 18 303,304,305,306,307,308,...\n9: 24 321,322,323,324,325,326,...\n\ntable(task$data(cols = c(\"species\", \"year\")))\n\n           year\nspecies     2007 2008 2009\n  Adelie      50   50   52\n  Chinstrap   26   18   24\n  Gentoo      34   46   44\n\n\n\n3.2.8 Plotting Resample Results\nmlr3viz provides a autoplot() method to automatically visualize the resampling results either in a boxplot or histogram:\n\nresampling = rsmp(\"bootstrap\")\nrr = resample(task, learner, resampling)\n\nlibrary(mlr3viz)\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"boxplot\")\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"histogram\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nThe histogram is useful to visually gauge the variance of the performance results across resampling iterations, whereas the boxplot is often used when multiple learners are compared side-by-side.\nWe can also visualize a 2-dimensional prediction surface of individual models in each resampling iteration if the task is restricted to two features:\n\ntask$select(c(\"bill_length\", \"flipper_length\"))\nresampling = rsmp(\"cv\", folds = 4)\nrr = resample(task, learner, resampling, store_models = TRUE)\nautoplot(rr, type = \"prediction\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nPrediction surfaces like this are a useful tool for model inspection, as they can help to identify the cause of unexpected performance result. Naturally, they are also popular for didactical purposes to illustrate the prediction behaviour of different learning algorithms, such as the classification tree in the example above with its characteristic orthogonal lines."
  },
  {
    "objectID": "performance.html#sec-benchmarking",
    "href": "performance.html#sec-benchmarking",
    "title": "3  Resampling and Benchmarking",
    "section": "\n3.3 Benchmarking",
    "text": "3.3 Benchmarking\n\nBenchmarking is used to compare the performance of different learning algorithms applied on one or more tasks using (potentially different) resampling strategies. The purpose is to rank the learning algorithms regarding a performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks. The mlr3 package offers the convenience function benchmark() to conduct a benchmark experiment and repeatedly train and evaluate multiple learners under the same conditions. In this section, we cover how to\n\n\nconstruct a benchmark design to define the benchmark experiments to be performed,\n\nrun the benchmark experiments and aggregate their results, and\n\nconvert benchmark objects to other types of objects that can be used for different purposes.\n\n\n3.3.1 Constructing Benchmarking Designs\nIn mlr3, we can define a design to perform benchmark experiments via the benchmark_grid() convenience function. The design is essentially a table of scenarios to be evaluated and usually consists of unique combinations of Task, Learner and Resampling triplets.\nThe benchmark_grid() function constructs an exhaustive design to describe which combinations of learner, task and resampling should be used in a benchmark experiment. It properly instantiates the used resampling strategies so that all learners are evaluated on the same train-test splits for each task, ensuring a fair comparison. To construct a list of Task, Learner and Resampling objects, we can use the convenience functions tsks(), lrns(), and rsmps().\n\n\nWe design an exemplary benchmark experiment and train a classification tree from the rpart package, a random forest from the ranger package and a featureless learner serving as a baseline on four different binary classification tasks. The constructed benchmark design is a data.table containing the task, learner, and resampling combinations in each row that should be performed:\n\nlibrary(\"mlr3verse\")\n\ntsks = tsks(c(\"spam\", \"german_credit\", \"sonar\", \"breast_cancer\"))\nlrns = lrns(c(\"classif.ranger\", \"classif.rpart\", \"classif.featureless\"),\n  predict_type = \"prob\")\nrsmp = rsmps(\"cv\", folds = 5)\n\ndesign = benchmark_grid(tsks, lrns, rsmp)\nhead(design)\n\n                task                         learner         resampling\n1: <TaskClassif[50]>      <LearnerClassifRanger[38]> <ResamplingCV[20]>\n2: <TaskClassif[50]>       <LearnerClassifRpart[38]> <ResamplingCV[20]>\n3: <TaskClassif[50]> <LearnerClassifFeatureless[38]> <ResamplingCV[20]>\n4: <TaskClassif[50]>      <LearnerClassifRanger[38]> <ResamplingCV[20]>\n5: <TaskClassif[50]>       <LearnerClassifRpart[38]> <ResamplingCV[20]>\n6: <TaskClassif[50]> <LearnerClassifFeatureless[38]> <ResamplingCV[20]>\n\n\n\nSince the data.table contains R6 columns within list-columns, we unfortunately can not infer too much about task column, but the ids utility function can be used for quick inspection or subsetting:\n\nmlr3misc::ids(design$task)\n\n [1] \"spam\"          \"spam\"          \"spam\"          \"german_credit\"\n [5] \"german_credit\" \"german_credit\" \"sonar\"         \"sonar\"        \n [9] \"sonar\"         \"breast_cancer\" \"breast_cancer\" \"breast_cancer\"\n\ndesign[mlr3misc::ids(task) == \"spam\", ]\n\n                task                         learner         resampling\n1: <TaskClassif[50]>      <LearnerClassifRanger[38]> <ResamplingCV[20]>\n2: <TaskClassif[50]>       <LearnerClassifRpart[38]> <ResamplingCV[20]>\n3: <TaskClassif[50]> <LearnerClassifFeatureless[38]> <ResamplingCV[20]>\n\n\nIt is also possible to subset the design, e.g., to exclude a specific task-learner combination by manually removing a certain row from the design which is a data.table. Alternatively, we can also construct a custom benchmark design by manually defining a data.table containing task, learner, and resampling objects (see also the examples section in the help page of benchmark_grid()).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Execution of Benchmark Experiments\nTo run the benchmark experiment, we can pass the constructed benchmark design to the benchmark() function:\n\nbmr = benchmark(design)\nprint(bmr)\n\n<BenchmarkResult> of 60 rows with 12 resampling runs\n nr       task_id          learner_id resampling_id iters warnings errors\n  1          spam      classif.ranger            cv     5        0      0\n  2          spam       classif.rpart            cv     5        0      0\n  3          spam classif.featureless            cv     5        0      0\n  4 german_credit      classif.ranger            cv     5        0      0\n  5 german_credit       classif.rpart            cv     5        0      0\n  6 german_credit classif.featureless            cv     5        0      0\n  7         sonar      classif.ranger            cv     5        0      0\n  8         sonar       classif.rpart            cv     5        0      0\n  9         sonar classif.featureless            cv     5        0      0\n 10 breast_cancer      classif.ranger            cv     5        0      0\n 11 breast_cancer       classif.rpart            cv     5        0      0\n 12 breast_cancer classif.featureless            cv     5        0      0\n\n\nOnce the benchmarking is finished (this can take some time, depending on the size of your design), we can aggregate the performance results with the $aggregate() method of the returned BenchmarkResult:\n\nacc = bmr$aggregate(msr(\"classif.acc\"))\nacc[, .(task_id, learner_id, classif.acc)]\n\n          task_id          learner_id classif.acc\n 1:          spam      classif.ranger   0.9500085\n 2:          spam       classif.rpart   0.8956734\n 3:          spam classif.featureless   0.6059557\n 4: german_credit      classif.ranger   0.7600000\n 5: german_credit       classif.rpart   0.7380000\n 6: german_credit classif.featureless   0.7000000\n 7:         sonar      classif.ranger   0.8029036\n 8:         sonar       classif.rpart   0.6880372\n 9:         sonar classif.featureless   0.5335656\n10: breast_cancer      classif.ranger   0.9721876\n11: breast_cancer       classif.rpart   0.9414556\n12: breast_cancer classif.featureless   0.6500966\n\n\nAs the results are shown in a data.table, we can easily aggregate the results even further. For example, if we are interested in the learner that performed best across all tasks, we could average the performance of each individual learner across all tasks. Please note that averaging accuracy scores across multiple tasks as in this example is not always appropriate for comparison purposes. A more common alternative to compare the overall algorithm performance across multiple tasks is to first compute the ranks of each learner on each task separately and then compute the average ranks. For illustration purposes, we show how to average the performance of each individual learner across all tasks:\n\nacc[, list(mean_accuracy = mean(classif.acc)), by = \"learner_id\"]\n\n            learner_id mean_accuracy\n1:      classif.ranger     0.8712749\n2:       classif.rpart     0.8157915\n3: classif.featureless     0.6224045\n\n\nRanking the performance scores can either be done via standard data.table syntax, or more conveniently with the mlr3benchmark package. We first use as.BenchmarkAggr to aggregate the BenchmarkResult using our measure, after which we use the $rank_data() method to convert the performance scores to ranks. The minimize argument is used to indicate that the classification accuracy should not be minimized, i.e. a higher score is better.\n\nlibrary(\"mlr3benchmark\")\n\nbma = as.BenchmarkAggr(bmr, measures = msr(\"classif.acc\"))\nbma$rank_data(minimize = FALSE)\n\n            spam german_credit sonar breast_cancer\nranger         1             1     1             1\nrpart          2             2     2             2\nfeatureless    3             3     3             3\n\n\nThis results in per-task rankings of the three learners. Unsurprisingly, the featureless learner ranks last, as it always predicts the majority class. However, it is common practice to include it as a baseline in benchmarking experiments to easily gauge the relative performance of other algorithms. In this simple benchmark experiment, the random forest ranked first, outperforming a single classification tree as one would expect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Inspect BenchmarkResult Objects\n\nA BenchmarkResult object is a collection of multiple ResampleResult objects. These can be extracted via the $resample_result(i) method, where i is the index of the performed benchmark experiment. This allows us to investigate the extracted ResampleResult or individual resampling iterations as shown previously (see Section 3.2).\n\nrr1 = bmr$resample_result(1)\nrr2 = bmr$resample_result(2)\nrr1\n\n<ResampleResult> of 5 iterations\n* Task: spam\n* Learner: classif.ranger\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\nrr2\n\n<ResampleResult> of 5 iterations\n* Task: spam\n* Learner: classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\nMultiple ResampleResult can be again converted to a BenchmarkResult with the function as_benchmark_result() and combined with c():\n\nbmr1 = as_benchmark_result(rr1)\nbmr2 = as_benchmark_result(rr2)\n\nbmr_combined = c(bmr1, bmr2)\nbmr_combined$aggregate(msr(\"classif.acc\"))\n\n   nr      resample_result task_id     learner_id resampling_id iters\n1:  1 <ResampleResult[21]>    spam classif.ranger            cv     5\n2:  2 <ResampleResult[21]>    spam  classif.rpart            cv     5\n1 variable not shown: [classif.acc]\n\n\nCombining multiple BenchmarkResults into a larger result object can be useful if related benchmarks where computed on different machines.\n\nSimilar to creating automated visualizations for tasks, predictions, or resample results, the mlr3viz package also provides a autoplot() method to visualize benchmark results, by default as a boxplot:\n\nautoplot(bmr, measure = msr(\"classif.acc\"))\n\n\n\n\nSuch a plot summarizes the benchmark experiment across all tasks and learners. Visualizing performance scores across all learners and tasks in a benchmark helps identifying potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task. In the case of our example above, the three learners show consistent relative performance to each other, in the order we would expect.\n\n3.3.4 Statistical Tests\n\n\n\n\n\n\nSome readers may want to skip this section of the book.\n\n\n\n\n\n\nThe package mlr3benchmark we previously used for ranking also provides infrastructure for applying statistical significance tests on BenchmarkResult objects. Currently, Friedman tests and pairwise Friedman-Nemenyi tests (Demšar 2006) are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.\n$friedman_posthoc() can be used for a pairwise comparison:\n\nbma = as.BenchmarkAggr(bmr, measures = msr(\"classif.acc\"))\nbma$friedman_posthoc()\n\n\n    Pairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design\n\n\ndata: acc and learner_id and task_id\n\n\n            ranger rpart\nrpart       0.333  -    \nfeatureless 0.013  0.333\n\n\n\nP value adjustment method: single-step\n\n\nThese results would indicate a statistically significant difference between the \"featureless\" learner and \"ranger\", assuming a 95% confidence level.\nThe results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:\n\nautoplot(bma, type = \"cd\")\n\n\n\n\nSimilar to the test output before, this visualization leads to the conclusion that the \"featureless\" learner and \"ranger\" are significantly different, whereas the critical rank difference of 1.66 is not exceed for the comparison of the \"featureless\" learner and \"rpar\", and \"rpart\" and \"ranger\", respectively."
  },
  {
    "objectID": "performance.html#sec-roc",
    "href": "performance.html#sec-roc",
    "title": "3  Resampling and Benchmarking",
    "section": "\n3.4 ROC Analysis",
    "text": "3.4 ROC Analysis\n\n\n\nROC (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifier. Although extensions for multiclass classifiers exist (see e.g., Hand and Till (2001)), we will only cover the much easier binary classification case here. For binary classifiers that predict discrete classes, we can compute a confusion matrix from which we can derive the following performance measures:\n\n\nTrue Positive Rate (TPR), Sensitivity or Recall: How many of the true positives did we predict as positive?\n\nTrue Negative Rate (TNR) or Specificity: How many of the true negatives did we predict as negative?\n\nFalse Positive Rate (FPR), or 1 - Specificity: How many of the true negatives did we predict as positive?\n\nPositive Predictive Value (PPV) or Precision: If we predict positive how likely is it a true positive?\n\nNegative Predictive Value (NPV): If we predict negative how likely is it a true negative?\n\nIn general, it is difficult to achieve a high TPR and low FPR simultaneously. ROC analysis aims at evaluating the performance of classifiers by visualizing the trade-off between the true positive rate (TPR) and the false positive rate (FPR) which can be obtained from a confusion matrix. The best classifier lies on the top-left corner where the TPR is 1 and the FPR is 0. Classifiers on the diagonal predict class labels randomly (possibly with different class proportions). For example, if each positive \\(x\\) will be randomly classified with 25% as to the positive class, we get a TPR of 0.25. If we assign each negative \\(x\\) randomly to the positive class, we get a FPR of 0.25. In practice, we should never obtain a classifier clearly below the diagonal. Swapping the predicted classes of a classifier would results in points in the ROC space being mirrored at the diagonal baseline. A point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.\nIf a binary classifier predicts probabilities instead of discrete classes, we could arbitrarily set a threshold to cut-off the probabilities and assign them to the positive and negative class. Different thresholds may lead to different confusion matrices. In this case, we can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values – this is the ROC curve (see Figure 3.3 for illustration). A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC). The higher the AUC value, the better the performance, whereas a random classifier would result in an AUC of 0.5. \n\n\n\n\nFigure 3.3: Panel (a): Illustration of 3 points referring to classifiers predicting discrete classes in the ROC space, Panel (b): Illustration of 3 ROC curves of classifiers predicting probabilities.\n\n\n\n\nIn Chapter Chapter 2, we have already seen how we can obtain the confusion matrix of a Prediction by accessing the $confusion field. In the code example below, we first retrieve the \"sonar\" task which is a binary classification task and construct a classification tree learner that predicts probabilities using the predict_type = \"prob\" option. Next, we use the partition() helper function to randomly split the rows of the Sonar task into two disjoint set, which acts as a convenience shortcut function to the \"holdout\" resampling strategy. We train the learner on the training set and use the trained model to generate predictions on the test set. Finally, we retrieve the confusion matrix (see also Section 2.4.3 for details on confusion matrices).\n\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nsplits = partition(task, ratio = 0.8)\n\nlearner$train(task, splits$train)\npred = learner$predict(task, splits$test)\npred$confusion\n\n        truth\nresponse  M  R\n       M 11  6\n       R 11 13\n\n\nFor mlr3 prediction objects, the ROC curve can be constructed with the previously seenautoplot.PredictionClassif from mlr3viz. The x-axis showing the FPR is labelled “1 - Specificity” by convention, whereas the y-axis shows “Sensitivity” for the TPR.\n\nautoplot(pred, type = \"roc\")\n\n\n\n\nWe can also plot the precision-recall (PR) curve which visualize the PPV vs. TPR. The main difference between ROC curves and PR curves is that the number of true-negatives are not used to produce a PR curve. PR curves are preferred over ROC curves for imbalanced populations. This is because the positive class is usually rare in imbalanced classification tasks. Hence, the FPR is often low even for a random classifier. As a result, the ROC curve may not provide a good assessment of the classifier’s performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations). See also Davis and Goadrich (2006) for a detailed discussion about the relationship between the PRC and ROC curves.\n\nautoplot(pred, type = \"prc\")\n\n\n\n\nThese visualizations are also available for ResampleResult. Here, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro-averaged):\n\nrr = resample(\n  task = tsk(\"spam\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 10)\n)\n\nautoplot(rr1, type = \"roc\")\nautoplot(rr1, type = \"prc\")\n\n\n\n\n\n\n\n\n\n\n\nWe can also visualize a BenchmarkResult to compare multiple learners on the same Task:\n\ndesign = benchmark_grid(\n  tasks = tsk(\"spam\"),\n  learners = lrns(c(\"classif.rpart\", \"classif.ranger\"), predict_type = \"prob\"),\n  resamplings = rsmp(\"cv\", folds = 3)\n)\nbmr = benchmark(design)\n\nautoplot(bmr, type = \"roc\")\nautoplot(bmr, type = \"prc\")"
  },
  {
    "objectID": "performance.html#conclusion-todo",
    "href": "performance.html#conclusion-todo",
    "title": "3  Resampling and Benchmarking",
    "section": "\n3.5 Conclusion (TODO)",
    "text": "3.5 Conclusion (TODO)\nIf data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. In chapter pipelines, we introduce the mlr3pipelines package that solves this issue by combining a Learner with a pre-processing step into a more general machine learning pipeline. As the pipeline itself behaves like a Learner, we can use all functions introduced in this chapter to estimate its generalization performance.\nSee also the section about nested resampling in the chapter on model optimization when a Learner involves tuning of hyperparameters.\nFurthermore, depending on the task at hand, more complex resampling strategies might be required, e.g., for spatiotemporal data.\n\n\nTable 3.1: Core S3 ‘sugar’ functions for resampling and benchmarking in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.\n\nS3 function\nR6 Class\nSummary\n\n\nrsmp()\nResampling\nDetermines the assignment of observations to train- and test set\n\n\n\n\n\n\nResources (TODO)\n\nLearn more about advanced resampling techniques in the blog post Resampling - Stratified, Blocked and Predefined1.\nCheckout the blogpost mlr3 Basics on “Iris” - Hello World!2 to see minimal examples on using resampling and benchmarking on the iris dataset.\nUse resampling and benchmarking for the comparison of decision boundaries of classification learners3.\n1 https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/2 https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/3 https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/"
  },
  {
    "objectID": "performance.html#exercises",
    "href": "performance.html#exercises",
    "title": "3  Resampling and Benchmarking",
    "section": "\n3.6 Exercises",
    "text": "3.6 Exercises\n\nUse the spam task and 5-fold cross-validation to benchmark Random Forest (classif.ranger), Logistic Regression (classif.log_reg), and XGBoost (classif.xgboost) with regards to AUC. Which learner appears to do best? How confident are you in your conclusion? How would you improve upon this?\nA colleague claims to have achieved a 93.1% classification accuracy using the classif.rpart learner on the penguins_simple task. You want to reproduce their results and ask them about their resampling strategy. They said they used 3-fold cross-validation, and they assigned rows using the task’s row_id modulo 3 to generate three evenly sized folds. Reproduce their results using the custom CV strategy.\n\n\n\n\n\n\n\nBengio, Yoshua, and Yves Grandvalet. 2003. “No Unbiased Estimator of the Variance of k-Fold Cross-Validation.” Advances in Neural Information Processing Systems 16.\n\n\nBischl, Bernd, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012. “Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.” Evolutionary Computation 20 (2): 249–75.\n\n\nDavis, Jesse, and Mark Goadrich. 2006. “The Relationship Between Precision-Recall and ROC Curves.” In Proceedings of the 23rd International Conference on Machine Learning, 233–40.\n\n\nDemšar, Janez. 2006. “Statistical Comparisons of Classifiers over Multiple Data Sets.” Journal of Machine Learning Research 7 (1): 1–30. https://jmlr.org/papers/v7/demsar06a.html.\n\n\nHand, David J, and Robert J Till. 2001. “A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.” Machine Learning 45: 171–86.\n\n\nJapkowicz, Nathalie, and Mohak Shah. 2011. Evaluating Learning Algorithms: A Classification Perspective. Cambridge University Press.\n\n\nKim, Ji-Hyun. 2009. “Estimating Classification Error Rate: Repeated Cross-Validation, Repeated Hold-Out and Bootstrap.” Computational Statistics & Data Analysis 53 (11): 3735–45.\n\n\nMolinaro, Annette M, Richard Simon, and Ruth M Pfeiffer. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7."
  },
  {
    "objectID": "optimization.html#sec-model-tuning",
    "href": "optimization.html#sec-model-tuning",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.1 Model Tuning",
    "text": "4.1 Model Tuning\nmlr3tuning is the hyperparameter optimization package of the mlr3 ecosystem. At the heart of the package (and indeed any optimization problem) are the R6 classes\n\n\nTuningInstanceSingleCrit and TuningInstanceMultiCrit, which are used to construct a tuning ‘instance’ which describes the optimization problem and stores the results; and\n\nTuner which is used to get and set optimization algorithms.\n\nIn this section, we will cover these classes as well as other supporting functions and classes. Throughout this section, we will look at optimizing a support vector machine (SVM) on the sonar data set as a running example.\n\n4.1.1 Learner and Search Space\nWe begin by constructing a support vector machine from the e1071 with a radial kernel and specify we want to tune this using \"C-classification\" (the alternative is \"nu-classification\", which has the same underlying algorithm but with a nu parameter to tune over [0,1] instead of cost over [0, \\(\\infty\\))).\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n\nLearner hyperparameter information is stored in the $param_set field, including parameter name, class (e.g., discrete or numeric), levels it can be tuned over, tuning limits, and more.\n\nas.data.table(learner$param_set)[, list(id, class, lower, upper, nlevels)]\n\n                 id    class lower upper nlevels\n 1:       cachesize ParamDbl  -Inf   Inf     Inf\n 2:   class.weights ParamUty    NA    NA     Inf\n 3:           coef0 ParamDbl  -Inf   Inf     Inf\n 4:            cost ParamDbl     0   Inf     Inf\n 5:           cross ParamInt     0   Inf     Inf\n 6: decision.values ParamLgl    NA    NA       2\n 7:          degree ParamInt     1   Inf     Inf\n 8:         epsilon ParamDbl     0   Inf     Inf\n 9:          fitted ParamLgl    NA    NA       2\n10:           gamma ParamDbl     0   Inf     Inf\n11:          kernel ParamFct    NA    NA       4\n12:              nu ParamDbl  -Inf   Inf     Inf\n13:           scale ParamUty    NA    NA     Inf\n14:       shrinking ParamLgl    NA    NA       2\n15:       tolerance ParamDbl     0   Inf     Inf\n16:            type ParamFct    NA    NA       2\n\n\nNote that $param_set also displays non-tunable parameters. Detailed information about parameters can be found in the help pages of the underlying implementation, for this example see e1071::svm().\nGiven infinite resources, we could tune every single hyperparameter, but in reality that is not possible so instead only a subset of hyperparameters can be tuned. This subset is referred to as the search space or tuning space. In this example we will tune the regularization and influence hyperparameters, cost and gamma.Search Space\nFor numeric hyperparameters (we will explore others later) one must specify the bounds to tune over. We do this by constructing a learner and using to_tune() to set the lower and upper limits for the parameters we want to tune. This function allows us to construct a learner in the usual way but to leave the hyperparameters of interest to be unspecified within a set range. This is best demonstrated by example:\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5),\n  gamma = to_tune(1e-5, 1e5),\n  type  = \"C-classification\",\n  kernel = \"radial\"\n)\nlearner\n\n<LearnerClassifSVM:classif.svm>\n* Model: -\n* Parameters: cost=<RangeTuneToken>, gamma=<RangeTuneToken>,\n  type=C-classification, kernel=radial\n* Packages: mlr3, mlr3learners, e1071\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass\n\n\nHere we have constructed a classification SVM by setting the type to “C-classification”, the kernel to “radial”, and not fully specifying the cost and gamma hyperparameters but instead indicating that we will tune these parameters.\n\n\n\n\n\n\nNote\n\n\n\nThe cost and gamma hyperparameters are usually tuned on the logarithmic scale. You can find out more in Section 4.2.2.\n\n\nSearch spaces are usually chosen by experience. In some cases these can be quite complex, Section 9.4 describes how to construct these. Section 4.2.3 introduces the mlr3tuningspaces extension package which allows loading of search spaces that have been established in published scientific articles.\n\n4.1.2 Terminator\nTheoretically, a tuner could search an entire search space exhaustively, however practically this is not possible and mathematically this is impossible for continuous hyperparameters. Therefore a core part of configuring tuning is to specify when to terminate the algorithm, this is also known as specifying the tuning budget. mlr3tuning includes many methods to specify when to terminate an algorithm, which are known as Terminators. Available terminators are listed in Table 4.1.Tuning BudgetTerminators\n\n\nTable 4.1: Terminators available in mlr3tuning, their function call and default parameters.\n\nTerminator\nFunction call and default parameters\n\n\n\nNumber of Evaluations\ntrm(\"evals\", n_evals = 500)\n\n\nRun Time\ntrm(\"run_time\", secs = 100)\n\n\nPerformance Level\ntrm(\"perf_reached\", level = 0.1)\n\n\nStagnation\ntrm(\"stagnation\", iters = 5, threshold = 1e-5)\n\n\nNone\ntrm(\"none\")\n\n\nClock Time\ntrm(\"clock_time\", stop_time = \"2022-11-06 08:42:53 CET\"\n\n\nCombo\ntrm(\"combo\", terminators = list(run_time_100, evals_200)\n\n\n\n\nThe most commonly used terminators are those that stop the tuning after a certain time (\"run_time\") or the number of evaluations (\"evals\"). Choosing a runtime is often based on practical considerations and intuition. Using a time limit can be important on clusters so that the tuning is finished before the account budget is exhausted. The \"perf_reached\" terminator stops the tuning when a certain performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model. However, one needs to be careful using this terminator as if the level is set too optimistically, the tuning might never terminate. The \"stagnation\" terminator stops when no progress is made in a certain amount of iterations. Note, this could result in the optimization being terminated too early if the search space is too complex. We use \"none\" when tuners, such as Grid Search and Hyperband, control the termination themselves. Terminators can be freely combined with the \"combo\" terminator, this is explored in the exercises at the end of this chapter. A complete and always up-to-date list of terminators can be found on our website at https://mlr-org.com/terminators.html.\n\n4.1.3 Tuning Instance with ti\n\nA tuning instance can be constructed manually (Section 4.1.3) with the ti() function or automated (Section 4.1.6) with the tune() function. We cover the manual approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of mlr3tuning. The ti function constructs a tuning instance which collects together the information required to optimise a model.Tuning Instance\nNow continuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the ti() function to create a TuningInstanceSingleCrit (note: supplying two measures to ti() would result in TuningInstanceMultiCrit (Section 4.3)). For this example we will use three-fold resampling and will optimise the classification error measure. Note that we use trm(\"none\") as we are using an exhaustive grid search.\n\nresampling = rsmp(\"cv\", folds = 3)\n\nmeasure = msr(\"classif.ce\")\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5),\n  gamma = to_tune(1e-5, 1e5),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.svm_on_sonar>\n* Search Space:\n      id    class lower upper nlevels\n1:  cost ParamDbl 1e-05 1e+05     Inf\n2: gamma ParamDbl 1e-05 1e+05     Inf\n* Terminator: <TerminatorNone>\n\n\n\n4.1.4 Tuner\nAfter we created the tuning problem, we can look at how to tune. There are multiple Tuners in mlr3tuning, which implement different HPO algorithms.Tuners\n\n\nTable 4.2: Tuning algorithms available in mlr3tuning, their function call and the methodology.\n\nTuner\nFunction call\nMethod\n\n\n\nRandom Search\ntnr(\"random_search\")\nSamples configurations from a uniform distribution randomly (Bergstra and Bengio 2012).\n\n\nGrid Search\ntnr(\"grid_search\")\nDiscretizes the range of each configuration and exhaustively evaluates each combination.\n\n\nIterative Racing\ntnr(\"irace\")\nRaces down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space (López-Ibáñez et al. 2016).\n\n\nBayesian Optimization\ntnr(\"mbo\")\nIterative algorithms that make use of a continuously updated surrogate model built for the objective function. By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency.\n\n\nHyperband\ntnr(\"hyperband\")\nMulti-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping (Li et al. 2017).\n\n\nCovariance Matrix Adaptation Evolution Strategy\ntnr(\"cmaes\")\nEvolution strategy algorithm with sampling from a multivariate Gaussian who is updated with the success of the previous population (Hansen and Auger 2011).\n\n\nGeneralized Simulated Annealing\ntnr(\"gensa\")\nProbabilistic algorithm for numeric search spaces (Xiang et al. 2013; Tsallis and Stariolo 1996).\n\n\nNonlinear Optimization\ntnr(\"nloptr\")\nSeveral nonlinear optimization algorithms for numeric search spaces.\n\n\n\n\nWhen selecting algorithms, grid search and random search are the most basic and are often selected first in initial experiments. They are ‘naive’ algorithms in that they try new configurations whilst ignoring performance from previous attempts. In contrast, more advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly. Some advanced algorithms are included in extension packages, for example the package mlr3mbo implements Bayesian optimization (also called Model-Based Optimization), and mlr3hyperband implements algorithms of the hyperband family. A complete and up-to-date list of tuners can be found on the website.\nFor our SVM example, we will use a simple grid search with a resolution of 5, which is the distinct values to try per hyperparameter. For example for a search space \\(\\{1, 2, 3, 4, 5, 6\\}\\) then a grid search with resolution 3 would pick three values evenly apart in this search space, i.e., \\(\\{2, 4, 6\\}\\). The batch_size controls how many configurations are evaluated at the same time (see Section 9.1).\n\ntuner = tnr(\"grid_search\", resolution = 5, batch_size = 5)\ntuner\n\n<TunerGridSearch>: Grid Search\n* Parameters: resolution=5, batch_size=5\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning\n\n\nIn our example we are tuning over two numeric parameters, TunerGridSearch will create an equidistant grid between the respective upper and lower bounds. This means our two-dimensional grid of resolution 5 consists of \\(5^2 = 25\\) configurations. Each configuration is a distinct set of hyperparameter values that is used to construct a model from the chosen learner, which is fit to the chosen task (Figure 4.2).\nAll configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (Section 4.1.2) signals that the budget is exhausted.\nJust like learners, tuners also have parameters, known as control parameters, which (as the name suggests) controls the behavior of the tuners. Unlike learners, default values for control parameters usually give good results and these rarely need to be changed. Control parameters are stored in the $param_set field.Control Parameters\n\ntuner$param_set\n\n<ParamSet>\n                  id    class lower upper nlevels        default value\n1:        batch_size ParamInt     1   Inf     Inf <NoDefault[3]>     5\n2:        resolution ParamInt     1   Inf     Inf <NoDefault[3]>     5\n3: param_resolutions ParamUty    NA    NA     Inf <NoDefault[3]>      \n\n\n\n4.1.5 Trigger the Tuning\nNow we have all our components, we are ready to start tuning! To do this we simply pass the constructed TuningInstanceSingleCrit to the $optimize() method of the initialized Tuner. The tuner then proceeds with the HPO loop we discussed at the beginning of the chapter (Figure 4.2).\n\ntuner$optimize(instance)\n\n    cost gamma learner_param_vals  x_domain classif.ce\n1: 25000 1e-05          <list[4]> <list[2]>  0.2358178\n\n\nThe optimizer returns the best hyperparameter configuration and the corresponding measured performance. This information is also stored in instance$result.\n\n\n\n\n\n\nNote\n\n\n\nThe column x_domain contains transformed values and learner_param_vals optional constants (none in this example). See section Section 4.2.2 for more information.\n\n\n\n4.1.6 Quick Tuning with tune\n\nIn the previous section, we looked at creating a tuning instance manually using ti(), which offers more control over the tuning process. However, you can also simplify this (albeit with slightly less control) using the tune() sugar function. Internally this creates a TuningInstanceSingleCrit, starts the tuning and returns the result with the instance.\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5),\n  gamma = to_tune(1e-5, 1e5),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\ninstance = tune(\n  method = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\")\n)\n\ninstance$result\n\n    cost gamma learner_param_vals  x_domain classif.ce\n1: 25000 1e-05          <list[4]> <list[2]>  0.2309179\n\n\n\n4.1.7 Analyzing the Result\nWhether you use ti or tune the output is the same and the ‘archive’ lists all evaluated hyperparameter configurations:\n\nas.data.table(instance$archive)[, list(cost, gamma, classif.ce)]\n\n       cost   gamma classif.ce\n 1: 5.0e+04 5.0e+04  0.4661836\n 2: 5.0e+04 1.0e+05  0.4661836\n 3: 7.5e+04 7.5e+04  0.4661836\n 4: 1.0e+05 1.0e-05  0.2499655\n 5: 1.0e+05 7.5e+04  0.4661836\n---                           \n21: 1.0e-05 7.5e+04  0.4661836\n22: 1.0e-05 1.0e+05  0.4661836\n23: 2.5e+04 2.5e+04  0.4661836\n24: 1.0e+05 2.5e+04  0.4661836\n25: 1.0e+05 5.0e+04  0.4661836\n\n\nEach row of the archive is a different evaluated configuration (there are 25 rows in total in the full data.table). The columns here show the tested configurations, the measure we optimize, the completed configuration time stamp, and the total train and predict times. If we only specify a single-objective criterium then the instance will return the configuration that optimizes this measure however we can manually inspect the archive to determine other important features. For example, how long did the model take to run? Were there any errors in running?\n\nas.data.table(instance$archive)[,\n  list(timestamp, runtime_learners, errors, warnings)]\n\n              timestamp runtime_learners errors warnings\n 1: 2023-02-27 19:32:38            0.059      0        0\n 2: 2023-02-27 19:32:38            0.063      0        0\n 3: 2023-02-27 19:32:38            0.069      0        0\n 4: 2023-02-27 19:32:38            0.052      0        0\n 5: 2023-02-27 19:32:38            0.067      0        0\n---                                                     \n21: 2023-02-27 19:32:41            0.060      0        0\n22: 2023-02-27 19:32:41            0.066      0        0\n23: 2023-02-27 19:32:41            0.059      0        0\n24: 2023-02-27 19:32:41            0.067      0        0\n25: 2023-02-27 19:32:41            0.059      0        0\n\n\nNow we see not only was our optimal configuration the best performing with respect to classification error, but also it had the fastest runtime.\nAnother powerful feature of the instance is that we can score the internal ResampleResults on a different performance measure, for example looking at false negative rate (FNR) and false positive rate (FPR) as well as classification error:\n\nas.data.table(instance$archive,\n  measures = msrs(c(\"classif.fpr\", \"classif.fnr\")))[,\n  list(cost, gamma, classif.ce, classif.fpr, classif.fnr)]\n\n       cost   gamma classif.ce classif.fpr classif.fnr\n 1: 5.0e+04 5.0e+04  0.4661836   1.0000000    0.000000\n 2: 5.0e+04 1.0e+05  0.4661836   1.0000000    0.000000\n 3: 7.5e+04 7.5e+04  0.4661836   1.0000000    0.000000\n 4: 1.0e+05 1.0e-05  0.2499655   0.2628968    0.232703\n 5: 1.0e+05 7.5e+04  0.4661836   1.0000000    0.000000\n---                                                   \n21: 1.0e-05 7.5e+04  0.4661836   1.0000000    0.000000\n22: 1.0e-05 1.0e+05  0.4661836   1.0000000    0.000000\n23: 2.5e+04 2.5e+04  0.4661836   1.0000000    0.000000\n24: 1.0e+05 2.5e+04  0.4661836   1.0000000    0.000000\n25: 1.0e+05 5.0e+04  0.4661836   1.0000000    0.000000\n\n\nNow we see our model is also the best performing with respect to FPR and FNR!\nYou can view all the resamplings in a BenchmarkResult object with instance$archive$benchmark_result.\nFinally, for more visually appealing results you can use mlr3viz (Figure 4.3).\n\nautoplot(instance, type = \"surface\")\n\n\n\nFigure 4.3: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high cost values and gamma values around exp(-5) achieve the best performance.\n\n\n\n\n\n4.1.8 Using a tuned model\nOnce the learner has been tuned we can start to use it like any other model in the mlr3 universe. To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters with the optimal configurations:\n\nsvm_tuned = lrn(\"classif.svm\", id = \"SVM Tuned\")\nsvm_tuned$param_set$values = instance$result_learner_param_vals\n\nNow we can train the learner on the full dataset and we are ready to make predictions. The trained model can then be used to predict new, external data:\n\nsvm_tuned$train(tsk(\"sonar\"))\nsvm_tuned$model\n\n\nCall:\nsvm.default(x = data, y = task$truth(), type = \"C-classification\", \n    kernel = \"radial\", gamma = 1e-05, cost = 25000.0000075, probability = (self$predict_type == \n        \"prob\"))\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  25000 \n\nNumber of Support Vectors:  89\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (instance$result$classif.ce) as the model’s performance. However, doing so would lead to bias and therefore nested resampling is required (Section 4.5). Therefore when tuning as above ensure that you do not make any statements about model performance without testing the model on more unseen data. We will come back to this in more detail in Section 4.4."
  },
  {
    "objectID": "optimization.html#advanced-tuning",
    "href": "optimization.html#advanced-tuning",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.2 Advanced Tuning",
    "text": "4.2 Advanced Tuning\n\n4.2.1 Encapsulation and Fallback Learner\nSo far, we have only looked at the case where no issues occur. However, it often happens that learners with certain configurations do not converge, run out of memory, or terminate with an error. We can protect the tuning process from failing learners with encapsulation. The encapsulation separates the tuning from the training of the individual learner. The encapsulation method is set in the learner.\n\nlearner$encapsulate = c(train = \"evaluate\", predict = \"evaluate\")\n\nThe encapsulation can be set individually for training and predicting. There are currently two options for encapsulating a learner. The evaluate package and the callr package. The callr package comes with more overhead because the encapsulation spawns a separate R process. Both packages allow setting a timeout which is useful when a learner does not converge. We set a timeout of 30 seconds.\n\nlearner$timeout = c(train = 30, predict = 30)\n\nWith encapsulation, exceptions and timeouts do not stop the tuning. Instead, the error message is recorded and a fallback learner is fitted.\nFallback learners allow scoring a result when no model was fitted during training. A common approach is to predict a weak baseline e.g. predicting the mean of the data or just the majority class. See ?sec-fallback-learner for more detailed information.\nThe featureless learner predicts the most frequent label.\n\nlearner$fallback = lrn(\"classif.featureless\")\n\nErrors and warnings that occurred during tuning are stored in the archive.\n\nas.data.table(instance$archive)[, list(cost, gamma, classif.ce, errors, warnings)]\n\n       cost   gamma classif.ce errors warnings\n 1: 5.0e+04 5.0e+04  0.4661836      0        0\n 2: 5.0e+04 1.0e+05  0.4661836      0        0\n 3: 7.5e+04 7.5e+04  0.4661836      0        0\n 4: 1.0e+05 1.0e-05  0.2499655      0        0\n 5: 1.0e+05 7.5e+04  0.4661836      0        0\n---                                           \n21: 1.0e-05 7.5e+04  0.4661836      0        0\n22: 1.0e-05 1.0e+05  0.4661836      0        0\n23: 2.5e+04 2.5e+04  0.4661836      0        0\n24: 1.0e+05 2.5e+04  0.4661836      0        0\n25: 1.0e+05 5.0e+04  0.4661836      0        0\n\n\n\n4.2.2 Advanced Search Spaces\nUsually, the cost and gamma hyperparameters are tuned on the logarithmic scale which means the optimization algorithm searches in \\([log(1e-5), log(1e5)]\\) but transforms the selected configuration with exp() before passing to the learner. Using the log transformation emphasizes smaller values but can also result in large values. The code below demonstrates this more clearly. The histograms show how the algorithm searches within a narrow range but exponentiating then results in the majority of points being relatively small but a few being very large.\n\ncost = runif(1000, log(1e-5), log(1e5))\n\n\n\n\n\n\n(a) cost values sampled by the optimization algorithm.\n\n\n\n\n\n\n(b) exp(cost) values seen by the learner.\n\n\n\n\nFigure 4.4: Histogram of sampled cost values.\n\n\nTo add the exp() transformation to a hyperparameter, we pass logscale = TRUE to to_tune().\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\ninstance = tune(\n  method = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\")\n)\n\ninstance$result\n\n       cost     gamma learner_param_vals  x_domain classif.ce\n1: 5.756463 -5.756463          <list[4]> <list[2]>  0.2014493\n\n\nThe column x_domain contains the hyperparameter values after the transformation i.e. exp(5.76) and exp(-5.76):\n\ninstance$result$x_domain\n\n[[1]]\n[[1]]$cost\n[1] 316.2278\n\n[[1]]$gamma\n[1] 0.003162278\n\n\n\n4.2.3 Search Spaces Collection\nSelected search spaces can require a lot of background knowledge or expertise. The package mlr3tuningspaces tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms. These search spaces should be applicable to a wide range of data sets, however, they may need to be adapted in specific situations. The search spaces are stored in the dictionary mlr_tuning_spaces.\n\nas.data.table(mlr_tuning_spaces)\n\n                       key                              label        learner\n 1: classif.glmnet.default    Classification GLM with Default classif.glmnet\n 2:    classif.glmnet.rbv2  Classification GLM with RandomBot classif.glmnet\n 3:   classif.kknn.default   Classification KKNN with Default   classif.kknn\n 4:      classif.kknn.rbv2 Classification KKNN with RandomBot   classif.kknn\n 5: classif.ranger.default Classification Ranger with Default classif.ranger\n---                                                                         \n20:        regr.rpart.rbv2    Regression Rpart with RandomBot     regr.rpart\n21:       regr.svm.default        Regression SVM with Default       regr.svm\n22:          regr.svm.rbv2      Regression SVM with RandomBot       regr.svm\n23:   regr.xgboost.default    Regression XGBoost with Default   regr.xgboost\n24:      regr.xgboost.rbv2  Regression XGBoost with RandomBot   regr.xgboost\n1 variable not shown: [n_values]\n\n\nThe tuning spaces are named according to the scheme {learner-id}.{publication}. The sugar function lts() is used to retrieve a TuningSpace.\n\nlts(\"classif.rpart.default\")\n\n<TuningSpace:classif.rpart.default>: Classification Rpart with Default\n          id lower upper levels logscale\n1:  minsplit 2e+00 128.0            TRUE\n2: minbucket 1e+00  64.0            TRUE\n3:        cp 1e-04   0.1            TRUE\n\n\nA tuning space can be passed to ti() as the search_space.\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts(\"classif.rpart.rbv2\")\n)\ninstance\n\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.rpart_on_sonar>\n* Search Space:\n          id    class    lower upper nlevels\n1:        cp ParamDbl -9.21034     0     Inf\n2:  maxdepth ParamInt  1.00000    30      30\n3: minbucket ParamInt  1.00000   100     100\n4:  minsplit ParamInt  1.00000   100     100\n* Terminator: <TerminatorEvals>\n\n\nAlternatively, we can explicitly set the search space of a learner with TuneTokens\n\nvals = lts(\"classif.rpart.default\")$values\nvals\n\n$minsplit\nTuning over:\nrange [2, 128] (log scale)\n\n\n$minbucket\nTuning over:\nrange [1, 64] (log scale)\n\n\n$cp\nTuning over:\nrange [1e-04, 0.1] (log scale)\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$set_values(.values = vals)\nlearner\n\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0, minsplit=<RangeTuneToken>,\n  minbucket=<RangeTuneToken>, cp=<RangeTuneToken>\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\nWhen passing a learner to lts(), the default search space from the Bischl et al. (2021) article is applied.\n\nlts(lrn(\"classif.rpart\"))\n\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0, minsplit=<RangeTuneToken>,\n  minbucket=<RangeTuneToken>, cp=<RangeTuneToken>\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\nIt is possible to simply overwrite a predefined tuning space in construction, for example here we change the range of the nrounds hyperparameter in XGBoost:\n\nlts(\"classif.xgboost.rbv2\", nrounds = to_tune(1, 1024))\n\n<TuningSpace:classif.xgboost.rbv2>: Classification XGBoost with RandomBot\n                   id lower upper               levels logscale\n 1:           booster    NA    NA gblinear,gbtree,dart    FALSE\n 2:           nrounds 1e+00  1024                         FALSE\n 3:               eta 1e-04     1                          TRUE\n 4:             gamma 1e-05     7                          TRUE\n 5:            lambda 1e-04  1000                          TRUE\n 6:             alpha 1e-04  1000                          TRUE\n 7:         subsample 1e-01     1                         FALSE\n 8:         max_depth 1e+00    15                         FALSE\n 9:  min_child_weight 1e+00   100                          TRUE\n10:  colsample_bytree 1e-02     1                         FALSE\n11: colsample_bylevel 1e-02     1                         FALSE\n12:         rate_drop 0e+00     1                         FALSE\n13:         skip_drop 0e+00     1                         FALSE"
  },
  {
    "objectID": "optimization.html#sec-multi-metrics-tuning",
    "href": "optimization.html#sec-multi-metrics-tuning",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.3 Multi-Objective Tuning",
    "text": "4.3 Multi-Objective Tuning\nSo far we have considered optimizing a model with respect to one metric but multi-metric, or multi-objective optimization is also possible. A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive and false negative predictions. In a more complex example, consider the problem of deploying a classifier in a healthcare setting, there is clearly an ethical argument to tune the model to make the best possible predictions, however in machine learning this can often lead to models that are harder to interpret (think about deep neural networks!). In this case, we may be interested in minimizing both classification error (for example) and complexity.Multi-objective Optimization\nIn general, when optimizing multiple metrics, these will be in competition (if they were not we would only need to optimize with respect to one of them!) and so no single configuration exists that optimizes all metrics. Focus is therefore given to the concept of Pareto optimality. One hyperparameter configuration is said to Pareto-dominate another one if the resulting model is equal or better in all metrics and strictly better in at least one metric. All configurations that are not Pareto-dominated are referred to as Pareto efficient and the set of all these configurations is referred to as the Pareto front (Figure 4.5).Pareto Front\nThe goal of multi-objective HPO is to approximate the true, unknown Pareto front. More methodological details on multi-objective HPO can be found in Karl et al. (2022).\nWe will now demonstrate multi-objective HPO by tuning a decision tree on the Spam data set with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables). We will tune\n\nThe complexity hyperparameter cp that controls when the learner considers introducing another branch.\nThe minsplit hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.\nThe maxdepth hyperparameter that limits the depth of the tree.\n\n\nlearner = lrn(\"classif.rpart\",\n  cp = to_tune(1e-04, 1e-1, logscale = TRUE),\n  minsplit = to_tune(2, 128, logscale = TRUE),\n  maxdepth = to_tune(1, 30)\n)\n\nmeasures = msrs(c(\"classif.ce\", \"selected_features\"))\n\nNote that as we tune with respect to multiple measures, the function ti creates a TuningInstanceMultiCrit instead of a TuningInstanceSingleCrit.\n\ninstance = ti(\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = measures,\n  terminator = trm(\"evals\", n_evals = 20),\n  store_models = TRUE  # required to inspect selected_features\n)\ninstance\n\n<TuningInstanceMultiCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.rpart_on_spam>\n* Search Space:\n         id    class      lower     upper nlevels\n1:       cp ParamDbl -9.2103404 -2.302585     Inf\n2: minsplit ParamDbl  0.6931472  4.859812     Inf\n3: maxdepth ParamInt  1.0000000 30.000000      30\n* Terminator: <TerminatorEvals>\n\n\nAs before we will then select and run a tuning algorithm, here we use random search:\n\ntuner = tnr(\"random_search\", batch_size = 20)\ntuner$optimize(instance)\n\nFinally, we inspect the best-performing configurations, i.e., the Pareto set. And then inspect the estimated Pareto set and visualize the estimated Pareto front:\n\ninstance$archive$best()[, list(cp, minsplit, maxdepth, classif.ce, selected_features)]\n\n           cp minsplit maxdepth classif.ce selected_features\n 1: -4.897655 3.338026       17 0.10302293          7.666667\n 2: -7.051424 3.896918       12 0.10041637         12.333333\n 3: -4.245193 1.541127        8 0.10780375          5.666667\n 4: -2.774911 3.148014       30 0.16387611          2.333333\n 5: -4.807622 1.987267       11 0.10432672          7.333333\n 6: -4.853549 4.375677       19 0.10671726          6.000000\n 7: -3.739909 1.772185       26 0.11193239          5.000000\n 8: -4.937744 1.286328       29 0.10302293          7.666667\n 9: -4.553218 2.750349       22 0.10563050          6.666667\n10: -3.714380 2.555973       25 0.11193239          5.000000\n11: -5.076189 2.735166       11 0.10085053          9.333333\n12: -4.519566 4.470367       30 0.10845564          5.333333\n13: -5.445322 1.663845       23 0.08715871         13.000000\n14: -6.465093 1.536021       16 0.07759311         29.000000\n15: -6.528747 2.155507        6 0.08585422         16.333333\n\n\n\n\n\n\nFigure 4.5: Pareto front of selected features and classification error. Black dots represent tested configurations, each red dot individually represents a Pareto-optimal configuration and all red dots together represent the Pareto front."
  },
  {
    "objectID": "optimization.html#sec-autotuner",
    "href": "optimization.html#sec-autotuner",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.4 Automated Tuning with AutoTuner\n",
    "text": "4.4 Automated Tuning with AutoTuner\n\nOne of the most powerful classes in mlr3 is the AutoTuner. The AutoTuner wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters – this allows transparent tuning of any learner, without the need to extract information on the best hyperparameter settings at the end. As the AutoTuner itself inherits from the Learner base class, it can be used like any other learner!\nLet us see this in practice. We will run the exact same example as above but this time using the AutoTuner for automated tuning:\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\nat = auto_tuner(\n  method = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\")\n)\n\nat\n\n<AutoTuner:classif.svm.tuned>\n* Model: list\n* Search Space:\n<ParamSet>\n      id    class     lower    upper nlevels        default value\n1:  cost ParamDbl -11.51293 11.51293     Inf <NoDefault[3]>      \n2: gamma ParamDbl -11.51293 11.51293     Inf <NoDefault[3]>      \nTrafo is set.\n* Packages: mlr3, mlr3tuning, mlr3learners, e1071\n* Predict Type: response\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass\n\n\nWe can now use this like any other learner, calling the $train() and $predict() methods. The key difference to a normal learner, is that calling $train() also tunes the model.\n\ntask = tsk(\"sonar\")\nsplit = partition(task)\nat$train(task, row_ids = split$train)\nat$predict(task, row_ids = split$test)$score()\n\nclassif.ce \n 0.2608696 \n\n\nWe could also pass the AutoTuner to resample() and benchmark(), which would result in a nested resampling (Section 4.5), discussed next."
  },
  {
    "objectID": "optimization.html#sec-nested-resampling",
    "href": "optimization.html#sec-nested-resampling",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.5 Nested Resampling",
    "text": "4.5 Nested Resampling\nHyperparameter optimization generally requires an additional layer or resampling to prevent bias in tuning. If the same data is used for determining the optimal configuration and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased (Simon 2007). This is analogous to optimism of the training error described in (James et al. 2014), which occurs when training error is taken as an estimate of out-of-sample performance. This bias is represented in Figure 4.6 which shows an algorithm being tuned on data that has been split intro training and testing data, and then the same data is used to estimate the model performance after selecting the best configuration after HPO.\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing': 25}}}%%\nflowchart LR\n    search[(Search Space)]\n    opt[(Optimal<br>configuration)]\n    train[(Train)]\n    test[(Test)]\n    alg[/Algorithm/]\n    search --> alg\n    alg --> train\n    alg --> train\n    train --> test\n    train --> test\n    test --> opt\n    test --> perf(Performance)\n    opt --> alg\n\n    linkStyle 1,3,6,7 stroke-width:2px, stroke:red;\n\n\n\n\n\nFigure 4.6: Illustration of biased tuning. An algorithm is tuned by training on the Train dataset and then the optimal configuration is selected by evaluation on the Test data. The model’s performance is then evaluated with the optimal configuration on the same data.\n\n\n\n\nNested resampling separates model optimization from the process of estimating the performance of the model by adding an additional layer of resampling, i.e., whilst model performance is estimated using a resampling method in the ‘usual way’, tuning is then performed by resampling the resampled data (Figure 4.7). For more details and a formal introduction to nested resampling the reader is referred to Bischl et al. (2021).Nested Resampling\nA common confusion is how and when to use nested resampling. In the rest of this section we will answer the ‘how’ question but first the ‘when’. A common mistake is to confuse nested resampling for model evaluation and comparison, with tuning for model deployment. To put it differently, nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset, it is not a procedure to select optimal hyperparameters. Nested resampling produces many hyperparameter configurations which should not be used to construct a final model (Simon 2007).\n\n\n\n\nFigure 4.7: An illustration of nested resampling. The green blocks represent 3-fold coss-validation for the outer resampling for model evaluation and the blue and gray blocks represent 4-fold cross-validation for the inner resampling for HPO.\n\n\n\n\nIn words this process runs as follows:\n\nOuter resampling – Instantiate 3-fold cross-validation to create different testing and training data sets.\nInner resampling – Within the training data instantiate 4-fold cross-validation to create different inner testing and training data sets.\nHPO – Tune the hyperparameters using the inner data splits (blue and gray blocks).\nTraining – Fit the learner on the outer training data set using the optimal hyperparameter configuration obtained from the inner resampling (dark green blocks).\nEvaluation – Evaluate the performance of the learner on the outer testing data (light green blocks).\nCross-validation – Repeat (2)-(5) for each of the three folds.\nAggregation – Take the sample mean of the three performance values for an unbiased performance estimate.\n\nThat is enough theory for now, let us take a look at how this works in mlr3.\n\n4.5.1 Nested Resampling with AutoTuner\n\nNested resampling in mlr3 becomes quite simple with the AutoTuner (Section 4.4). We simply specify the inner-resampling and tuning setup with the AutoTuner and then pass this to resample() or benchmark(). Continuing with our previous example we will use the auto-tuner to resample a support vector classifier with 3-fold cross-validation in the outer-resampling and 4-fold cross-validation in the inner resampling.\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\nat = auto_tuner(\n  method = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.ce\"),\n  term_evals = 20,\n)\n\ntask = tsk(\"sonar\")\nouter_resampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n\nrr\n\n<ResampleResult> of 3 iterations\n* Task: sonar\n* Learner: classif.svm.tuned\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\nNote we set store_models = TRUE so that the AutoTuner models are stored to investigate the inner tuning. In this example, we utilized the same resampling strategy (K-fold cross-validation) but the mlr3 infrastructure is not limited to this, you can freely combine different inner and outer resampling strategies as you choose. You can also mix-and-match parallelization methods for controlling the process (Section 9.1.4).\nThere are some special functions for nested resampling available in addition to the methods described in Section 3.2.\nThe extract_inner_tuning_results() and extract_inner_tuning_archives() functions return the optimal configurations (across all outer folds) and full tuning archives respectively.\n\nextract_inner_tuning_results(rr)[,\n  list(iteration, cost, gamma, classif.ce)]\n\n   iteration      cost     gamma classif.ce\n1:         1 11.512925 -5.756463  0.1663866\n2:         2  5.756463 -5.756463  0.2090336\n3:         3  5.756463 -5.756463  0.1941176\n\nextract_inner_tuning_archives(rr)[,\n  list(iteration, cost, gamma, classif.ce)]\n\n    iteration       cost      gamma classif.ce\n 1:         1 -11.512925 -11.512925  0.4567227\n 2:         1 -11.512925   5.756463  0.4567227\n 3:         1  -5.756463 -11.512925  0.4567227\n 4:         1   0.000000   5.756463  0.4567227\n 5:         1   5.756463 -11.512925  0.2747899\n---                                           \n56:         3   0.000000 -11.512925  0.4678571\n57:         3   0.000000  -5.756463  0.2151261\n58:         3   0.000000   5.756463  0.4678571\n59:         3   5.756463   0.000000  0.4678571\n60:         3  11.512925  -5.756463  0.1941176\n\n\nFrom the optimal results, we observe a trend toward larger cost and smaller gamma values. However, as we discussed earlier, these values should not be used to fit a final model as the selected hyperparameters might differ greatly between the resampling iterations. On the one hand, this could be due to the optimization algorithm used, for example, with simple algorithms like random search, we do not expect stability of hyperparameters. On the other hand, more advanced methods like irace converge to an optimal hyperparameter configuration. Another reason for instability in hyperparameters could be due to small data sets and/or a low number of resampling iterations (i.e., the usual small data high variance problem).\n\n4.5.2 Performance comparison\nFinally, we will compare the predictive performances estimated on the outer resampling to the inner resampling to gain an understanding of model overfitting and general performance.\n\nextract_inner_tuning_results(rr)[,\n  list(iteration, cost, gamma, classif.ce)]\n\n   iteration      cost     gamma classif.ce\n1:         1 11.512925 -5.756463  0.1663866\n2:         2  5.756463 -5.756463  0.2090336\n3:         3  5.756463 -5.756463  0.1941176\n\nrr$score()[,\n  list(iteration, classif.ce)]\n\n   iteration classif.ce\n1:         1  0.1285714\n2:         2  0.1014493\n3:         3  0.1884058\n\n\nSignificantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.\nIt is therefore important to ensure that the performance of a tuned model is always reported as the aggregated performance of all outer resampling iterations, which is an unbiased estimate of future model performance. Note here we use the term unbiased to refer only to the statistical procedure of the performance estimation. The underlying prediction of the model could still be biased e.g. due to a bias in the data set.\n\nrr$aggregate()\n\nclassif.ce \n 0.1394755 \n\n\nAs a final note, nested resampling is computationally expensive, as a simple example using five outer folds and three inner folds with a grid search of resolution 5 used to tune 2 parameters, results in 535*5 = 375 iterations of model training/testing. In practice, you may often see closer to three folds used in inner resampling or even holdout, or if you have the resources then we recommend parallelization (Section 9.1)."
  },
  {
    "objectID": "optimization.html#conclusion",
    "href": "optimization.html#conclusion",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.6 Conclusion",
    "text": "4.6 Conclusion\nIn this chapter, we learned how to optimize a model using tuning instances, about different tuners and terminators, how to make use of the automated methods for quicker implementation in larger experiments, and the importance of nested resampling. The most important functions and classes we learned about are in Table 4.3 alongside their R6 classes. If you are interested in learning more about the underlying R6 classes to gain finer control of these methods, then take a look at the online API.\n\n\nTable 4.3: Core S3 ‘sugar’ functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.\n\nS3 function\nR6 Class\nSummary\n\n\n\ntnr()\nTuner\nDetermines an optimisation algorithm\n\n\ntrm()\nTerminator\nControls when to terminate the tuning algorithm\n\n\nti()\n\nTuningInstanceSingleCrit or TuningInstanceMultiCrit\n\nStores tuning settings and save results\n\n\nparadox::to_tune()\nparadox::TuneToken\nSets which parameters in a learner to tune and over what search space\n\n\nauto_tuner()\nAutoTuner\nAutomates the tuning process\n\n\nextract_inner_tuning_results()\n-\nExtracts inner results from nested resampling\n\n\nextract_inner_tuning_archives()\n-\nExtracts inner archives from nested resampling\n\n\n\n\nResources\nThe mlr3tuning cheatsheet1 summarizes the most important functions of mlr3tuning and the mlr3 gallery2 features a collection of case studies and demonstrations about optimization, most notably learn how to:1 https://cheatsheets.mlr-org.com/mlr3tuning.pdf2 https://mlr-org.com/gallery.html#category:tuning\n\nApply advanced methods in the practical tuning series3.\nOptimize an rpart classification tree with only a few lines of code4.\nTune an XGBoost model with early stopping5.\nQuickly load and tune over search spaces that have been published in literature with mlr3tuningspaces6.\n3 https://mlr-org.com/gallery.html#category:practical_tuning_series4 https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/5 https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/6 https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/"
  },
  {
    "objectID": "optimization.html#exercises",
    "href": "optimization.html#exercises",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.7 Exercises",
    "text": "4.7 Exercises\n\nTune the mtry, sample.fraction, num.trees hyperparameters of a random forest model (regr.ranger) on the Motor Trend data set (mtcars). Use a simple random search with 50 evaluations and select a suitable batch size. Evaluate with a 3-fold cross-validation and the root mean squared error.\nEvaluate the performance of the model created in Question 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling. Print the unbiased performance estimate of the model.\nTune and benchmark an XGBoost model against a logistic regression and determine which has the best Brier score. Use mlr3tuningspaces and nested resampling.\n\n\n\n\n\n\n\nBergstra, James, and Yoshua Bengio. 2012. “Random Search for Hyper-Parameter Optimization.” Journal of Machine Learning Research 13 (10): 281–305. http://jmlr.org/papers/v13/bergstra12a.html.\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2021. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges.” https://doi.org/10.48550/ARXIV.2107.05847.\n\n\nFeurer, Matthias, and Frank Hutter. 2019. “Hyperparameter Optimization.” In Automated Machine Learning: Methods, Systems, Challenges, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 3–33. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_1.\n\n\nHansen, Nikolaus, and Anne Auger. 2011. “CMA-ES: Evolution Strategies and Covariance Matrix Adaptation.” In Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation, 991–1010.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in r. Springer Publishing Company, Incorporated.\n\n\nKarl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. “Multi-Objective Hyperparameter Optimization - an Overview.” https://doi.org/10.48550/ARXIV.2206.07438.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2017. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” The Journal of Machine Learning Research 18 (1): 6765–6816.\n\n\nLópez-Ibáñez, Manuel, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres, Mauro Birattari, and Thomas Stützle. 2016. “The Irace Package: Iterated Racing for Automatic Algorithm Configuration.” Operations Research Perspectives 3: 43–58.\n\n\nSimon, Richard. 2007. “Resampling Strategies for Model Assessment and Selection.” In Fundamentals of Data Mining in Genomics and Proteomics, edited by Werner Dubitzky, Martin Granzow, and Daniel Berrar, 173–86. Boston, MA: Springer US. https://doi.org/10.1007/978-0-387-47509-7_8.\n\n\nTsallis, Constantino, and Daniel A Stariolo. 1996. “Generalized Simulated Annealing.” Physica A: Statistical Mechanics and Its Applications 233 (1-2): 395–406.\n\n\nXiang, Yang, Sylvain Gubian, Brian Suomela, and Julia Hoeng. 2013. “Generalized Simulated Annealing for Global Optimization: The GenSA Package.” R J. 5 (1): 13."
  },
  {
    "objectID": "feature-selection.html#sec-fs-filter",
    "href": "feature-selection.html#sec-fs-filter",
    "title": "5  Feature Selection",
    "section": "\n5.1 Filters",
    "text": "5.1 Filters\nFilter methods are preprocessing steps that can be applied before training a model. A very simple filter approach could look like this:\n\ncalculate the correlation coefficient \\(\\rho\\) between each feature and a numeric target variable, and\nselect all features with \\(\\rho > 0.2\\) for further modelling steps.\n\nThis approach is a univariate filter because it only considers the univariate relationship between each feature and the target variable. Further, it can only be applied to regression tasks with continuous features and the threshold of \\(\\rho > 0.2\\) is quite arbitrary. Thus, more advanced filter methods, e.g. multivariate filters based on feature importance, usually perform better (Bommert et al. 2020). On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods. In the following, it is described how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.\nFilter algorithms select features by assigning numeric scores to each feature, e.g. correlation between feature and target variables, use these to rank the features and select a feature subset based on the ranking. Features that are assigned lower scores can then be omitted in subsequent modeling steps. All filters are implemented via the package mlr3filters. Below, we cover how to\n\ninstantiate a Filter object,\ncalculate scores for a given task, and\nuse calculated scores to select or drop features.\n\nOne special case of filters are feature importance filters (Section 5.1.2). They select features that are important according to the model induced by a selected Learner. Feature importance filters rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (Chapter 10) values for each feature.\nMany filter methods are implemented in mlr3filters, for example:\n\nCorrelation, calculating Pearson or Spearman correlation between numeric features and numeric targets (FilterCorrelation)\nInformation gain, i.e. mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (FilterInformationGain)\nMinimal joint mutual information maximization, minimizing the joint information between selected features to avoid redundancy (FilterJMIM)\nPermutation score, which calculates permutation feature importance (see Chapter 10) with a given learner for each feature (FilterPermutation)\nArea under the ROC curve calculated for each feature separately (FilterAUC)\n\nMost of the filter methods have some limitations, e.g. the correlation filter can only be calculated for regression tasks with numeric features. For a full list of all implemented filter methods we refer the reader to the mlr3filters website1, which also shows the supported task and features types. A benchmark of filter methods was performed by Bommert et al. (2020), who recommend to not rely on a single filter method but try several ones if the available computational resources allow. If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see Section 5.1.2), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.1 https://mlr3filters.mlr-org.com\n\n5.1.1 Calculating Filter Values\nThe first step is to create a new R object using the class of the desired filter method. Similar to other instances in mlr3, these are registered in a dictionary (mlr_filters) with an associated shortcut function flt(). Each object of class Filter has a $calculate() method which computes the filter values and ranks them in a descending order. For example, we can use the information gain filter described above:\n\nlibrary(\"mlr3verse\")\nfilter = flt(\"information_gain\")\n\nSuch a Filter object can now be used to calculate the filter on the penguins data and get the results:\n\ntask = tsk(\"penguins\")\nfilter$calculate(task)\n\nas.data.table(filter)\n\n          feature       score\n1: flipper_length 0.581167901\n2:    bill_length 0.544896584\n3:     bill_depth 0.538718879\n4:         island 0.520157171\n5:      body_mass 0.442879511\n6:            sex 0.007244168\n7:           year 0.000000000\n\n\nSome filters have hyperparameters, which can be changed similar to setting hyperparameters of a Learner using $param_set$values. For example, to calculate \"spearman\" instead of \"pearson\" correlation with the correlation filter:\n\nfilter_cor = flt(\"correlation\")\nfilter_cor$param_set$values = list(method = \"spearman\")\nfilter_cor$param_set\n\n<ParamSet>\n       id    class lower upper nlevels    default    value\n1:    use ParamFct    NA    NA       5 everything         \n2: method ParamFct    NA    NA       3    pearson spearman\n\n\n\n5.1.2 Feature Importance Filters\nTo use feature importance filters, we can use a learner with integrated feature importance methods. All learners with the property “importance” have this functionality. A list of all learners with this property can be found with\n\nas.data.table(mlr_learners)[sapply(properties, function(x) \"importance\" %in% x)]\n\n                         key                              label task_type\n 1:         classif.catboost                  Gradient Boosting   classif\n 2:      classif.featureless Featureless Classification Learner   classif\n 3:              classif.gbm                  Gradient Boosting   classif\n 4: classif.imbalanced_rfsrc           Imbalanced Random Forest   classif\n 5:         classif.lightgbm                  Gradient Boosting   classif\n---                                                                      \n22:                 surv.gbm                  Gradient Boosting      surv\n23:              surv.mboost Boosted Generalized Additive Model      surv\n24:              surv.ranger                      Random Forest      surv\n25:               surv.rfsrc                      Random Forest      surv\n26:             surv.xgboost                  Gradient Boosting      surv\n4 variables not shown: [feature_types, packages, properties, predict_types]\n\n\nor on the mlr3 website2.2 https://mlr-org.com/learners.html\nFor some learners, the desired filter method needs to be set during learner creation. For example, learner classif.ranger comes with multiple integrated methods, c.f. the help page of ranger::ranger(). To use the feature importance method “impurity”, select it during learner construction:\n\nlrn = lrn(\"classif.ranger\", importance = \"impurity\")\n\nNow we can use the FilterImportance filter class:\n\ntask = tsk(\"penguins\")\n\n# Remove observations with missing data\ntask$filter(which(complete.cases(task$data())))\n\nfilter = flt(\"importance\", learner = lrn)\nfilter$calculate(task)\nas.data.table(filter)\n\n          feature     score\n1:    bill_length 76.374739\n2: flipper_length 45.348924\n3:     bill_depth 36.305939\n4:      body_mass 26.457564\n5:         island 24.077990\n6:            sex  1.597289\n7:           year  1.215536\n\n\n\n5.1.3 Embedded Methods\nMany learners internally select a subset of the features which they find helpful for prediction, but ignore other features. For example, a decision tree might never select some features for splitting. These subsets can be used for feature selection, which we call embedded methods because the feature selection is embedded in the learner. The selected features (and those not selected) can be queried if the learner has the \"selected_features\" property, as the following example demonstrates:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\n\n# ensure that the learner selects features\nstopifnot(\"selected_features\" %in% learner$properties)\n\nlearner = learner$train(task)\nlearner$selected_features()\n\n[1] \"flipper_length\" \"bill_length\"    \"island\"        \n\n\nThe features selected by the model can be extracted by a Filter object, where $calculate() corresponds to training the learner on the given task:\n\nfilter = flt(\"selected_features\", learner = learner)\nfilter$calculate(task)\nas.data.table(filter)\n\n          feature score\n1:         island     1\n2: flipper_length     1\n3:    bill_length     1\n4:     bill_depth     0\n5:            sex     0\n6:           year     0\n7:      body_mass     0\n\n\nContrary to other filter methods, embedded methods just return value of 1 (selected features) and 0 (dropped feature).\n\n5.1.4 Filter-based Feature Selection\nAfter calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modelling steps. For the \"selected_features\" filter described in embedded methods (Section 5.1.3), this step is straight-forward since the methods assigns either a value of 1 for a feature to be kept or 0 for a feature to be dropped. With task$select() the features with a value of 1 can be selected:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\nfilter = flt(\"selected_features\", learner = learner)\nfilter$calculate(task)\n\n# select all features used by rpart\nkeep = names(which(filter$scores == 1))\ntask$select(keep)\ntask$feature_names\n\n[1] \"bill_length\"    \"flipper_length\" \"island\"        \n\n\n\n\n\n\n\n\nTip\n\n\n\nTo select features, we use the function task$select() and not task$filter(), which is used to filter rows (not columns) of the data matrix, see task mutators (Section 2.1.3).\n\n\nFor filter methods which assign continuous scores, there are essentially two ways to select features:\n\nselect the top \\(k\\) features, or\nselect all features with a score above a threshold \\(\\tau\\).\n\nWhere the first option is equivalent to dropping the bottom \\(p-k\\) features. For both options, one has to decide on a threshold, which is often quite arbitrary. For example, to implement the first option with the information gain filter:\n\ntask = tsk(\"penguins\")\nfilter = flt(\"information_gain\")\nfilter$calculate(task)\n\n# select top 3 features from information gain filter\nkeep = names(head(filter$scores, 3))\ntask$select(keep)\ntask$feature_names\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\"\n\n\nOr, the second option with \\(\\tau = 0.5\\):\n\ntask = tsk(\"penguins\")\nfilter = flt(\"information_gain\")\nfilter$calculate(task)\n\n# select all features with score >0.5 from information gain filter\nkeep = names(which(filter$scores > 0.5))\ntask$select(keep)\ntask$feature_names\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\" \"island\"        \n\n\nFilters can be integrated into pipelines. While pipelines are described in detail in Chapter 6, here is a brief preview:\n\nlibrary(mlr3pipelines)\ntask = tsk(\"penguins\")\n\n# combine filter (keep top 3 features) with learner\ngraph = po(\"filter\", filter = flt(\"information_gain\"), filter.nfeat = 3) %>>%\n  po(\"learner\", lrn(\"classif.rpart\"))\n\n# now it can be used as any learner, but it includes the feature selection\nlearner = as_learner(graph)\nlearner$train(task)\n\nPipelines can also be used to apply hyperparameter optimization (Chapter 4) to the filter, i.e. tune the filter threshold to optimize the feature selection regarding prediction performance. We first combine a filter with a learner,\n\ngraph = po(\"filter\", filter = flt(\"information_gain\")) %>>%\n  po(\"learner\", lrn(\"classif.rpart\"))\nlearner = as_learner(graph)\n\nand tune how many feature to include\n\nlibrary(\"mlr3tuning\")\nps = ps(information_gain.filter.nfeat = p_int(lower = 1, upper = 7))\ninstance = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  search_space = ps,\n  terminator = trm(\"none\")\n)\ntuner = tnr(\"grid_search\")\ntuner$optimize(instance)\n\n   information_gain.filter.nfeat learner_param_vals  x_domain classif.acc\n1:                             5          <list[2]> <list[1]>   0.9391304\n\n\nThe output above shows only the best result. To show the results of all tuning steps, retrieve them from the archive of the tuning instance:\n\nas.data.table(instance$archive)\n\n   information_gain.filter.nfeat classif.acc\n1:                             2   0.9304348\n2:                             5   0.9391304\n3:                             1   0.7478261\n4:                             7   0.9391304\n5:                             3   0.9391304\n6:                             6   0.9391304\n7:                             4   0.9391304\n7 variables not shown: [x_domain_information_gain.filter.nfeat, runtime_learners, timestamp, batch_nr, warnings, errors, resample_result]\n\n\nWe can also plot the tuning results:\n\nautoplot(instance)\n\n\n\nFigure 5.1: Model performance with different numbers of features, selected by an information gain filter.\n\n\n\n\nFor more details, see Pipelines (Chapter 6) and Hyperparameter Optimization (Chapter 4)."
  },
  {
    "objectID": "feature-selection.html#sec-fs-wrapper",
    "href": "feature-selection.html#sec-fs-wrapper",
    "title": "5  Feature Selection",
    "section": "\n5.2 Wrapper Methods",
    "text": "5.2 Wrapper Methods\nWrapper methods work by fitting models on selected feature subsets and evaluating their performance. This can be done in a sequential fashion, e.g. by iteratively adding features to the model in the so-called sequential forward selection, or in a parallel fashion, e.g. by evaluating random feature subsets in a random search. Below, the use of these simple approaches is described in a common framework along with more advanced methods such as genetic search. It is further shown how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.\nIn more detail, wrapper methods iteratively select features that optimize a performance measure. Instead of ranking features, a model is fit on a selected subset of features in each iteration and evaluated in resampling with respect to a selected performance measure. The strategy that determines which feature subset is used in each iteration is given by the FSelector object. A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively adds the feature that leads to the largest performance improvement. Wrapper methods can be used with any learner but need to train the learner potentially many times, leading to a computationally intensive method. All wrapper methods are implemented via the package mlr3fselect. In this chapter, we cover how to\n\ninstantiate an FSelector object,\nconfigure it, to e.g. respect a runtime limit or for different objectives,\nrun it or fuse it with a Learner via an AutoFSelector.\n\n\n\n\n\n\n\nNote\n\n\n\nWrapper-based feature selection is very similar to hyperparameter optimization (Chapter 4). The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations. We will see below, that we can even use the same terminators, that some feature selection algorithms are similar to tuners and that we can also optimize multiple performance measures with feature selection.\n\n\n\n5.2.1 Simple Forward Selection Example\nWe start with the simple example from above and do sequential forward selection with the penguins data:\n\nlibrary(\"mlr3fselect\")\n\n# subset features to ease visualization\ntask = tsk(\"penguins\")\ntask$select(c(\"bill_depth\", \"bill_length\", \"body_mass\", \"flipper_length\"))\n\ninstance = fselect(\n  method = \"sequential\",\n  task =  task,\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\")\n)\n\nTo show all analyzed feature subsets and the corresponding performance, we use as.data.table(instance$archive).\n\ndt = as.data.table(instance$archive)\ndt[batch_nr == 1, 1:5]\n\n   bill_depth bill_length body_mass flipper_length classif.acc\n1:       TRUE       FALSE     FALSE          FALSE   0.6956522\n2:      FALSE        TRUE     FALSE          FALSE   0.7652174\n3:      FALSE       FALSE      TRUE          FALSE   0.7043478\n4:      FALSE       FALSE     FALSE           TRUE   0.7913043\n\n\nWe see that the feature flipper_length achieved the highest prediction performance in the first iteration and is thus selected. In the second round, adding bill_length improves performance to over 90%:\n\ndt[batch_nr == 2, 1:5]\n\n   bill_depth bill_length body_mass flipper_length classif.acc\n1:       TRUE       FALSE     FALSE           TRUE   0.7652174\n2:      FALSE        TRUE     FALSE           TRUE   0.9391304\n3:      FALSE       FALSE      TRUE           TRUE   0.8173913\n\n\nHowever, adding a third feature does not improve performance\n\ndt[batch_nr == 3, 1:5]\n\n   bill_depth bill_length body_mass flipper_length classif.acc\n1:       TRUE        TRUE     FALSE           TRUE   0.9391304\n2:      FALSE        TRUE      TRUE           TRUE   0.9391304\n\n\nand the algorithm terminates. To directly show the best feature set, we can use:\n\ninstance$result_feature_set\n\n[1] \"bill_length\"    \"flipper_length\"\n\n\n\n\n\n\n\n\nNote\n\n\n\ninstance$result_feature_set shows features in alphabetical order and not in the order selected.\n\n\nInternally, the fselect function creates an FSelectInstanceSingleCrit object and executes the feature selection with an FSelector object, based on the selected method, in this example an FSelectorSequential object. It uses the supplied resampling and measure to evaluate all feature subsets provided by the FSelector on the task.\nAt the heart of mlr3fselect are the R6 classes:\n\n\nFSelectInstanceSingleCrit, FSelectInstanceMultiCrit: These two classes describe the feature selection problem and store the results.\n\nFSelector: This class is the base class for implementations of feature selection algorithms.\n\nIn the following two sections, these classes will be created manually, to learn more about the mlr3fselect package.\n\n5.2.2 The FSelectInstance Classes\nTo create an FSelectInstanceSingleCrit object, we use the sugar function fsi, which is short for FSelectInstanceSingleCrit$new() or FSelectInstanceMultiCrit$new(), depending on the selected measure(s):\n\ninstance = fsi(\n  task = tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 20)\n)\n\nNote that we have not selected a feature selection algorithm and thus did not select any features, yet. We have also supplied a so-called Terminator, which is used to stop the feature selection. For the forward selection in the example above, we did not need a terminator because we simply tried all remaining features until the full model or no further performance improvement. However, for other feature selection algorithms such as random search, a terminator is required. The following terminator are available:\n\nTerminate after a given time (TerminatorClockTime)\nTerminate after a given amount of iterations (TerminatorEvals)\nTerminate after a specific performance is reached (TerminatorPerfReached)\nTerminate when feature selection does not improve (TerminatorStagnation)\nA combination of the above in an ALL or ANY fashion (TerminatorCombo)\n\nAbove we used the sugar function trm to select TerminatorEvals with 20 evaluations.\nTo start the feature selection, we still need to select an algorithm which are defined via the FSelector class, described in the next section.\n\n5.2.3 The FSelector Class\nThe FSelector class is the base class for different feature selection algorithms. The following algorithms are currently implemented in mlr3fselect:\n\nRandom search, trying random feature subsets until termination (FSelectorRandomSearch)\nExhaustive search, trying all possible feature subsets (FSelectorExhaustiveSearch)\nSequential search, i.e. sequential forward or backward selection (FSelectorSequential)\nRecursive feature elimination, which uses learner’s importance scores to iteratively remove features with low feature importance (FSelectorRFE)\nDesign points, trying all user-supplied feature sets (FSelectorDesignPoints)\nGenetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (FSelectorGeneticSearch)\nShadow variable search, which adds permuted copies of all features (shadow variables) and stops when a shadow variable is selected (FSelectorShadowVariableSearch)\n\nIn this example, we will use a simple random search and retrieve it from the dictionary mlr_fselectors with the fs() sugar function, which is short for FSelectorRandomSearch$new():\n\nfselector = fs(\"random_search\")\n\n\n5.2.4 Starting the Feature Selection\nTo start the feature selection, we pass the FSelectInstanceSingleCrit object to the $optimize() method of the initialized FSelector object:\n\nfselector$optimize(instance)\n\nThe algorithm proceeds as follows\n\nThe FSelector proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting batch_size.\nFor each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.\nAll evaluations are stored in the archive of the FSelectInstanceSingleCrit object.\nThe terminator is queried if the budget is exhausted. If the budget is not exhausted, restart with 1) until it is.\nDetermine the feature subset with the best observed performance.\nStore the best feature subset as the result in the instance object.\n\nThe best feature subset and the corresponding measured performance can be accessed from the instance:\n\nas.data.table(instance$result)[, .(features, classif.acc)]\n\n                                                         features classif.acc\n1: bill_depth,bill_length,body_mass,flipper_length,island,sex,...   0.9391304\n\n\nAs in the forward selection example above, one can investigate all resamplings which were undertaken, as they are stored in the archive of the FSelectInstanceSingleCrit object and can be accessed by using as.data.table():\n\nas.data.table(instance$archive)[, .(bill_depth, bill_length, body_mass, classif.acc)]\n\n    bill_depth bill_length body_mass classif.acc\n 1:       TRUE        TRUE      TRUE   0.9391304\n 2:      FALSE       FALSE      TRUE   0.7391304\n 3:       TRUE        TRUE      TRUE   0.9391304\n 4:       TRUE        TRUE      TRUE   0.9391304\n 5:       TRUE        TRUE      TRUE   0.9391304\n 6:      FALSE       FALSE     FALSE   0.6869565\n 7:       TRUE       FALSE      TRUE   0.8086957\n 8:      FALSE       FALSE      TRUE   0.7391304\n 9:       TRUE       FALSE     FALSE   0.7739130\n10:       TRUE       FALSE     FALSE   0.8000000\n11:      FALSE       FALSE     FALSE   0.8086957\n12:      FALSE       FALSE      TRUE   0.6869565\n13:       TRUE        TRUE      TRUE   0.9391304\n14:      FALSE       FALSE     FALSE   0.6173913\n15:       TRUE        TRUE      TRUE   0.9217391\n16:       TRUE        TRUE      TRUE   0.9391304\n17:       TRUE        TRUE      TRUE   0.9043478\n18:      FALSE       FALSE      TRUE   0.7391304\n19:      FALSE       FALSE     FALSE   0.7739130\n20:      FALSE       FALSE     FALSE   0.8086957\n\n\nNow the optimized feature subset can be used to subset the task and fit the model on all observations:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\n\ntask$select(instance$result_feature_set)\nlearner$train(task)\n\nThe trained model can now be used to make a prediction on external data.\n\n\n\n\n\n\nWarning\n\n\n\nPredicting on observations present in the task used for feature selection should be avoided. The model has seen these observations already during feature selection and therefore performance evaluation results would be over-optimistic. Instead, to get unbiased performance estimates for the current task, nested resampling (see Section 5.2.6 and Section 4.5) is required.\n\n\n\n5.2.5 Optimizing Multiple Performance Measures\nYou might want to use multiple criteria to evaluate the performance of the feature subsets. For example, you might want to select the subset with the highest classification accuracy and lowest time to train the model. However, these two subsets will generally not coincide, i.e. the subset with highest classification accuracy will probably be another subset than that with lowest training time. With mlr3fselect, the result is the pareto-optimal solution, i.e. the best feature subset for each of the criteria that is not dominated by another subset. For the example with classification accuracy and training time, a feature subset that is best in accuracy and training time will dominate all other subsets and thus will be the only pareto-optimal solution. If, however, different subsets are best in the two criteria, both subsets are pareto-optimal.\nWe will expand the previous example and perform feature selection on the penguins dataset, however, this time we will use FSelectInstanceMultiCrit to select the subset of features that has the highest classification accuracy and the one with the lowest time to train the model.\nThe feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:\n\ninstance = fsi(\n  task = tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msrs(c(\"classif.acc\", \"time_train\")),\n  terminator = trm(\"evals\", n_evals = 5)\n)\n\nThe function fsi creates an instance of FSelectInstanceMultiCrit if more than one measure is selected. We now create an FSelector and call the $optimize() function of the FSelector with the FSelectInstanceMultiCrit object, to search for the subset of features with the best classification accuracy and time to train the model. This time, we use design points to manually specify two feature sets to try: one with only the feature sex and one with all features except island, sex and year. We expect the sex-only model to train fast and the model including many features to be accurate.\n\ndesign = mlr3misc::rowwise_table(\n  ~bill_depth, ~bill_length, ~body_mass, ~flipper_length, ~island, ~sex, ~year,\n  FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,\n  TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE\n)\n\nfselector = fs(\"design_points\", design = design)\nfselector$optimize(instance)\n\nAs above, the best feature subset and the corresponding measured performance can be accessed from the instance. However, in this simple case, if the fastest subset is not also the best performing subset, the result consists of two subsets: one with the lowest training time and one with the best classification accuracy:\n\nas.data.table(instance$result)[, .(features, classif.acc, time_train)]\n\n                                          features classif.acc time_train\n1:                                             sex   0.4347826      0.003\n2: bill_depth,bill_length,body_mass,flipper_length   0.9304348      0.004\n\n\nAs explained above, the result is the pareto-optimal solution.\n\n5.2.6 Automating the Feature Selection\nThe AutoFSelector class wraps a learner and augments it with an automatic feature selection for a given task. Because the AutoFSelector itself inherits from the Learner base class, it can be used like any other learner. Below, a new learner is created. This learner is then wrapped in a random search feature selector, which automatically starts a feature selection on the given task using an inner resampling, as soon as the wrapped learner is trained. Here, the function auto_fselector creates an instance of AutoFSelector, i.e. it is short for AutoFSelector$new().\n\nat = auto_fselector(\n  method = fs(\"random_search\"),\n  learner = lrn(\"classif.log_reg\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\nat\n\n<AutoFSelector:classif.log_reg.fselector>\n* Model: list\n* Packages: mlr3, mlr3fselect, mlr3learners, stats\n* Predict Type: response\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: loglik, twoclass\n\n\nWe can now, as with any other learner, call the $train() and $predict() method. This time however, we pass it to benchmark() to compare the optimized feature subset to the complete feature set. This way, the AutoFSelector will do its resampling for feature selection on the training set of the respective split of the outer resampling. The learner then undertakes predictions using the test set of the outer resampling. Here, the outer resampling refers to the resampling specified in benchmark(), whereas the inner resampling is that specified in auto_fselector(). This is called nested resampling (Section 4.5) and yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.\nIn the call to benchmark(), we compare our wrapped learner at with a normal logistic regression lrn(\"classif.log_reg\"). For that, we create a benchmark grid with the task, the learners and a 3-fold cross validation on the sonar data.\n\ngrid = benchmark_grid(\n  task = tsk(\"sonar\"),\n  learner = list(at, lrn(\"classif.log_reg\")),\n  resampling = rsmp(\"cv\", folds = 3)\n)\n\nbmr = benchmark(grid)\n\nNow, we compare those two learners regarding classification accuracy and training time:\n\naggr = bmr$aggregate(msrs(c(\"classif.acc\", \"time_train\")))\nas.data.table(aggr)[, .(learner_id, classif.acc, time_train)]\n\n                  learner_id classif.acc time_train\n1: classif.log_reg.fselector   0.7351967   1.073333\n2:           classif.log_reg   0.6585231   0.034000\n\n\nWe can see that, in this example, the feature selection improves prediction performance but also drastically increases the training time, since the feature selection (including resampling and random search) is part of the model training of the wrapped learner."
  },
  {
    "objectID": "feature-selection.html#conclusion",
    "href": "feature-selection.html#conclusion",
    "title": "5  Feature Selection",
    "section": "\n5.3 Conclusion",
    "text": "5.3 Conclusion\nIn this chapter, we learned how to perform feature selection with mlr3. We introduced filter and wrapper methods, combined feature selection with pipelines, learned how to automate the feature selection and covered the optimization of multiple performance measures. Table 5.1 gives an overview of the most important functions (S3) and classes (R6) used in this chapter.\n\n\nTable 5.1: Core S3 ‘sugar’ functions for feature selection in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.\n\nS3 function\nR6 Class\nSummary\n\n\n\nflt()\nFilter\nSelects features by calculating a score for each feature\n\n\nFilter$calculate()\nFilter\nCalculates scores on a given task\n\n\nfselect()\n\nFSelectInstanceSingleCrit or FSelectInstanceMultiCrit\n\nSpecifies a feature selection problem and stores the results\n\n\nfs()\nFSelector\nSpecifies a feature selection algorithm\n\n\nFSelector$optimize()\nFSelector\nExecutes the features selection specified by the FSelectInstance with the algorithm specified by the FSelector\n\n\n\nauto_fselector()\nAutoFSelector\nDefines a learner that includes feature selection\n\n\n\n\nResources\n\nA list of implemented filters in the mlr3filters package is provided on the mlr3filters website3.\nA summary of wrapper-based feature selection with the mlr3fselect package is provided in the mlr3fselect cheatsheet4.\nAn overview of feature selection methods is provided by Chandrashekar and Sahin (2014).\nA more formal and detailed introduction to filters and wrappers is given in Guyon and Elisseeff (2003).\n\nBommert et al. (2020) perform a benchmark of filter methods.\nFilters can be used as part of a machine learning pipeline (Chapter 6).\nFilters can be optimized with hyperparameter optimization (Chapter 4).\n3 https://mlr3filters.mlr-org.com4 https://cheatsheets.mlr-org.com/mlr3fselect.pdf"
  },
  {
    "objectID": "feature-selection.html#exercises",
    "href": "feature-selection.html#exercises",
    "title": "5  Feature Selection",
    "section": "\n5.4 Exercises",
    "text": "5.4 Exercises\n\nCalculate a correlation filter on the Motor Trend data set (mtcars).\nUse the filter from the first exercise to select the five best features in the mtcars data set.\nApply a backward selection to the penguins data set with a classification tree learner \"classif.rpart\" and holdout resampling by the measure classification accuracy. Compare the results with those in Section 5.2.1. Answer the following questions:\n\nDo the selected features differ?\nWhich feature selection method achieves a higher classification accuracy?\nAre the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?\n\n\nAutomate the feature selection as in Section 5.2.6 with the spam data set and a logistic regression learner (\"classif.log_reg\"). Hint: Remember to call library(\"mlr3learners\") for the logistic regression learner.\n\n\n\n\n\n\n\nBommert, Andrea, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel Lang. 2020. “Benchmark for Filter Methods for Feature Selection in High-Dimensional Classification Data.” Computational Statistics & Data Analysis 143: 106839. https://doi.org/https://doi.org/10.1016/j.csda.2019.106839.\n\n\nChandrashekar, Girish, and Ferat Sahin. 2014. “A Survey on Feature Selection Methods.” Computers and Electrical Engineering 40 (1): 16–28. https://doi.org/https://doi.org/10.1016/j.compeleceng.2013.11.024.\n\n\nGuyon, Isabelle, and André Elisseeff. 2003. “An Introduction to Variable and Feature Selection.” Journal of Machine Learning Research 3 (Mar): 1157–82."
  },
  {
    "objectID": "pipelines.html#pipe-pipeops",
    "href": "pipelines.html#pipe-pipeops",
    "title": "6  Pipelines",
    "section": "\n6.1 The Building Blocks: PipeOps",
    "text": "6.1 The Building Blocks: PipeOps\nThe building blocks of mlr3pipelines are PipeOp-objects (PO). They can be constructed directly using PipeOp<NAME>$new(), but the recommended way is to retrieve them from the mlr_pipeops dictionary:\n\nlibrary(\"mlr3pipelines\")\nas.data.table(mlr_pipeops)\n\n               key                                               label\n 1:         boxcox          Box-Cox Transformation of Numeric Features\n 2:         branch                                      Path Branching\n 3:          chunk                   Chunk Input into Multiple Outputs\n 4: classbalancing                                     Class Balancing\n 5:     classifavg                            Majority Vote Prediction\n---                                                                   \n60:      threshold Change the Threshold of a Classification Prediction\n61:  tunethreshold   Tune the Threshold of a Classification Prediction\n62:       unbranch                            Unbranch Different Paths\n63:         vtreat                     Interface to the vtreat Package\n64:     yeojohnson      Yeo-Johnson Transformation of Numeric Features\n9 variables not shown: [packages, tags, feature_types, input.num, output.num, input.type.train, input.type.predict, output.type.train, output.type.predict]\n\n\nSingle POs can be created using the dictionary:\n\npca = mlr_pipeops$get(\"pca\")\n\nor using syntactic sugar po(<name>):\n\npca = po(\"pca\")\n\nSome POs require additional arguments for construction:\n\nlearner = po(\"learner\")\n\n# Error in as_learner(learner) : argument \"learner\" is missing, with no default argument \"learner\" is missing, with no default\n\n\nlearner = mlr_pipeops$get(\"learner\", lrn(\"classif.rpart\"))\n\nor in short po(\"learner\", lrn(\"classif.rpart\")).\nHyperparameters of POs can be set through the param_vals argument. Here we set the fraction of features for a filter:\n\nfilter = po(\"filter\",\n  filter = mlr3filters::flt(\"variance\"),\n  param_vals = list(filter.frac = 0.5))\n\nor in short notation:\n\npo(\"filter\", mlr3filters::flt(\"variance\"), filter.frac = 0.5)\n\nThe figure below shows an exemplary PipeOp. It takes an input, transforms it during .$train and .$predict and returns data:"
  },
  {
    "objectID": "pipelines.html#pipe-operator",
    "href": "pipelines.html#pipe-operator",
    "title": "6  Pipelines",
    "section": "\n6.2 The Pipeline Operator: %>>%\n",
    "text": "6.2 The Pipeline Operator: %>>%\n\nIt is possible to create intricate Graphs with edges going all over the place (as long as no loops are introduced). Irrespective, there is usually a clear direction of flow between “layers” in the Graph. It is therefore convenient to build up a Graph from layers.\nThis can be done using the %>>% (“double-arrow”) operator. It takes either a PipeOp or a Graph on each of its sides and connects all of the outputs of its left-hand side to one of the inputs each of its right-hand side. The number of inputs therefore must match the number of outputs.\n\nlibrary(\"magrittr\")\n\ngr = po(\"scale\") %>>% po(\"pca\")\ngr$plot(html = FALSE)"
  },
  {
    "objectID": "pipelines.html#pipe-nodes-edges-graphs",
    "href": "pipelines.html#pipe-nodes-edges-graphs",
    "title": "6  Pipelines",
    "section": "\n6.3 Nodes, Edges and Graphs",
    "text": "6.3 Nodes, Edges and Graphs\nPOs are combined into Graphs.\nPOs are identified by their $id. Note that the operations all modify the object in-place and return the object itself. Therefore, multiple modifications can be chained.\nFor this example we use the pca PO defined above and a new PO named “mutate”. The latter creates a new feature from existing variables. Additionally, we use the filter PO again.\n\nmutate = po(\"mutate\")\n\nfilter = po(\"filter\",\n  filter = mlr3filters::flt(\"variance\"),\n  param_vals = list(filter.frac = 0.5))\n\nThe recommended way to construct a graph is to use the %>>% operator to chain POs or Graphs.\n\ngraph = mutate %>>% filter\n\nTo illustrate how this sugar operator works under the surface we will include an example of the manual way (= hard way) to construct a Graph. This is done by creating an empty graph first. Then one fills the empty graph with POs, and connects edges between the POs. Conceptually, this may look like this:\n\n\n\n\n\n\ngraph = Graph$new()$\n  add_pipeop(mutate)$\n  add_pipeop(filter)$\n  add_edge(\"mutate\", \"variance\") # add connection mutate -> filter\n\nThe constructed Graph can be inspected using its $plot() function:\n\ngraph$plot()\n\n\n\n\nChaining multiple POs of the same kind\nIf multiple POs of the same kind should be chained, it is necessary to change the id to avoid name clashes. This can be done by either accessing the $id slot or during construction:\n\ngraph$add_pipeop(po(\"pca\"))\n\n\ngraph$add_pipeop(po(\"pca\", id = \"pca2\"))\n\n\ngraph$plot()"
  },
  {
    "objectID": "pipelines.html#pipe-modeling",
    "href": "pipelines.html#pipe-modeling",
    "title": "6  Pipelines",
    "section": "\n6.4 Modeling",
    "text": "6.4 Modeling\nThe main purpose of a Graph is to build combined preprocessing and model fitting pipelines that can be used as mlr3 Learner.\nConceptually, the process may be summarized as follows:\n\n\n\n\n\nIn the following we chain two preprocessing tasks:\n\nmutate (creation of a new feature)\nfilter (filtering the dataset)\n\nSubsequently one can chain a PO learner to train and predict on the modified dataset.\n\nmutate = po(\"mutate\")\nfilter = po(\"filter\",\n  filter = mlr3filters::flt(\"variance\"),\n  param_vals = list(filter.frac = 0.5))\n\ngraph = mutate %>>%\n  filter %>>%\n  po(\"learner\",\n    learner = lrn(\"classif.rpart\"))\n\nUntil here we defined the main pipeline stored in Graph. Now we can train and predict the pipeline:\n\ntask = tsk(\"iris\")\ngraph$train(task)\n\n$classif.rpart.output\nNULL\n\ngraph$predict(task)\n\n$classif.rpart.output\n<PredictionClassif> for 150 observations:\n    row_ids     truth  response\n          1    setosa    setosa\n          2    setosa    setosa\n          3    setosa    setosa\n---                            \n        148 virginica virginica\n        149 virginica virginica\n        150 virginica virginica\n\n\nRather than calling $train() and $predict() manually, we can put the pipeline Graph into a GraphLearner object. A GraphLearner encapsulates the whole pipeline (including the preprocessing steps) and can be put into resample() or benchmark() . If you are familiar with the old mlr package, this is the equivalent of all the make*Wrapper() functions. The pipeline being encapsulated (here Graph) must always produce a Prediction with its $predict() call, so it will probably contain at least one PipeOpLearner .\n\nglrn = as_learner(graph)\n\nThis learner can be used for model fitting, resampling, benchmarking, and tuning:\n\ncv3 = rsmp(\"cv\", folds = 3)\nresample(task, glrn, cv3)\n\n<ResampleResult> of 3 iterations\n* Task: iris\n* Learner: mutate.variance.classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\n\n6.4.1 Setting Hyperparameters\nIndividual POs offer hyperparameters because they contain $param_set slots that can be read and written from $param_set$values (via the paradox package). The parameters get passed down to the Graph, and finally to the GraphLearner . This makes it not only possible to easily change the behavior of a Graph / GraphLearner and try different settings manually, but also to perform tuning using the mlr3tuning package.\n\nglrn$param_set$values$variance.filter.frac = 0.25\ncv3 = rsmp(\"cv\", folds = 3)\nresample(task, glrn, cv3)\n\n<ResampleResult> of 3 iterations\n* Task: iris\n* Learner: mutate.variance.classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\n\n6.4.2 Tuning\nIf you are unfamiliar with tuning in mlr3, we recommend to take a look at the section about tuning first. Here we define a ParamSet for the “rpart” learner and the “variance” filter which should be optimized during the tuning process.\n\nlibrary(\"paradox\")\nps = ps(\n  classif.rpart.cp = p_dbl(lower = 0, upper = 0.05),\n  variance.filter.frac = p_dbl(lower = 0.25, upper = 1)\n)\n\nAfter having defined the Tuner, a random search with 10 iterations is created. For the inner resampling, we are simply using holdout (single split into train/test) to keep the runtimes reasonable.\n\nlibrary(\"mlr3tuning\")\ninstance = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = glrn,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  search_space = ps,\n  terminator = trm(\"evals\", n_evals = 20)\n)\n\n\ntuner = tnr(\"random_search\")\ntuner$optimize(instance)\n\n   classif.rpart.cp variance.filter.frac learner_param_vals  x_domain\n1:       0.04886918            0.6870548          <list[5]> <list[2]>\n1 variable not shown: [classif.ce]\n\n\nThe tuning result can be found in the respective result slots.\n\ninstance$result_learner_param_vals\n\n$mutate.mutation\nlist()\n\n$mutate.delete_originals\n[1] FALSE\n\n$variance.filter.frac\n[1] 0.6870548\n\n$classif.rpart.xval\n[1] 0\n\n$classif.rpart.cp\n[1] 0.04886918\n\ninstance$result_y\n\nclassif.ce \n      0.02"
  },
  {
    "objectID": "pipelines.html#pipe-nonlinear",
    "href": "pipelines.html#pipe-nonlinear",
    "title": "6  Pipelines",
    "section": "\n6.5 Non-Linear Graphs",
    "text": "6.5 Non-Linear Graphs\nThe Graphs seen so far all have a linear structure. Some POs may have multiple input or output channels. These channels make it possible to create non-linear Graphs with alternative paths taken by the data.\nPossible types are:\n\n\nBranching: Splitting of a node into several paths, e.g. useful when comparing multiple feature-selection methods (pca, filters). Only one path will be executed.\n\nCopying: Splitting of a node into several paths, all paths will be executed (sequentially). Parallel execution is not yet supported.\n\nStacking: Single graphs are stacked onto each other, i.e. the output of one Graph is the input for another. In machine learning this means that the prediction of one Graph is used as input for another Graph\n\n\n\n6.5.1 Branching & Copying\nThe PipeOpBranch and PipeOpUnbranch POs make it possible to specify multiple alternative paths. Only one path is actually executed, the others are ignored. The active path is determined by a hyperparameter. This concept makes it possible to tune alternative preprocessing paths (or learner models).\nBelow a conceptual visualization of branching:\n\n\n\n\n\n\n\n\nPipeOp(Un)Branch is initialized either with the number of branches, or with a character-vector indicating the names of the branches. If names are given, the “branch-choosing” hyperparameter becomes more readable. In the following, we set three options:\n\nDoing nothing (“nop”)\nApplying a PCA\nScaling the data\n\nIt is important to “unbranch” again after “branching”, so that the outputs are merged into one result objects.\nIn the following we first create the branched graph and then show what happens if the “unbranching” is not applied:\n\ngraph = po(\"branch\", c(\"nop\", \"pca\", \"scale\")) %>>%\n  gunion(list(\n    po(\"nop\", id = \"null1\"),\n    po(\"pca\"),\n    po(\"scale\")\n  ))\n\nWithout “unbranching” one creates the following graph:\n\ngraph$plot(html = FALSE)\n\n\n\n\nNow when “unbranching”, we obtain the following results:\n\n(graph %>>% po(\"unbranch\", c(\"nop\", \"pca\", \"scale\")))$plot(html = FALSE)\n\n\n\n\nThe same can be achieved using a shorter notation:\n\n# List of pipeops\nopts = list(po(\"nop\", \"no_op\"), po(\"pca\"), po(\"scale\"))\n# List of po ids\nopt_ids = mlr3misc::map_chr(opts, `[[`, \"id\")\npo(\"branch\", options = opt_ids) %>>%\n  gunion(opts) %>>%\n  po(\"unbranch\", options = opt_ids)\n\nGraph with 5 PipeOps:\n       ID         State        sccssors       prdcssors\n   branch <<UNTRAINED>> no_op,pca,scale                \n    no_op <<UNTRAINED>>        unbranch          branch\n      pca <<UNTRAINED>>        unbranch          branch\n    scale <<UNTRAINED>>        unbranch          branch\n unbranch <<UNTRAINED>>                 no_op,pca,scale\n\n\n\n6.5.2 Model Ensembles\nWe can leverage the different operations presented to connect POs. This allows us to form powerful graphs.\nBefore we go into details, we split the task into train and test indices.\n\ntask = tsk(\"iris\")\ntrain.idx = sample(seq_len(task$nrow), 120)\ntest.idx = setdiff(seq_len(task$nrow), train.idx)\n\n\n6.5.2.1 Bagging\nWe first examine Bagging introduced by (Breiman 1996). The basic idea is to create multiple predictors and then aggregate those to a single, more powerful predictor.\n\n“… multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets” (Breiman 1996)\n\nBagging then aggregates a set of predictors by averaging (regression) or majority vote (classification). The idea behind bagging is, that a set of weak, but different predictors can be combined in order to arrive at a single, better predictor.\nWe can achieve this by downsampling our data before training a learner, repeating this e.g. 10 times and then performing a majority vote on the predictions. Graphically, it may be summarized as follows:\n\n\n\n\n\nFirst, we create a simple pipeline, that uses PipeOpSubsample before a PipeOpLearner is trained:\n\nsingle_pred = po(\"subsample\", frac = 0.7) %>>%\n  po(\"learner\", lrn(\"classif.rpart\"))\n\nWe can now copy this operation 10 times using pipeline_greplicate. The pipeline_greplicate allows us to parallelize many copies of an operation by creating a Graph containing n copies of the input Graph. We can also create it using syntactic sugar via ppl():\n\npred_set = ppl(\"greplicate\", single_pred, 10L)\n\nAfterwards we need to aggregate the 10 pipelines to form a single model:\n\nbagging = pred_set %>>%\n  po(\"classifavg\", innum = 10)\n\nNow we can plot again to see what happens:\n\nbagging$plot(html = FALSE)\n\n\n\n\nThis pipeline can again be used in conjunction with GraphLearner in order for Bagging to be used like a Learner:\n\nbaglrn = as_learner(bagging)\nbaglrn$train(task, train.idx)\nbaglrn$predict(task, test.idx)\n\n<PredictionClassif> for 30 observations:\n    row_ids     truth   response prob.setosa prob.versicolor prob.virginica\n         16    setosa     setosa           1             0.0            0.0\n         18    setosa     setosa           1             0.0            0.0\n         21    setosa     setosa           1             0.0            0.0\n---                                                                        \n        129 virginica  virginica           0             0.0            1.0\n        134 virginica versicolor           0             0.5            0.5\n        149 virginica  virginica           0             0.0            1.0\n\n\nIn conjunction with different Backends, this can be a very powerful tool. In cases when the data does not fully fit in memory, one can obtain a fraction of the data for each learner from a DataBackend and then aggregate predictions over all learners.\n\n6.5.2.2 Stacking\nStacking (Wolpert 1992) is another technique that can improve model performance. The basic idea behind stacking is the use of predictions from one model as features for a subsequent model to possibly improve performance.\nBelow an conceptual illustration of stacking:\n\n\n\n\n\n\n\n\nAs an example we can train a decision tree and use the predictions from this model in conjunction with the original features in order to train an additional model on top.\nTo limit overfitting, we additionally do not predict on the original predictions of the learner. Instead, we predict on out-of-bag predictions. To do all this, we can use PipeOpLearnerCV .\nPipeOpLearnerCV performs nested cross-validation on the training data, fitting a model in each fold. Each of the models is then used to predict on the out-of-fold data. As a result, we obtain predictions for every data point in our input data.\nWe first create a “level 0” learner, which is used to extract a lower level prediction. Additionally, we $clone() the learner object to obtain a copy of the learner. Subsequently, one sets a custom id for the PipeOp .\n\nlrn = lrn(\"classif.rpart\")\nlrn_0 = po(\"learner_cv\", lrn$clone())\nlrn_0$id = \"rpart_cv\"\n\nWe use PipeOpNOP in combination with gunion, in order to send the unchanged Task to the next level. There it is combined with the predictions from our decision tree learner.\n\nlevel_0 = gunion(list(lrn_0, po(\"nop\")))\n\nAfterwards, we want to concatenate the predictions from PipeOpLearnerCV and the original Task using PipeOpFeatureUnion :\n\ncombined = level_0 %>>% po(\"featureunion\", 2)\n\nNow we can train another learner on top of the combined features:\n\nstack = combined %>>% po(\"learner\", lrn$clone())\nstack$plot(html = FALSE)\n\n\n\n\n\nstacklrn = as_learner(stack)\nstacklrn$train(task, train.idx)\nstacklrn$predict(task, test.idx)\n\n<PredictionClassif> for 30 observations:\n    row_ids     truth   response\n         16    setosa     setosa\n         18    setosa     setosa\n         21    setosa     setosa\n---                             \n        129 virginica  virginica\n        134 virginica versicolor\n        149 virginica  virginica\n\n\nIn this vignette, we showed a very simple use-case for stacking. In many real-world applications, stacking is done for multiple levels and on multiple representations of the dataset. On a lower level, different preprocessing methods can be defined in conjunction with several learners. On a higher level, we can then combine those predictions in order to form a very powerful model.\n\n6.5.2.3 Multilevel Stacking\nIn order to showcase the power of mlr3pipelines, we will show a more complicated stacking example.\nIn this case, we train a glmnet and 2 different rpart models (some transform its inputs using PipeOpPCA) on our task in the “level 0” and concatenate them with the original features (via gunion). The result is then passed on to “level 1”, where we copy the concatenated features 3 times and put this task into an rpart and a glmnet model. Additionally, we keep a version of the “level 0” output (via PipeOpNOP) and pass this on to “level 2”. In “level 2” we simply concatenate all “level 1” outputs and train a final decision tree.\nIn the following examples, use <lrn>$param_set$values$<param_name> = <param_value> to set hyperparameters for the different learner.\n\nlibrary(\"magrittr\")\nlibrary(\"mlr3learners\") # for classif.glmnet\n\nrprt = lrn(\"classif.rpart\", predict_type = \"prob\")\nglmn = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\n#  Create Learner CV Operators\nlrn_0 = po(\"learner_cv\", rprt, id = \"rpart_cv_1\")\nlrn_0$param_set$values$maxdepth = 5L\nlrn_1 = po(\"pca\", id = \"pca1\") %>>% po(\"learner_cv\", rprt, id = \"rpart_cv_2\")\nlrn_1$param_set$values$rpart_cv_2.maxdepth = 1L\nlrn_2 = po(\"pca\", id = \"pca2\") %>>% po(\"learner_cv\", glmn)\n\n# Union them with a PipeOpNULL to keep original features\nlevel_0 = gunion(list(lrn_0, lrn_1, lrn_2, po(\"nop\", id = \"NOP1\")))\n\n# Cbind the output 3 times, train 2 learners but also keep level\n# 0 predictions\nlevel_1 = level_0 %>>%\n  po(\"featureunion\", 4) %>>%\n  po(\"copy\", 3) %>>%\n  gunion(list(\n    po(\"learner_cv\", rprt, id = \"rpart_cv_l1\"),\n    po(\"learner_cv\", glmn, id = \"glmnt_cv_l1\"),\n    po(\"nop\", id = \"NOP_l1\")\n  ))\n\n# Cbind predictions, train a final learner\nlevel_2 = level_1 %>>%\n  po(\"featureunion\", 3, id = \"u2\") %>>%\n  po(\"learner\", rprt, id = \"rpart_l2\")\n\n# Plot the resulting graph\nlevel_2$plot(html = FALSE)\n\n\n\ntask = tsk(\"iris\")\nlrn = as_learner(level_2)\n\nAnd we can again call .$train and .$predict:\n\nlrn$\n  train(task, train.idx)$\n  predict(task, test.idx)$\n  score()\n\nclassif.ce \n       0.1"
  },
  {
    "objectID": "pipelines.html#extending-pipeops",
    "href": "pipelines.html#extending-pipeops",
    "title": "6  Pipelines",
    "section": "\n6.6 Adding new PipeOps",
    "text": "6.6 Adding new PipeOps\nThis section showcases how the mlr3pipelines package can be extended to include custom PipeOps. To run the following examples, we will need a Task; we are using the well-known “Iris” task:\n\nlibrary(\"mlr3\")\ntask = tsk(\"iris\")\ntask$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0\n\n\nmlr3pipelines is fundamentally built around R6. When planning to create custom PipeOp objects, it can only help to familiarize yourself with it.\nIn principle, all a PipeOp must do is inherit from the PipeOp R6 class and implement the .train() and .predict() functions. There are, however, several auxiliary subclasses that can make the creation of certain operations much easier.\n\n6.6.1 General Case Example: PipeOpCopy\n\nA very simple yet useful PipeOp is PipeOpCopy, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data. It is a simple example that showcases the important steps in defining a custom PipeOp. We will show a simplified version here, PipeOpCopyTwo, that creates exactly two copies of its input data.\nThe following figure visualizes how our PipeOp is situated in the Pipeline and the significant in- and outputs.\n\n\n\n\n\n\n6.6.1.1 First Steps: Inheriting from PipeOp\n\nThe first part of creating a custom PipeOp is inheriting from PipeOp. We make a mental note that we need to implement a .train() and a .predict() function, and that we probably want to have an initialize() as well:\n\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      ....\n    },\n  ),\n  private == list(\n    .train = function(inputs) {\n      ....\n    },\n\n    .predict = function(inputs) {\n      ....\n    }\n  )\n)\n\nNote, that private methods, e.g. .train and .predict etc are prefixed with a ..\n\n6.6.1.2 Channel Definitions\nWe need to tell the PipeOp the layout of its channels: How many there are, what their names are going to be, and what types are acceptable. This is done on initialization of the PipeOp (using a super$initialize call) by giving the input and output data.table objects. These must have three columns: a \"name\" column giving the names of input and output channels, and a \"train\" and \"predict\" column naming the class of objects we expect during training and prediction as input / output. A special value for these classes is \"*\", which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels.\nBy convention, we name a single channel \"input\" or \"output\", and a group of channels [\"input1\", \"input2\", …], unless there is a reason to give specific different names. Therefore, our input data.table will have a single row <\"input\", \"*\", \"*\">, and our output table will have two rows, <\"output1\", \"*\", \"*\"> and <\"output2\", \"*\", \"*\">.\nAll of this is given to the PipeOp creator. Our initialize() will thus look as follows:\n\ninitialize = function(id = \"copy.two\") {\n  input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\")\n  # the following will create two rows and automatically fill the `train`\n  # and `predict` cols with \"*\"\n  output = data.table::data.table(\n    name = c(\"output1\", \"output2\"),\n    train = \"*\", predict = \"*\"\n  )\n  super$initialize(id,\n    input = input,\n    output = output\n  )\n}\n\n\n6.6.1.3 Train and Predict\nBoth .train() and .predict() will receive a list as input and must give a list in return. According to our input and output definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using c(inputs, inputs).\nTwo things to consider:\n\nThe .train() function must always modify the self$state variable to something that is not NULL or NO_OP. This is because the $state slot is used as a signal that PipeOp has been trained on data, even if the state itself is not important to the PipeOp (as in our case). Therefore, our .train() will set self$state = list().\nIt is not necessary to “clone” our input or make deep copies, because we don’t modify the data. However, if we were changing a reference-passed object, for example by changing data in a Task, we would have to make a deep copy first. This is because a PipeOp may never modify its input object by reference.\n\nOur .train() and .predict() functions are now:\n\n.train = function(inputs) {\n  self$state = list()\n  c(inputs, inputs)\n}\n\n\n.predict = function(inputs) {\n  c(inputs, inputs)\n}\n\n\n6.6.1.4 Putting it Together\nThe whole definition thus becomes\n\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      super$initialize(id,\n        input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\"),\n        output = data.table::data.table(name = c(\"output1\", \"output2\"),\n                            train = \"*\", predict = \"*\")\n      )\n    }\n  ),\n  private = list(\n    .train = function(inputs) {\n      self$state = list()\n      c(inputs, inputs)\n    },\n\n    .predict = function(inputs) {\n      c(inputs, inputs)\n    }\n  )\n)\n\nWe can create an instance of our PipeOp, put it in a graph, and see what happens when we train it on something:\n\nlibrary(\"mlr3pipelines\")\npoct = PipeOpCopyTwo$new()\ngr = Graph$new()\ngr$add_pipeop(poct)\n\nprint(gr)\n\nGraph with 1 PipeOps:\n       ID         State sccssors prdcssors\n copy.two <<UNTRAINED>>                   \n\nresult = gr$train(task)\n\nstr(result)\n\nList of 2\n $ copy.two.output1:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n $ copy.two.output2:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n\n\n\n6.6.2 Special Case: Preprocessing\nMany PipeOps perform an operation on exactly one Task, and return exactly one Task. They may even not care about the “Target” / “Outcome” variable of that task, and only do some modification of some input data. However, it is usually important to them that the Task on which they perform prediction has the same data columns as the Task on which they train. For these cases, the auxiliary base class PipeOpTaskPreproc exists. It inherits from PipeOp itself, and other PipeOps should use it if they fall in the kind of use-case named above.\nWhen inheriting from PipeOpTaskPreproc, one must either implement the private methods .train_task() and .predict_task(), or the methods .train_dt(), .predict_dt(), depending on whether wants to operate on a Task object or on its data as data.tables. In the second case, one can optionally also overload the .select_cols() method, which chooses which of the incoming Task’s features are given to the .train_dt() / .predict_dt() functions.\nThe following will show two examples: PipeOpDropNA, which removes a Task’s rows with missing values during training (and implements .train_task() and .predict_task()), and PipeOpScale, which scales a Task’s numeric columns (and implements .train_dt(), .predict_dt(), and .select_cols()).\n\n6.6.2.1 Example: PipeOpDropNA\n\nDropping rows with missing values may be important when training a model that can not handle them.\nBecause mlr3 \"Task\", text = \"Tasks\") only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values. Instead, the rows can be removed using the Task’s $filter method, which modifies the Task in-place. This is done in the private method .train_task(). We take care that we also set the $state slot to signal that the PipeOp was trained.\nThe private method .predict_task() does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows. Furthermore, mlr3 expects a Learner to always return just as many predictions as it was given input rows, so a PipeOp that removes Task rows during training can not be used inside a GraphLearner.\nWhen we inherit from PipeOpTaskPreproc, it sets the input and output data.tables for us to only accept a single Task. The only thing we do during initialize() is therefore to set an id (which can optionally be changed by the user).\nThe complete PipeOpDropNA can therefore be written as follows. Note that it inherits from PipeOpTaskPreproc, unlike the PipeOpCopyTwo example from above:\n\nPipeOpDropNA = R6::R6Class(\"PipeOpDropNA\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"drop.na\") {\n      super$initialize(id)\n    }\n  ),\n\n  private = list(\n    .train_task = function(task) {\n      self$state = list()\n      featuredata = task$data(cols = task$feature_names)\n      exclude = apply(is.na(featuredata), 1, any)\n      task$filter(task$row_ids[!exclude])\n    },\n\n    .predict_task = function(task) {\n      # nothing to be done\n      task\n    }\n  )\n)\n\nTo test this PipeOp, we create a small task with missing values:\n\nsmalliris = iris[(1:5) * 30, ]\nsmalliris[1, 1] = NA\nsmalliris[2, 2] = NA\nsitask = as_task_classif(smalliris, target = \"Species\")\nprint(sitask$data())\n\n      Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:     setosa          1.6         0.2           NA         3.2\n2: versicolor          3.9         1.4          5.2          NA\n3: versicolor          4.0         1.3          5.5         2.5\n4:  virginica          5.0         1.5          6.0         2.2\n5:  virginica          5.1         1.8          5.9         3.0\n\n\nWe test this by feeding it to a new Graph that uses PipeOpDropNA.\n\ngr = Graph$new()\ngr$add_pipeop(PipeOpDropNA$new())\n\nfiltered_task = gr$train(sitask)[[1]]\nprint(filtered_task$data())\n\n      Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1: versicolor          4.0         1.3          5.5         2.5\n2:  virginica          5.0         1.5          6.0         2.2\n3:  virginica          5.1         1.8          5.9         3.0\n\n\n\n6.6.2.2 Example: PipeOpScaleAlways\n\nAn often-applied preprocessing step is to simply center and/or scale the data to mean \\(0\\) and standard deviation \\(1\\). This fits the PipeOpTaskPreproc pattern quite well. Because it always replaces all columns that it operates on, and does not require any information about the task’s target, it only needs to overload the .train_dt() and .predict_dt() functions. This saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification.\nBecause scaling only makes sense on numeric features, we want to instruct PipeOpTaskPreproc to give us only these numeric columns. We do this by overloading the .select_cols() function: It is called by the class to determine which columns to pass to .train_dt() and .predict_dt(). Its input is the Task that is being transformed, and it should return a character vector of all features to work with. When it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns. Because the levels() of the data table given to .train_dt() and .predict_dt() may be different from the Task’s levels, these functions must also take a levels argument that is a named list of column names indicating their levels. When working with numeric data, this argument can be ignored, but it should be used instead of levels(dt[[column]]) for factorial or character columns.\nThis is the first PipeOp where we will be using the $state slot for something useful: We save the centering offset and scaling coefficient and use it in $.predict()!\nFor simplicity, we are not using hyperparameters and will always scale and center all data. Compare this PipeOpScaleAlways operator to the one defined inside the mlr3pipelines package, PipeOpScale.\n\nPipeOpScaleAlways = R6::R6Class(\"PipeOpScaleAlways\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"scale.always\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .train_dt = function(dt, levels, target) {\n      sc = scale(as.matrix(dt))\n      self$state = list(\n        center = attr(sc, \"scaled:center\"),\n        scale = attr(sc, \"scaled:scale\")\n      )\n      sc\n    },\n\n    .predict_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\n\n(Note for the observant: If you check PipeOpScale.R from the mlr3pipelines package, you will notice that is uses “get(\"type\")” and “get(\"id\")” instead of “type” and “id”, because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a “problem” with data.table and not exclusive to mlr3pipelines.)\nWe can, again, create a new Graph that uses this PipeOp to test it. Compare the resulting data to the original “iris” Task data printed at the beginning:\n\ngr = Graph$new()\ngr$add_pipeop(PipeOpScaleAlways$new())\n\nresult = gr$train(task)\n\nresult[[1]]$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n\n\n\n6.6.3 Special Case: Preprocessing with Simple Train\nIt is possible to make even further simplifications for many PipeOps that perform mostly the same operation during training and prediction. The point of Task preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that may depend on training data).\nConsider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level. However, what features get removed must be decided during training, and may only depend on training data. Furthermore, the actual process of removing features is the same during training and prediction.\nA simplification to make is therefore to have a private method .get_state(task) which sets the $state slot during training, and a private method .transform(task), which gets called both during training and prediction. This is done in the PipeOpTaskPreprocSimple class. Just like PipeOpTaskPreproc, one can inherit from this and overload these functions to get a PipeOp that performs preprocessing with very little boilerplate code.\nJust like PipeOpTaskPreproc, PipeOpTaskPreprocSimple offers the possibility to instead overload the .get_state_dt(dt, levels) and .transform_dt(dt, levels) methods (and optionally, again, the .select_cols(task) function) to operate on data.table feature data instead of the whole Task.\nEven some methods that do not use PipeOpTaskPreprocSimple could work in a similar way: The PipeOpScaleAlways example from above will be shown to also work with this paradigm.\n\n6.6.3.1 Example: PipeOpDropConst\n\nA typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training. One simple example of this is dropping constant features. Because the mlr3 Task class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its $select() function, so the .get_state_dt(dt, levels) / .transform_dt(dt, levels) functions will not get used; instead we overload the .get_state(task) and .transform(task) methods.\nThe .get_state() function’s result is saved to the $state slot, so we want to return something that is useful for dropping features. We choose to save the names of all the columns that have nonzero variance. For brevity, we use length(unique(column)) > 1 to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other.\nThe .transform() method is evaluated both during training and prediction, and can rely on the $state slot being present. All it does here is call the Task$select function with the columns we chose to keep.\nThe full PipeOp could be written as follows:\n\nPipeOpDropConst = R6::R6Class(\"PipeOpDropConst\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"drop.const\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .get_state = function(task) {\n      data = task$data(cols = task$feature_names)\n      nonconst = sapply(data, function(column) length(unique(column)) > 1)\n      list(cnames = colnames(data)[nonconst])\n    },\n\n    .transform = function(task) {\n      task$select(self$state$cnames)\n    }\n  )\n)\n\nThis can be tested using the first five rows of the “Iris” Task, for which one feature (\"Petal.Width\") is constant:\n\nirishead = task$clone()$filter(1:5)\nirishead$data()\n\n   Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:  setosa          1.4         0.2          5.1         3.5\n2:  setosa          1.4         0.2          4.9         3.0\n3:  setosa          1.3         0.2          4.7         3.2\n4:  setosa          1.5         0.2          4.6         3.1\n5:  setosa          1.4         0.2          5.0         3.6\n\n\n\ngr = Graph$new()$add_pipeop(PipeOpDropConst$new())\ndropped_task = gr$train(irishead)[[1]]\n\ndropped_task$data()\n\n   Species Petal.Length Sepal.Length Sepal.Width\n1:  setosa          1.4          5.1         3.5\n2:  setosa          1.4          4.9         3.0\n3:  setosa          1.3          4.7         3.2\n4:  setosa          1.5          4.6         3.1\n5:  setosa          1.4          5.0         3.6\n\n\nWe can also see that the $state was correctly set. Calling $.predict() with this graph, even with different data (the whole Iris Task!) will still drop the \"Petal.Width\" column, as it should.\n\ngr$pipeops$drop.const$state\n\n$cnames\n[1] \"Petal.Length\" \"Sepal.Length\" \"Sepal.Width\" \n\n$affected_cols\n[1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Length\" \"Sepal.Width\" \n\n$intasklayout\n             id    type\n1: Petal.Length numeric\n2:  Petal.Width numeric\n3: Sepal.Length numeric\n4:  Sepal.Width numeric\n\n$outtasklayout\n             id    type\n1: Petal.Length numeric\n2: Sepal.Length numeric\n3:  Sepal.Width numeric\n\n$outtaskshell\nEmpty data.table (0 rows and 4 cols): Species,Petal.Length,Sepal.Length,Sepal.Width\n\n\n\ndropped_predict = gr$predict(task)[[1]]\n\ndropped_predict$data()\n\n       Species Petal.Length Sepal.Length Sepal.Width\n  1:    setosa          1.4          5.1         3.5\n  2:    setosa          1.4          4.9         3.0\n  3:    setosa          1.3          4.7         3.2\n  4:    setosa          1.5          4.6         3.1\n  5:    setosa          1.4          5.0         3.6\n ---                                                \n146: virginica          5.2          6.7         3.0\n147: virginica          5.0          6.3         2.5\n148: virginica          5.2          6.5         3.0\n149: virginica          5.4          6.2         3.4\n150: virginica          5.1          5.9         3.0\n\n\n\n6.6.3.2 Example: PipeOpScaleAlwaysSimple\n\nThis example will show how a PipeOpTaskPreprocSimple can be used when only working on feature data in form of a data.table. Instead of calling the scale() function, the center and scale values are calculated directly and saved to the $state slot. The .transform_dt() function will then perform the same operation during both training and prediction: subtract the center and divide by the scale value. As in the PipeOpScaleAlways example above, we use .select_cols() so that we only work on numeric columns.\n\nPipeOpScaleAlwaysSimple = R6::R6Class(\"PipeOpScaleAlwaysSimple\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"scale.always.simple\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .get_state_dt = function(dt, levels, target) {\n      list(\n        center = sapply(dt, mean),\n        scale = sapply(dt, sd)\n      )\n    },\n\n    .transform_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\n\nWe can compare this PipeOp to the one above to show that it behaves the same.\n\ngr = Graph$new()$add_pipeop(PipeOpScaleAlways$new())\nresult_posa = gr$train(task)[[1]]\n\ngr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new())\nresult_posa_simple = gr$train(task)[[1]]\n\n\nresult_posa$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n\n\n\nresult_posa_simple$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n\n\n\n6.6.4 Hyperparameters\nmlr3pipelines uses the [paradox](https://paradox.mlr-org.com) package to define parameter spaces for PipeOps. Parameters for PipeOps can modify their behavior in certain ways, e.g. switch centering or scaling off in the PipeOpScale operator. The unified interface makes it possible to have parameters for whole Graphs that modify the individual PipeOp’s behavior. The Graphs, when encapsulated in GraphLearners, can even be tuned using the tuning functionality in mlr3tuning.\nHyperparameters are declared during initialization, when calling the PipeOp’s $initialize() function, by giving a param_set argument. The param_set must be a ParamSet from the paradox package; see the tuning chapter or Section 9.4 for more information on how to define parameter spaces. After construction, the ParamSet can be accessed through the $param_set slot. While it is possible to modify this ParamSet, using e.g. the $add() and $add_dep() functions, after adding it to the PipeOp, it is strongly advised against.\nHyperparameters can be set and queried through the $values slot. When setting hyperparameters, they are automatically checked to satisfy all conditions set by the $param_set, so it is not necessary to type check them. Be aware that it is always possible to remove hyperparameter values.\nWhen a PipeOp is initialized, it usually does not have any parameter values—$values takes the value list(). It is possible to set initial parameter values in the $initialize() constructor; this must be done after the super$initialize() call where the corresponding ParamSet must be supplied. This is because setting $values checks against the current $param_set, which would fail if the $param_set was not set yet.\nWhen using an underlying library function (the scale function in PipeOpScale, say), then there is usually a “default” behaviour of that function when a parameter is not given. It is good practice to use this default behaviour whenever a parameter is not set (or when it was removed). This can easily be done when using the mlr3misc library’s mlr3misc::invoke() function, which has functionality similar to \"do.call()\".\n\n6.6.4.1 Hyperparameter Example: PipeOpScale\n\nHow to use hyperparameters can best be shown through the example of PipeOpScale, which is very similar to the example above, PipeOpScaleAlways. The difference is made by the presence of hyperparameters. PipeOpScale constructs a ParamSet in its $initialize function and passes this on to the super$initialize function:\n\nPipeOpScale$public_methods$initialize\n\nfunction (id = \"scale\", param_vals = list()) \n.__PipeOpScale__initialize(self = self, private = private, super = super, \n    id = id, param_vals = param_vals)\n<environment: namespace:mlr3pipelines>\n\n\nThe user has access to this and can set and get parameters. Types are automatically checked:\n\npss = po(\"scale\")\nprint(pss$param_set)\n\n<ParamSet:scale>\n               id    class lower upper nlevels        default value\n1:         center ParamLgl    NA    NA       2           TRUE      \n2:          scale ParamLgl    NA    NA       2           TRUE      \n3:         robust ParamLgl    NA    NA       2 <NoDefault[3]> FALSE\n4: affect_columns ParamUty    NA    NA     Inf  <Selector[1]>      \n\n\n\npss$param_set$values$center = FALSE\nprint(pss$param_set$values)\n\n$robust\n[1] FALSE\n\n$center\n[1] FALSE\n\n\n\npss$param_set$values$scale = \"TRUE\" # bad input is checked!\n\nError in self$assert(xs): Assertion on 'xs' failed: scale: Must be of type 'logical flag', not 'character'.\n\n\nHow PipeOpScale handles its parameters can be seen in its $.train_dt method: It gets the relevant parameters from its $values slot and uses them in the mlr3misc::invoke() call. This has the advantage over calling scale() directly that if a parameter is not given, its default value from the \"scale()\" function will be used.\n\nPipeOpScale$private_methods$.train_dt\n\nfunction (dt, levels, target) \n.__PipeOpScale__.train_dt(self = self, private = private, super = super, \n    dt = dt, levels = levels, target = target)\n<environment: namespace:mlr3pipelines>\n\n\nAnother change that is necessary compared to PipeOpScaleAlways is that the attributes \"scaled:scale\" and \"scaled:center\" are not always present, depending on parameters, and possibly need to be set to default values \\(1\\) or \\(0\\), respectively.\nIt is now even possible (if a bit pointless) to call PipeOpScale with both scale and center set to FALSE, which returns the original dataset, unchanged.\n\npss$param_set$values$scale = FALSE\npss$param_set$values$center = FALSE\n\ngr = Graph$new()\ngr$add_pipeop(pss)\n\nresult = gr$train(task)\n\nresult[[1]]$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0"
  },
  {
    "objectID": "pipelines.html#pipe-special-ops",
    "href": "pipelines.html#pipe-special-ops",
    "title": "6  Pipelines",
    "section": "\n6.7 Special Operators",
    "text": "6.7 Special Operators\nThis section introduces some special operators, that might be useful in numerous further applications.\n\n6.7.1 Imputation: PipeOpImpute\n\nOften you will be using data sets that have missing values. There are many methods of dealing with this issue, from relatively simple imputation using either mean, median or histograms to way more involved methods including using machine learning algorithms in order to predict missing values. These methods are called imputation.\nThe following PipeOps, PipeOpImpute:\n\nAdd an indicator column marking whether a value for a given feature was missing or not (numeric only)\nImpute numeric values from a histogram\nImpute categorical values using a learner\nWe use po(\"featureunion\") and po(\"nop\") to cbind the missing indicator features. In other words to combine the indicator columns with the rest of the data.\n\n\n# Imputation example\ntask = tsk(\"penguins\")\ntask$missings()\n\n       species     bill_depth    bill_length      body_mass flipper_length \n             0              2              2              2              2 \n        island            sex           year \n             0             11              0 \n\n# Add missing indicator columns (\"dummy columns\") to the Task\npom = po(\"missind\")\n# Simply pushes the input forward\nnop = po(\"nop\")\n# Imputes numerical features by histogram.\npon = po(\"imputehist\", id = \"imputer_num\")\n# combines features (used here to add indicator columns to original data)\npou = po(\"featureunion\")\n# Impute categorical features by fitting a Learner (\"classif.rpart\") for each feature.\npof = po(\"imputelearner\", lrn(\"classif.rpart\"), id = \"imputer_fct\", affect_columns = selector_type(\"factor\"))\n\nNow we construct the graph.\n\nimpgraph = list(\n  pom,\n  nop\n) %>>% pou %>>% pof %>>% pon\n\nimpgraph$plot()\n\n\n\n\nNow we get the new task and we can see that all of the missing values have been imputed.\n\nnew_task = impgraph$train(task)[[1]]\n\nnew_task$missings()\n\n               species     missing_bill_depth    missing_bill_length \n                     0                      0                      0 \n     missing_body_mass missing_flipper_length                 island \n                     0                      0                      0 \n                  year                    sex             bill_depth \n                     0                      0                      0 \n           bill_length              body_mass         flipper_length \n                     0                      0                      0 \n\n\nA learner can thus be equipped with automatic imputation of missing values by adding an imputation Pipeop.\n\npolrn = po(\"learner\", lrn(\"classif.rpart\"))\nlrn = as_learner(impgraph %>>% polrn)\n\n\n6.7.2 Feature Engineering: PipeOpMutate\n\nNew features can be added or computed from a task using PipeOpMutate . The operator evaluates one or multiple expressions provided in an alist. In this example, we compute some new features on top of the iris task. Then we add them to the data as illustrated below:\niris dataset looks like this:\n\ntask = task = tsk(\"iris\")\nhead(as.data.table(task))\n\n   Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:  setosa          1.4         0.2          5.1         3.5\n2:  setosa          1.4         0.2          4.9         3.0\n3:  setosa          1.3         0.2          4.7         3.2\n4:  setosa          1.5         0.2          4.6         3.1\n5:  setosa          1.4         0.2          5.0         3.6\n6:  setosa          1.7         0.4          5.4         3.9\n\n\nOnce we do the mutations, you can see the new columns:\n\npom = po(\"mutate\")\n\n# Define a set of mutations\nmutations = list(\n  Sepal.Sum = ~ Sepal.Length + Sepal.Width,\n  Petal.Sum = ~ Petal.Length + Petal.Width,\n  Sepal.Petal.Ratio = ~ (Sepal.Length / Petal.Length)\n)\npom$param_set$values$mutation = mutations\n\nnew_task = pom$train(list(task))[[1]]\nhead(as.data.table(new_task))\n\n   Species Petal.Length Petal.Width Sepal.Length Sepal.Width Sepal.Sum\n1:  setosa          1.4         0.2          5.1         3.5       8.6\n2:  setosa          1.4         0.2          4.9         3.0       7.9\n3:  setosa          1.3         0.2          4.7         3.2       7.9\n4:  setosa          1.5         0.2          4.6         3.1       7.7\n5:  setosa          1.4         0.2          5.0         3.6       8.6\n6:  setosa          1.7         0.4          5.4         3.9       9.3\n2 variables not shown: [Petal.Sum, Sepal.Petal.Ratio]\n\n\nIf outside data is required, we can make use of the env parameter. Moreover, we provide an environment, where expressions are evaluated (env defaults to .GlobalEnv).\n\n6.7.3 Training on data subsets: PipeOpChunk\n\nIn cases, where data is too big to fit into the machine’s memory, an often-used technique is to split the data into several parts. Subsequently, the parts are trained on each part of the data.\nAfter undertaking these steps, we aggregate the models. In this example, we split our data into 4 parts using PipeOpChunk . Additionally, we create 4 PipeOpLearner POS, which are then trained on each split of the data.\n\nchks = po(\"chunk\", 4)\nlrns = ppl(\"greplicate\", po(\"learner\", lrn(\"classif.rpart\")), 4)\n\nAfterwards we can use PipeOpClassifAvg to aggregate the predictions from the 4 different models into a new one.\n\nmjv = po(\"classifavg\", 4)\n\nWe can now connect the different operators and visualize the full graph:\n\npipeline = chks %>>% lrns %>>% mjv\npipeline$plot(html = FALSE)\n\n\n\n\n\ntask = tsk(\"iris\")\ntrain.idx = sample(seq_len(task$nrow), 120)\ntest.idx = setdiff(seq_len(task$nrow), train.idx)\n\npipelrn = as_learner(pipeline)\npipelrn$train(task, train.idx)$\n  predict(task, train.idx)$\n  score()\n\nclassif.ce \n 0.2083333 \n\n\n\n6.7.4 Feature Selection: PipeOpFilter and PipeOpSelect\n\nThe package mlr3filters contains many different \"mlr3filters::Filter\")s that can be used to select features for subsequent learners. This is often required when the data has a large amount of features.\nA PipeOp for filters is PipeOpFilter:\n\npo(\"filter\", mlr3filters::flt(\"information_gain\"))\n\nPipeOp: <information_gain> (not trained)\nvalues: <list()>\nInput channels <name [train type, predict type]>:\n  input [Task,Task]\nOutput channels <name [train type, predict type]>:\n  output [Task,Task]\n\n\nHow many features to keep can be set using filter_nfeat, filter_frac and filter_cutoff.\nFilters can be selected / de-selected by name using PipeOpSelect."
  },
  {
    "objectID": "pipelines.html#in-depth-pipelines",
    "href": "pipelines.html#in-depth-pipelines",
    "title": "6  Pipelines",
    "section": "\n6.8 In-depth look into mlr3pipelines",
    "text": "6.8 In-depth look into mlr3pipelines\nThis vignette is an in-depth introduction to mlr3pipelines, the dataflow programming toolkit for machine learning in R using mlr3. It will go through basic concepts and then give a few examples that both show the simplicity as well as the power and versatility of using mlr3pipelines.\n\n6.8.1 What’s the Point\nMachine learning toolkits often try to abstract away the processes happening inside machine learning algorithms. This makes it easy for the user to switch out one algorithm for another without having to worry about what is happening inside it, what kind of data it is able to operate on etc. The benefit of using mlr3, for example, is that one can create a Learner, a Task, a Resampling etc. and use them for typical machine learning operations. It is trivial to exchange individual components and therefore use, for example, a different Learner in the same experiment for comparison.\n\ntask = as_task_classif(iris, target = \"Species\")\nlrn = lrn(\"classif.rpart\")\nrsmp = rsmp(\"holdout\")\nresample(task, lrn, rsmp)\n\n<ResampleResult> of 1 iterations\n* Task: iris\n* Learner: classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\nHowever, this modularity breaks down as soon as the learning algorithm encompasses more than just model fitting, like data preprocessing, ensembles or other meta models. mlr3pipelines takes modularity one step further than mlr3: it makes it possible to build individual steps within a Learner out of building blocks called PipeOps.\n\n6.8.2 PipeOp: Pipeline Operators\nThe most basic unit of functionality within mlr3pipelines is the PipeOp, short for “pipeline operator”, which represents a trans-formative operation on input (for example a training dataset) leading to output. It can therefore be seen as a generalized notion of a function, with a certain twist: PipeOps behave differently during a “training phase” and a “prediction phase”. The training phase will typically generate a certain model of the data that is saved as internal state. The prediction phase will then operate on the input data depending on the trained model.\nAn example of this behavior is the principal component analysis operation (“PipeOpPCA”): During training, it will transform incoming data by rotating it in a way that leads to uncorrelated features ordered by their contribution to total variance. It will also save the rotation matrix to be use for new data during the “prediction phase”. This makes it possible to perform “prediction” with single rows of new data, where a row’s scores on each of the principal components (the components of the training data!) is computed.\n\npo = po(\"pca\")\npo$train(list(task))[[1]]$data()\n\n       Species       PC1         PC2         PC3          PC4\n  1:    setosa -2.684126 -0.31939725  0.02791483 -0.002262437\n  2:    setosa -2.714142  0.17700123  0.21046427 -0.099026550\n  3:    setosa -2.888991  0.14494943 -0.01790026 -0.019968390\n  4:    setosa -2.745343  0.31829898 -0.03155937  0.075575817\n  5:    setosa -2.728717 -0.32675451 -0.09007924  0.061258593\n ---                                                         \n146: virginica  1.944110 -0.18753230 -0.17782509 -0.426195940\n147: virginica  1.527167  0.37531698  0.12189817 -0.254367442\n148: virginica  1.764346 -0.07885885 -0.13048163 -0.137001274\n149: virginica  1.900942 -0.11662796 -0.72325156 -0.044595305\n150: virginica  1.390189  0.28266094 -0.36290965  0.155038628\n\n\n\nsingle_line_task = task$clone()$filter(1)\npo$predict(list(single_line_task))[[1]]$data()\n\n   Species       PC1        PC2        PC3          PC4\n1:  setosa -2.684126 -0.3193972 0.02791483 -0.002262437\n\n\n\npo$state\n\nStandard deviations (1, .., p=4):\n[1] 2.0562689 0.4926162 0.2796596 0.1543862\n\nRotation (n x k) = (4 x 4):\n                     PC1         PC2         PC3        PC4\nPetal.Length  0.85667061  0.17337266 -0.07623608  0.4798390\nPetal.Width   0.35828920  0.07548102 -0.54583143 -0.7536574\nSepal.Length  0.36138659 -0.65658877  0.58202985 -0.3154872\nSepal.Width  -0.08452251 -0.73016143 -0.59791083  0.3197231\n\n\nThis shows the most important primitives incorporated in a PipeOp: * $train(), taking a list of input arguments, turning them into a list of outputs, meanwhile saving a state in $state * $predict(), taking a list of input arguments, turning them into a list of outputs, making use of the saved $state * $state, the “model” trained with $train() and utilized during $predict().\nSchematically we can represent the PipeOp like so:\n\n\n\n\n\n\n6.8.2.1 Why the $state\n\nIt is important to take a moment and notice the importance of a $state variable and the $train() / $predict() dichotomy in a PipeOp. There are many preprocessing methods, for example scaling of parameters or imputation, that could in theory just be applied to training data and prediction / validation data separately, or they could be applied to a task before resampling is performed. This would, however, be fallacious:\n\nThe preprocessing of each instance of prediction data should not depend on the remaining prediction dataset. A prediction on a single instance of new data should give the same result as prediction performed on a whole dataset.\nIf preprocessing is performed on a task before resampling is done, information about the test set can leak into the training set. Resampling should evaluate the generalization performance of the entire machine learning method, therefore the behavior of this entire method must only depend only on the content of the training split during resampling.\n\n6.8.2.2 Where to get PipeOps\nEach PipeOp is an instance of an “R6” class, many of which are provided by the mlr3pipelines package itself. They can be constructed explicitly (“PipeOpPCA$new()”) or retrieved from the mlr_pipeops dictionary: po(\"pca\"). The entire list of available PipeOps, and some meta-information, can be retrieved using as.data.table():\n\nas.data.table(mlr_pipeops)[, c(\"key\", \"input.num\", \"output.num\")]\n\n               key input.num output.num\n 1:         boxcox         1          1\n 2:         branch         1         NA\n 3:          chunk         1         NA\n 4: classbalancing         1          1\n 5:     classifavg        NA          1\n---                                    \n60:      threshold         1          1\n61:  tunethreshold         1          1\n62:       unbranch        NA          1\n63:         vtreat         1          1\n64:     yeojohnson         1          1\n\n\nWhen retrieving PipeOps from the mlr_pipeops dictionary, it is also possible to give additional constructor arguments, such as an id or parameter values.\n\npo(\"pca\", rank. = 3)\n\nPipeOp: <pca> (not trained)\nvalues: <rank.=3>\nInput channels <name [train type, predict type]>:\n  input [Task,Task]\nOutput channels <name [train type, predict type]>:\n  output [Task,Task]\n\n\n\n6.8.3 PipeOp Channels\n\n6.8.3.1 Input Channels\nJust like functions, PipeOps can take multiple inputs. These multiple inputs are always given as elements in the input list. For example, there is a PipeOpFeatureUnion that combines multiple tasks with different features and “cbind()s” them together, creating one combined task. When two halves of the iris task are given, for example, it recreates the original task:\n\niris_first_half = task$clone()$select(c(\"Petal.Length\", \"Petal.Width\"))\niris_second_half = task$clone()$select(c(\"Sepal.Length\", \"Sepal.Width\"))\n\npofu = po(\"featureunion\", innum = 2)\n\npofu$train(list(iris_first_half, iris_second_half))[[1]]$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0\n\n\nBecause PipeOpFeatureUnion effectively takes two input arguments here, we can say it has two input channels. An input channel also carries information about the type of input that is acceptable. The input channels of the pofu object constructed above, for example, each accept a Task during training and prediction. This information can be queried from the $input slot:\n\npofu$input\n\n     name train predict\n1: input1  Task    Task\n2: input2  Task    Task\n\n\nOther PipeOps may have channels that take different types during different phases. The backuplearner PipeOp, for example, takes a NULL and a Task during training, and a Prediction and a Task during prediction:\n\n# TODO this is an important case to handle here, do not delete unless there is a better example.\n# po(\"backuplearner\")$input\n\n\n6.8.3.2 Output Channels\nUnlike the typical notion of a function, PipeOps can also have multiple output channels. $train() and $predict() always return a list, so certain PipeOps may return lists with more than one element. Similar to input channels, the information about the number and type of outputs given by a PipeOp is available in the $output slot. The chunk PipeOp, for example, chunks a given Task into subsets and consequently returns multiple Task objects, both during training and prediction. The number of output channels must be given during construction through the outnum argument.\n\npo(\"chunk\", outnum = 3)$output\n\n      name train predict\n1: output1  Task    Task\n2: output2  Task    Task\n3: output3  Task    Task\n\n\nNote that the number of output channels during training and prediction is the same. A schema of a PipeOp with two output channels:\n\n\n\n\n\n\n6.8.3.3 Channel Configuration\nMost PipeOps have only one input channel (so they take a list with a single element), but there are a few with more than one; In many cases, the number of input or output channels is determined during construction, e.g. through the innum / outnum arguments. The input.num and output.num columns of the mlr_pipeops-table above show the default number of channels, and NA if the number depends on a construction argument.\nThe default printer of a PipeOp gives information about channel names and types:\n\n# po(\"backuplearner\")\n\n\n6.8.4 Graph: Networks of PipeOps\n\n6.8.4.1 Basics\nWhat is the advantage of this tedious way of declaring input and output channels and handling in/output through lists? Because each PipeOp has a known number of input and output channels that always produce or accept data of a known type, it is possible to network them together in Graphs. A Graph is a collection of PipeOps with “edges” that mandate that data should be flowing along them. Edges always pass between PipeOp channels, so it is not only possible to explicitly prescribe which position of an input or output list an edge refers to, it makes it possible to make different components of a PipeOp’s output flow to multiple different other PipeOps, as well as to have a PipeOp gather its input from multiple other PipeOps.\nA schema of a simple graph of PipeOps:\n\n\n\n\n\nA Graph is empty when first created, and PipeOps can be added using the $add_pipeop() method. The $add_edge() method is used to create connections between them. While the printer of a Graph gives some information about its layout, the most intuitive way of visualizing it is using the $plot() function.\n\ngr = Graph$new()\ngr$add_pipeop(po(\"scale\"))\ngr$add_pipeop(po(\"subsample\", frac = 0.1))\ngr$add_edge(\"scale\", \"subsample\")\n\n\nprint(gr)\n\nGraph with 2 PipeOps:\n        ID         State  sccssors prdcssors\n     scale <<UNTRAINED>> subsample          \n subsample <<UNTRAINED>>               scale\n\n\n\ngr$plot(html = FALSE)\n\n\n\n\nA Graph itself has a $train() and a $predict() method that accept some data and propagate this data through the network of PipeOps. The return value corresponds to the output of the PipeOp output channels that are not connected to other PipeOps.\n\ngr$train(task)[[1]]$data()\n\n       Species Petal.Length   Petal.Width Sepal.Length Sepal.Width\n 1:     setosa  -1.27910398 -1.3110521482  -1.01843718   0.7861738\n 2:     setosa  -1.22245633 -1.3110521482  -1.25996379   0.7861738\n 3:     setosa  -1.50569459 -1.4422448248  -1.86378030  -0.1315388\n 4:     setosa  -1.16580868 -1.3110521482  -0.53538397   0.7861738\n 5:     setosa  -1.44904694 -1.3110521482  -1.01843718   0.3273175\n 6:     setosa  -1.39239929 -1.3110521482  -1.74301699  -0.1315388\n 7: versicolor   0.42032558  0.3944526477   0.67224905   0.3273175\n 8: versicolor  -0.14615094 -0.2615107354  -1.01843718  -2.4258204\n 9: versicolor  -0.08950329  0.1320672944  -0.29385737  -0.3609670\n10: versicolor   0.42032558  0.3944526477   0.43072244  -1.9669641\n11: versicolor   0.42032558  0.3944526477   0.18919584  -0.3609670\n12: versicolor   0.36367793  0.0008746178  -0.41462067  -1.0492515\n13: versicolor   0.47697323  0.2632599711   0.30995914  -0.1315388\n14: versicolor   0.13708732  0.0008746178  -0.05233076  -1.0492515\n15:  virginica   1.04344975  1.1816087073   0.67224905  -0.5903951\n\n\n\ngr$predict(single_line_task)[[1]]$data()\n\n   Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:  setosa    -1.335752   -1.311052   -0.8976739    1.015602\n\n\nThe collection of PipeOps inside a Graph can be accessed through the $pipeops slot. The set of edges in the Graph can be inspected through the $edges slot. It is possible to modify individual PipeOps and edges in a Graph through these slots, but this is not recommended because no error checking is performed and it may put the Graph in an unsupported state.\n\n6.8.4.2 Networks\nThe example above showed a linear preprocessing pipeline, but it is in fact possible to build true “graphs” of operations, as long as no loops are introduced1. PipeOps with multiple output channels can feed their data to multiple different subsequent PipeOps, and PipeOps with multiple input channels can take results from different PipeOps. When a PipeOp has more than one input / output channel, then the Graph’s $add_edge() method needs an additional argument that indicates which channel to connect to. This argument can be given in the form of an integer, or as the name of the channel.1 It is tempting to denote this as a “directed acyclic graph”, but this would not be entirely correct because edges run between channels of PipeOps, not PipeOps themselves.\nThe following constructs a Graph that copies the input and gives one copy each to a “scale” and a “pca” PipeOp. The resulting columns of each operation are put next to each other by “featureunion”.\n\ngr = Graph$new()$\n  add_pipeop(po(\"copy\", outnum = 2))$\n  add_pipeop(po(\"scale\"))$\n  add_pipeop(po(\"pca\"))$\n  add_pipeop(po(\"featureunion\", innum = 2))\n\ngr$\n  add_edge(\"copy\", \"scale\", src_channel = 1)$        # designating channel by index\n  add_edge(\"copy\", \"pca\", src_channel = \"output2\")$  # designating channel by name\n  add_edge(\"scale\", \"featureunion\", dst_channel = 1)$\n  add_edge(\"pca\", \"featureunion\", dst_channel = 2)\n\ngr$plot(html = FALSE)\n\n\n\n\n\ngr$train(iris_first_half)[[1]]$data()\n\n       Species Petal.Length Petal.Width       PC1          PC2\n  1:    setosa   -1.3357516  -1.3110521 -2.561012 -0.006922191\n  2:    setosa   -1.3357516  -1.3110521 -2.561012 -0.006922191\n  3:    setosa   -1.3923993  -1.3110521 -2.653190  0.031849692\n  4:    setosa   -1.2791040  -1.3110521 -2.468834 -0.045694073\n  5:    setosa   -1.3357516  -1.3110521 -2.561012 -0.006922191\n ---                                                          \n146: virginica    0.8168591   1.4439941  1.755953  0.455479438\n147: virginica    0.7035638   0.9192234  1.416510  0.164312126\n148: virginica    0.8168591   1.0504160  1.639637  0.178946130\n149: virginica    0.9301544   1.4439941  1.940308  0.377935674\n150: virginica    0.7602115   0.7880307  1.469915  0.033362474\n\n\n\n6.8.4.3 Syntactic Sugar\nAlthough it is possible to create intricate Graphs with edges going all over the place (as long as no loops are introduced), there is usually a clear direction of flow between “layers” in the Graph. It is therefore convenient to build up a Graph from layers, which can be done using the %>>% (“double-arrow”) operator. It takes either a PipeOp or a Graph on each of its sides and connects all of the outputs of its left-hand side to one of the inputs each of its right-hand side–the number of inputs therefore must match the number of outputs. Together with the gunion() operation, which takes PipeOps or Graphs and arranges them next to each other akin to a (disjoint) graph union, the above network can more easily be constructed as follows:\n\ngr = po(\"copy\", outnum = 2) %>>%\n  gunion(list(po(\"scale\"), po(\"pca\"))) %>>%\n  po(\"featureunion\", innum = 2)\n\ngr$plot(html = FALSE)\n\n\n\n\n\n6.8.4.4 PipeOp IDs and ID Name Clashes\nPipeOps within a graph are addressed by their $id-slot. It is therefore necessary for all PipeOps within a Graph to have a unique $id. The $id can be set during or after construction, but it should not directly be changed after a PipeOp was inserted in a Graph. At that point, the $set_names()-method can be used to change PipeOp ids.\n\npo1 = po(\"scale\")\npo2 = po(\"scale\")\npo1 %>>% po2 # name clash\n\nError in gunion(list(g1, g2), in_place = c(TRUE, TRUE)): Assertion on 'ids of pipe operators of graphs' failed: Must have unique names, but element 2 is duplicated.\n\n\n\npo2$id = \"scale2\"\ngr = po1 %>>% po2\ngr\n\nGraph with 2 PipeOps:\n     ID         State sccssors prdcssors\n  scale <<UNTRAINED>>   scale2          \n scale2 <<UNTRAINED>>              scale\n\n\n\n# Alternative ways of getting new ids:\npo(\"scale\", id = \"scale2\")\n\nPipeOp: <scale2> (not trained)\nvalues: <robust=FALSE>\nInput channels <name [train type, predict type]>:\n  input [Task,Task]\nOutput channels <name [train type, predict type]>:\n  output [Task,Task]\n\n\n\n# sometimes names of PipeOps within a Graph need to be changed\ngr2 = po(\"scale\") %>>% po(\"pca\")\ngr %>>% gr2\n\nError in gunion(list(g1, g2), in_place = c(TRUE, TRUE)): Assertion on 'ids of pipe operators of graphs' failed: Must have unique names, but element 3 is duplicated.\n\n\n\ngr2$set_names(\"scale\", \"scale3\")\ngr %>>% gr2\n\nGraph with 4 PipeOps:\n     ID         State sccssors prdcssors\n  scale <<UNTRAINED>>   scale2          \n scale2 <<UNTRAINED>>   scale3     scale\n scale3 <<UNTRAINED>>      pca    scale2\n    pca <<UNTRAINED>>             scale3\n\n\n\n6.8.5 Learners in Graphs, Graphs in Learners\nThe true power of mlr3pipelines derives from the fact that it can be integrated seamlessly with mlr3. Two components are mainly responsible for this:\n\n\nPipeOpLearner, a PipeOp that encapsulates a mlr3 Learner and creates a PredictionData object in its $predict() phase\n\nGraphLearner, a mlr3 Learner that can be used in place of any other mlr3 Learner, but which does prediction using a Graph given to it\n\nNote that these are dual to each other: One takes a Learner and produces a PipeOp (and by extension a Graph); the other takes a Graph and produces a Learner.\n\n6.8.5.1 PipeOpLearner\n\nThe PipeOpLearner is constructed using a mlr3 Learner and will use it to create PredictionData in the $predict() phase. The output during $train() is NULL. It can be used after a preprocessing pipeline, and it is even possible to perform operations on the PredictionData, for example by averaging multiple predictions or by using the PipeOpBackupLearner” operator to impute predictions that a given model failed to create.\nThe following is a very simple Graph that performs training and prediction on data after performing principal component analysis.\n\ngr = po(\"pca\") %>>% po(\"learner\",\n  lrn(\"classif.rpart\"))\n\n\ngr$train(task)\n\n$classif.rpart.output\nNULL\n\ngr$predict(task)\n\n$classif.rpart.output\n<PredictionClassif> for 150 observations:\n    row_ids     truth  response\n          1    setosa    setosa\n          2    setosa    setosa\n          3    setosa    setosa\n---                            \n        148 virginica virginica\n        149 virginica virginica\n        150 virginica virginica\n\n\n\n6.8.5.2 GraphLearner\n\nAlthough a Graph has $train() and $predict() functions, it can not be used directly in places where mlr3 Learners can be used like resampling or benchmarks. For this, it needs to be wrapped in a GraphLearner object, which is a thin wrapper that enables this functionality. The resulting Learner is extremely versatile, because every part of it can be modified, replaced, parameterized and optimized over. Resampling the graph above can be done the same way that resampling of the Learner was performed in the introductory example.\n\nlrngrph = as_learner(gr)\nresample(task, lrngrph, rsmp)\n\n<ResampleResult> of 1 iterations\n* Task: iris\n* Learner: pca.classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\n\n6.8.6 Hyperparameters\nmlr3pipelines relies on the paradox package to provide parameters that can modify each PipeOp’s behavior. paradox parameters provide information about the parameters that can be changed, as well as their types and ranges. They provide a unified interface for benchmarks and parameter optimization (“tuning”). For a deep dive into paradox, see the tuning chapter or Section 9.4.\nThe ParamSet, representing the space of possible parameter configurations of a PipeOp, can be inspected by accessing the $param_set slot of a PipeOp or a Graph.\n\nop_pca = po(\"pca\")\nop_pca$param_set\n\n<ParamSet:pca>\n               id    class lower upper nlevels       default value\n1:         center ParamLgl    NA    NA       2          TRUE      \n2:         scale. ParamLgl    NA    NA       2         FALSE      \n3:          rank. ParamInt     1   Inf     Inf                    \n4: affect_columns ParamUty    NA    NA     Inf <Selector[1]>      \n\n\nTo set or retrieve a parameter, the $param_set$values slot can be accessed. Alternatively, the param_vals value can be given during construction.\n\nop_pca$param_set$values$center = FALSE\nop_pca$param_set$values\n\n$center\n[1] FALSE\n\n\n\nop_pca = po(\"pca\", center = TRUE)\nop_pca$param_set$values\n\n$center\n[1] TRUE\n\n\nEach PipeOp can bring its own individual parameters which are collected together in the Graph’s $param_set. A PipeOp’s parameter names are prefixed with its $id to prevent parameter name clashes.\n\ngr = op_pca %>>% po(\"scale\")\ngr$param_set\n\n<ParamSetCollection>\n                     id    class lower upper nlevels        default value\n1:           pca.center ParamLgl    NA    NA       2           TRUE  TRUE\n2:           pca.scale. ParamLgl    NA    NA       2          FALSE      \n3:            pca.rank. ParamInt     1   Inf     Inf                     \n4:   pca.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>      \n5:         scale.center ParamLgl    NA    NA       2           TRUE      \n6:          scale.scale ParamLgl    NA    NA       2           TRUE      \n7:         scale.robust ParamLgl    NA    NA       2 <NoDefault[3]> FALSE\n8: scale.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>      \n\n\n\ngr$param_set$values\n\n$pca.center\n[1] TRUE\n\n$scale.robust\n[1] FALSE\n\n\nBoth PipeOpLearner and GraphLearner preserve parameters of the objects they encapsulate.\n\nop_rpart = po(\"learner\", lrn(\"classif.rpart\"))\nop_rpart$param_set\n\n<ParamSet:classif.rpart>\n                id    class lower upper nlevels        default value\n 1:             cp ParamDbl     0     1     Inf           0.01      \n 2:     keep_model ParamLgl    NA    NA       2          FALSE      \n 3:     maxcompete ParamInt     0   Inf     Inf              4      \n 4:       maxdepth ParamInt     1    30      30             30      \n 5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n 6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n 7:       minsplit ParamInt     1   Inf     Inf             20      \n 8: surrogatestyle ParamInt     0     1       2              0      \n 9:   usesurrogate ParamInt     0     2       3              2      \n10:           xval ParamInt     0   Inf     Inf             10     0\n\n\n\nglrn = as_learner(gr %>>% op_rpart)\nglrn$param_set\n\n<ParamSetCollection>\n                              id    class lower upper nlevels        default\n 1:                   pca.center ParamLgl    NA    NA       2           TRUE\n 2:                   pca.scale. ParamLgl    NA    NA       2          FALSE\n 3:                    pca.rank. ParamInt     1   Inf     Inf               \n 4:           pca.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>\n 5:                 scale.center ParamLgl    NA    NA       2           TRUE\n 6:                  scale.scale ParamLgl    NA    NA       2           TRUE\n 7:                 scale.robust ParamLgl    NA    NA       2 <NoDefault[3]>\n 8:         scale.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>\n 9:             classif.rpart.cp ParamDbl     0     1     Inf           0.01\n10:     classif.rpart.keep_model ParamLgl    NA    NA       2          FALSE\n11:     classif.rpart.maxcompete ParamInt     0   Inf     Inf              4\n12:       classif.rpart.maxdepth ParamInt     1    30      30             30\n13:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf              5\n14:      classif.rpart.minbucket ParamInt     1   Inf     Inf <NoDefault[3]>\n15:       classif.rpart.minsplit ParamInt     1   Inf     Inf             20\n16: classif.rpart.surrogatestyle ParamInt     0     1       2              0\n17:   classif.rpart.usesurrogate ParamInt     0     2       3              2\n18:           classif.rpart.xval ParamInt     0   Inf     Inf             10\n1 variable not shown: [value]\n\n\n\n\n\n\n\n\nBinder, Martin, Florian Pfisterer, Michel Lang, Lennart Schneider, Lars Kotthoff, and Bernd Bischl. 2021. “mlr3pipelines - Flexible Machine Learning Pipelines in R.” Journal of Machine Learning Research 22 (184): 1–7. http://jmlr.org/papers/v22/21-0281.html.\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40.\n\n\nWolpert, David H. 1992. “Stacked Generalization.” Neural Networks 5 (2): 241–59. https://doi.org/https://doi.org/10.1016/S0893-6080(05)80023-1."
  },
  {
    "objectID": "special.html#survival",
    "href": "special.html#survival",
    "title": "8  Special Tasks",
    "section": "\n8.1 Survival Analysis",
    "text": "8.1 Survival Analysis\n\n\n\n\n\n\nWarning\n\n\n\nSurvival analysis with {mlr3proba} is currently in a fragile state after its removal from CRAN. Hence most code examples listed in this page will not work for the time being. We are actively working on a solution!\n\n\nSurvival analysis is a sub-field of supervised machine learning in which the aim is to predict the survival distribution of a given individual. Arguably the main feature of survival analysis is that, unlike classification and regression, learners are trained on two features:\n\nthe time until the event takes place\nthe event type: either censoring or death.\n\nAt a particular time-point, an individual is either: alive, dead, or censored. Censoring occurs if it is unknown if an individual is alive or dead. For example, say we are interested in patients in hospital and every day it is recorded if they are alive or dead, then after a patient leaves, it is unknown if they are alive or dead. Hence they are censored. If there was no censoring, then ordinary regression analysis could be used instead. Furthermore, survival data contains solely positive values and therefore needs to be transformed to avoid biases.\nNote that survival analysis accounts for both censored and uncensored observations while adjusting respective model parameters.\nThe package mlr3proba (Sonabend et al. 2021) extends mlr3 with the following objects for survival analysis:\n\n\nTaskSurv to define (censored) survival tasks\n\nLearnerSurv as base class for survival learners\n\nPredictionSurv as specialized class for Prediction objects\n\nMeasureSurv as specialized class for performance measures\n\nFor a good introduction to survival analysis see Modelling Survival Data in Medical Research (Collett 2014).\n\n8.1.1 TaskSurv\nUnlike TaskClassif and TaskRegr which have a single ‘target’ argument, TaskSurv mimics the survival::Surv object and has three to four target arguments (dependent on censoring type). A TaskSurv can be constructed with the function as_task_surv():\n\nlibrary(\"mlr3\")\nlibrary(\"mlr3proba\")\nlibrary(\"survival\")\n\nas_task_surv(survival::bladder2[, -1L], id = \"interval_censored\",\n  time = \"start\", event = \"event\", time2 = \"stop\", type = \"interval\")\n\n# type = \"right\" is default\ntask = as_task_surv(survival::rats, id = \"right_censored\",\n  time = \"time\", event = \"status\", type = \"right\")\n\nprint(task)\n\n# the target column is a survival object:\nhead(task$truth())\n\n# kaplan-meier plot\n# library(\"mlr3viz\")\nautoplot(task)\n\n\n8.1.2 Predict Types - crank, lp, and distr\nEvery PredictionSurv object can predict one or more of:\n\n\nlp - Linear predictor calculated as the fitted coefficients multiplied by the test data.\n\ndistr - Predicted survival distribution, either discrete or continuous. Implemented in distr6.\n\ncrank - Continuous risk ranking.\n\nresponse - Predicted survival time.\n\nlp and crank can be used with measures of discrimination such as the concordance index. Whilst lp is a specific mathematical prediction, crank is any continuous ranking that identifies who is more or less likely to experience the event. So far the only implemented learner that only returns a continuous ranking is surv.svm. If a PredictionSurv returns an lp then the crank is identical to this. Otherwise crank is calculated as the expectation of the predicted survival distribution. Note that for linear proportional hazards models, the ranking (but not necessarily the crank score itself) given by lp and the expectation of distr, is identical.\nThe example below uses the rats task shipped with mlr3proba.\n\ntask = tsk(\"rats\")\nlearn = lrn(\"surv.coxph\")\n\ntrain_set = sample(task$nrow, 0.8 * task$nrow)\ntest_set = setdiff(seq_len(task$nrow), train_set)\n\nlearn$train(task, row_ids = train_set)\nprediction = learn$predict(task, row_ids = test_set)\n\nprint(prediction)\n\n<PredictionSurv> for 60 observations:\n    row_ids time status      crank         lp     distr\n          2   49   TRUE -0.5038971 -0.5038971 <list[1]>\n          3  104  FALSE -0.5038971 -0.5038971 <list[1]>\n         12  102  FALSE -3.4446695 -3.4446695 <list[1]>\n---                                                    \n        281   89  FALSE -2.5217340 -2.5217340 <list[1]>\n        295  104  FALSE  1.3955682  1.3955682 <list[1]>\n        300  102  FALSE -2.4602050 -2.4602050 <list[1]>\n\n\n\n8.1.3 Composition\nFinally we take a look at the PipeOps implemented in mlr3proba, which are used for composition of predict types. For example, a predict linear predictor does not have a lot of meaning by itself, but it can be composed into a survival distribution. See mlr3pipelines for full tutorials and details on PipeOps.\n\nlibrary(\"mlr3pipelines\")\nlibrary(\"mlr3learners\")\n# PipeOpDistrCompositor - Train one model with a baseline distribution,\n# (Kaplan-Meier or Nelson-Aalen), and another with a predicted linear predictor.\ntask = tsk(\"rats\")\n# remove the factor column for support with glmnet\ntask$select(c(\"litter\", \"rx\"))\nlearner_lp = lrn(\"surv.glmnet\")\nlearner_distr = lrn(\"surv.kaplan\")\nprediction_lp = learner_lp$train(task)$predict(task)\nprediction_distr = learner_distr$train(task)$predict(task)\nprediction_lp$distr\n\n# Doesn't need training. Base = baseline distribution. ph = Proportional hazards.\n\npod = po(\"compose_distr\", form = \"ph\", overwrite = FALSE)\nprediction = pod$predict(list(base = prediction_distr, pred = prediction_lp))$output\n\n# Now we have a predicted distr!\n\nprediction$distr\n\n# This can all be simplified by using the distrcompose pipeline\n\nglm.distr = ppl(\"distrcompositor\", learner = lrn(\"surv.glmnet\"),\n  estimator = \"kaplan\", form = \"ph\", overwrite = FALSE, graph_learner = TRUE)\nglm.distr$train(task)$predict(task)\n\n\n8.1.4 Benchmark Experiment\nFinally, we conduct a small benchmark study on the rats task using some of the integrated survival learners:\n\nlibrary(\"mlr3learners\")\n\ntask = tsk(\"rats\")\n\n# some integrated learners\nlearners = lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.ranger\"))\nprint(learners)\n\n$surv.coxph\n<LearnerSurvCoxPH:surv.coxph>: Cox Proportional Hazards\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3proba, survival, distr6\n* Predict Types:  crank, [distr], lp\n* Feature Types: logical, integer, numeric, factor\n* Properties: weights\n\n$surv.kaplan\n<LearnerSurvKaplan:surv.kaplan>: Kaplan-Meier Estimator\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3proba, survival, distr6\n* Predict Types:  [crank], distr\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: missings\n\n$surv.ranger\n<LearnerSurvRanger:surv.ranger>: Random Forest\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3proba, mlr3extralearners, ranger\n* Predict Types:  crank, [distr]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: importance, oob_error, weights\n\n# Harrell's C-Index for survival\nmeasure = msr(\"surv.cindex\")\nprint(measure)\n\n<MeasureSurvCindex:surv.cindex>\n* Packages: mlr3, mlr3proba\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: weight_meth=I, tiex=0.5, eps=0.001\n* Properties: -\n* Predict type: crank\n* Return type: Score\n\nset.seed(1)\nbmr = benchmark(benchmark_grid(task, learners, rsmp(\"cv\", folds = 3)))\nbmr$aggregate(measure)\n\n   nr      resample_result task_id  learner_id resampling_id iters surv.cindex\n1:  1 <ResampleResult[21]>    rats  surv.coxph            cv     3   0.7671307\n2:  2 <ResampleResult[21]>    rats surv.kaplan            cv     3   0.5000000\n3:  3 <ResampleResult[21]>    rats surv.ranger            cv     3   0.7799153\n\nautoplot(bmr, measure = measure)\n\n\n\n\nThe experiment indicates that both the Cox PH and the random forest have better discrimination than the Kaplan-Meier baseline estimator, but that the machine learning random forest is not consistently better than the interpretable Cox PH."
  },
  {
    "objectID": "special.html#density",
    "href": "special.html#density",
    "title": "8  Special Tasks",
    "section": "\n8.2 Density Estimation",
    "text": "8.2 Density Estimation\n\n\n\n\n\n\nWarning\n\n\n\nSurvival analysis with {mlr3proba} is currently in a fragile state after its removal from CRAN. Hence most code examples listed in this page will not work for the time being. We are actively working on a solution!\n\n\nDensity estimation is the learning task to find the unknown distribution from which an i.i.d. data set is generated. We interpret this broadly, with this distribution not necessarily being continuous (so may possess a mass not density). The conditional case, where a distribution is predicted conditional on covariates, is known as ‘probabilistic supervised regression’, and will be implemented in mlr3proba in the near-future. Unconditional density estimation is viewed as an unsupervised task. For a good overview to density estimation see Density estimation for statistics and data analysis (Silverman 1986).\nThe package mlr3proba extends mlr3 with the following objects for density estimation:\n\n\nTaskDens to define density tasks\n\nLearnerDens as base class for density estimators\n\nPredictionDens as specialized class for Prediction objects\n\nMeasureDens as specialized class for performance measures\n\nIn this example we demonstrate the basic functionality of the package on the faithful data from the datasets package. This task ships as pre-defined TaskDens with mlr3proba.\n\nlibrary(\"mlr3\")\nlibrary(\"mlr3proba\")\n\ntask = tsk(\"precip\")\nprint(task)\n\n<TaskDens:precip> (70 x 1): Annual Precipitation\n* Target: -\n* Properties: -\n* Features (1):\n  - dbl (1): precip\n\n# histogram and density plot\nlibrary(\"mlr3viz\")\n# FIXME\n# autoplot(task, type = \"overlay\")\n\nUnconditional density estimation is an unsupervised method. Hence, TaskDens is an unsupervised task which inherits directly from Task unlike TaskClassif and TaskRegr. However, TaskDens still has a target argument and a $truth field defined by:\n\n\ntarget - the name of the variable in the data for which to estimate density\n\n$truth - the values of the target column (which is not the true density, which is always unknown)\n\n\n8.2.1 Train and Predict\nDensity learners have train and predict methods, though being unsupervised, ‘prediction’ is actually ‘estimation’. In training, a distr6 object is created, see here for full tutorials on how to access the probability density function, pdf, cumulative distribution function, cdf, and other important fields and methods. The predict method is simply a wrapper around self$model$pdf and if available self$model$cdf, i.e. evaluates the pdf/cdf at given points. Note that in prediction the points to evaluate the pdf and cdf are determined by the target column in the TaskDens object used for testing.\n\n# create task and learner\n\ntask_faithful = TaskDens$new(id = \"eruptions\", backend = datasets::faithful$eruptions)\nlearner = lrn(\"dens.hist\")\n\n# train/test split\ntrain_set = sample(task_faithful$nrow, 0.8 * task_faithful$nrow)\ntest_set = setdiff(seq_len(task_faithful$nrow), train_set)\n\n# fitting KDE and model inspection\nlearner$train(task_faithful, row_ids = train_set)\nlearner$model\n\n$distr\nHistogram() \n\n$hist\n$breaks\n[1] 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5\n\n$counts\n[1] 39 30  5  4 25 62 49  3\n\n$density\n[1] 0.35944700 0.27649770 0.04608295 0.03686636 0.23041475 0.57142857 0.45161290\n[8] 0.02764977\n\n$mids\n[1] 1.75 2.25 2.75 3.25 3.75 4.25 4.75 5.25\n\n$xname\n[1] \"dat\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\nattr(,\"class\")\n[1] \"dens.hist\"\n\nclass(learner$model)\n\n[1] \"dens.hist\"\n\n# make predictions for new data\nprediction = learner$predict(task_faithful, row_ids = test_set)\n\nEvery PredictionDens object can estimate:\n\n\npdf - probability density function\n\nSome learners can estimate:\n\n\ncdf - cumulative distribution function\n\n8.2.2 Benchmark Experiment\nFinally, we conduct a small benchmark study on the precip task using some of the integrated survival learners:\n\n# some integrated learners\nlearners = lrns(c(\"dens.hist\", \"dens.kde\"))\nprint(learners)\n\n$dens.hist\n<LearnerDensHistogram:dens.hist>: Histogram Density Estimator\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3proba, distr6\n* Predict Types:  [pdf], cdf, distr\n* Feature Types: integer, numeric\n* Properties: -\n\n$dens.kde\n<LearnerDensKDE:dens.kde>: Kernel Density Estimator\n* Model: -\n* Parameters: kernel=Epan, bandwidth=silver\n* Packages: mlr3, mlr3proba, distr6\n* Predict Types:  [pdf], distr\n* Feature Types: integer, numeric\n* Properties: missings\n\n# Logloss for probabilistic predictions\nmeasure = msr(\"dens.logloss\")\nprint(measure)\n\n<MeasureDensLogloss:dens.logloss>: Log Loss\n* Packages: mlr3, mlr3proba\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: eps=1e-15\n* Properties: -\n* Predict type: pdf\n\nset.seed(1)\nbmr = benchmark(benchmark_grid(task, learners, rsmp(\"cv\", folds = 3)))\nbmr$aggregate(measure)\n\n   nr      resample_result task_id learner_id resampling_id iters dens.logloss\n1:  1 <ResampleResult[21]>  precip  dens.hist            cv     3     4.396138\n2:  2 <ResampleResult[21]>  precip   dens.kde            cv     3     4.817715\n\nautoplot(bmr, measure = measure)\n\n\n\n\nThe results of this experiment show that the sophisticated Penalized Density Estimator does not outperform the baseline histogram, but that the Kernel Density Estimator has at least consistently better (i.e. lower logloss) results."
  },
  {
    "objectID": "special.html#spatiotemporal",
    "href": "special.html#spatiotemporal",
    "title": "8  Special Tasks",
    "section": "\n8.3 Spatiotemporal Analysis",
    "text": "8.3 Spatiotemporal Analysis\nData observations may entail reference information about spatial or temporal characteristics. Spatial information is stored as coordinates, usually named “x” and “y” or “lat”/“lon”. Treating spatiotemporal data using non-spatial data methods can lead to over-optimistic performance estimates. Hence, methods specifically designed to account for the special nature of spatiotemporal data are needed.\nIn the mlr3 framework, the following packages relate to this field:\n\n\nmlr3spatiotempcv (biased-reduced performance estimation)\n\nmlr3spatial (spatial prediction support)\n\nThe following (sub-)sections introduce the potential pitfalls of spatiotemporal data in machine learning and how to account for it. Note that not all functionality will be covered, and that some of the used packages are still in early lifecycles. If you want to contribute to one of the packages mentioned above, please contact Patrick Schratz.\n\n8.3.1 Creating a spatial Task\nTo make use of spatial resampling methods, a {mlr3} task that is aware of its spatial characteristic needs to be created. Two Task child classes exist in {mlr3spatiotempcv} for this purpose:\n\nTaskClassifST\nTaskRegrST\n\nTo create one of these, you have multiple options:\n\nUse the constructor of the Task directly via $new() - this only works for data.table backends (!)\nUse the as_task_* converters (e.g. if your data is stored in an sf object)\n\nWe recommend the latter, as the as_task_* converters aim to make task construction easier, e.g., by creating the DataBackend (which is required to create a Task in {mlr3}) automatically and setting the crs and coordinate_names fields. Let’s assume your (point) data is stored in with an sf object, which is a common scenario for spatial analysis in R.\n\n# create 'sf' object\ndata_sf = sf::st_as_sf(ecuador, coords = c(\"x\", \"y\"), crs = 32717)\n\n# create `TaskClassifST` from `sf` object\ntask = as_task_classif_st(data_sf, id = \"ecuador_task\", target = \"slides\", positive = \"TRUE\")\n\nYou can also use a plain data.frame. In this case, crs and coordinate_names need to be passed along explicitly as they cannot be inferred directly from the sf object:\n\ntask = as_task_classif_st(ecuador, id = \"ecuador_task\", target = \"slides\",\n  positive = \"TRUE\", coordinate_names = c(\"x\", \"y\"), crs = 32717)\n\nThe *ST task family prints a subset of the coordinates by default:\n\nprint(task)\n\n<TaskClassifST:ecuador_task> (751 x 11)\n* Target: slides\n* Properties: twoclass\n* Features (10):\n  - dbl (10): carea, cslope, dem, distdeforest, distroad,\n    distslidespast, hcurv, log.carea, slope, vcurv\n* Coordinates:\n            x       y\n  1: 712882.5 9560002\n  2: 715232.5 9559582\n  3: 715392.5 9560172\n  4: 715042.5 9559312\n  5: 715382.5 9560142\n ---                 \n747: 714472.5 9558482\n748: 713142.5 9560992\n749: 713322.5 9560562\n750: 715392.5 9557932\n751: 713802.5 9560862\n\n\nAll *ST tasks can be treated as their super class equivalents TaskClassif or TaskRegr in subsequent {mlr3} modeling steps.\n\n8.3.2 Autocorrelation\nData which includes spatial or temporal information requires special treatment in machine learning (similar to survival, ordinal and other task types listed in the special tasks chapter). In contrast to non-spatial/non-temporal data, observations inherit a natural grouping, either in space or time or in both space and time (Legendre 1993). This grouping causes observations to be autocorrelated, either in space (spatial autocorrelation (SAC)), time (temporal autocorrelation (TAC)) or both space and time (spatiotemporal autocorrelation (STAC)). For simplicity, the acronym STAC is used as a generic term in the following chapter for all the different characteristics introduced above.\nWhat effects does STAC have in statistical/machine learning?\nThe overarching problem is that STAC violates the assumption that the observations in the train and test datasets are independent (Hastie, Friedman, and Tibshirani 2001). If this assumption is violated, the reliability of the resulting performance estimates, for example retrieved via cross-validation, is decreased. The magnitude of this decrease is linked to the magnitude of STAC in the dataset, which cannot be determined easily.\nOne approach to account for the existence of STAC is to use dedicated resampling methods. mlr3spatiotempcv provides access to the most frequently used spatiotemporal resampling methods. The following example showcases how a spatial dataset can be used to retrieve a bias-reduced performance estimate of a learner.\nThe following examples use the ecuador dataset created by Jannes Muenchow. It contains information on the occurrence of landslides (binary) in the Andes of Southern Ecuador. The landslides were mapped from aerial photos taken in 2000. The dataset is well suited to serve as an example because it it relatively small and of course due to the spatial nature of the observations. Please refer to Muenchow, Brenning, and Richter (2012) for a detailed description of the dataset.\nTo account for the spatial autocorrelation probably present in the landslide data, we will make use one of the most used spatial partitioning methods, a cluster-based k-means grouping (Brenning 2012), (spcv_coords in mlr3spatiotempcv). This method performs a clustering in 2D space which contrasts with the commonly used random partitioning for non-spatial data. The grouping has the effect that train and test data are more separated in space as they would be by conducting a random partitioning, thereby reducing the effect of STAC.\nBy contrast, when using the classical random partitioning approach with spatial data, train and test observations would be located side-by-side across the full study area (a visual example is provided further below). This leads to a high similarity between train and test sets, resulting in “better” but biased performance estimates in every fold of a CV compared to the spatial CV approach. However, these low error rates are mainly caused due to the STAC in the observations and the lack of appropriate partitioning methods and not by the power of the fitted model.\n\n8.3.3 Spatiotemporal Cross-Validation and Partitioning\nOne part of spatiotemporal machine learning is dealing with the spatiotemporal components of the data during performance estimation. Performance is commonly estimated via cross-validation and mlr3spatiotempcv provides specialized resamplings methods for spatiotemporal data. The following chapters showcases how these methods can be applied and how they differ compared to non-spatial resampling methods, e.g. random partitioning. In addition, examples which show how resamplings with spatial information can be visualized using mlr3spatiotempcv.\nBesides performance estimation, prediction on spatiotemporal data is another challenging task. See Section 8.3.6 for more information about how this topic is handled within the mlr3 framework.\n\n8.3.3.1 Spatial CV vs. Non-Spatial CV\nIn the following a spatial and non-spatial CV will be applied to showcase the mentioned performance differences.\nThe performance of a simple classification tree (\"classif.rpart\") is evaluated on a random partitioning (repeated_cv) with four folds and two repetitions. The chosen evaluation measure is “classification error” (\"classif.ce\").\nFor the spatial example, repeated_spcv_coords is chosen whereas repeated_cv represents the non-spatial example.\n\n\n\n\n\n\nNote\n\n\n\nThe selection of repeated_spcv_coords in this example is arbitrary. For your use case, you might want to use a different spatial partitioning method (but not necessarily!). Have a look at the “Getting Started” vignette of mlr3spatiotempcv to see all available methods and choose one which fits your data and its prediction purpose.\n\n\n\n8.3.3.1.1 Non-Spatial CV\nIn this example the ecuador example task is taken to estimate the performance of an rpart learner with fixed parameters on it.\n\n\n\n\n\n\nWarning\n\n\n\nIn practice you usually might want to tune the hyperparameters of the learner in this case and apply a nested CV in which the inner loop is used for hyperparameter tuning.\n\n\n\nlibrary(\"mlr3\")\nlibrary(\"mlr3spatiotempcv\")\nset.seed(42)\n\n# be less verbose\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\ntask = tsk(\"ecuador\")\n\nlearner = lrn(\"classif.rpart\", maxdepth = 3, predict_type = \"prob\")\nresampling_nsp = rsmp(\"repeated_cv\", folds = 4, repeats = 2)\nrr_nsp = resample(\n  task = task, learner = learner,\n  resampling = resampling_nsp)\n\nrr_nsp$aggregate(measures = msr(\"classif.ce\"))\n\nclassif.ce \n 0.3388575 \n\n\n\n8.3.3.1.2 Spatial CV\n\ntask = tsk(\"ecuador\")\n\nlearner = lrn(\"classif.rpart\", maxdepth = 3, predict_type = \"prob\")\nresampling_sp = rsmp(\"repeated_spcv_coords\", folds = 4, repeats = 2)\nrr_sp = resample(\n  task = task, learner = learner,\n  resampling = resampling_sp)\n\nrr_sp$aggregate(measures = msr(\"classif.ce\"))\n\nclassif.ce \n  0.412529 \n\n\nHere, the performance of the classification tree learner is around 7 percentage points worse when using Spatial Cross-Validation (SpCV) compared to Non-Spatial Cross-Validation (NSpCV). The resulting difference in performance is variable as it depends on the dataset, the magnitude of STAC and the learner itself. For algorithms with a higher tendency of overfitting to the training set, the difference between the two methods will be larger.\nNow, what does it mean that the performance in the spatial case is worse? Should you ditch SpCV and keep using non-spatial partitioning? The answer is NO. The reason why the spatial partitioning scenario results in a lower predictive performance is because throughout the CV the model has been trained on data that is less similar than the test data compared against the non-spatial scenario. Or in other words: in the non-spatial scenario, train and test data are almost identical (due to spatial autocorrelation).\nThis means that the result from the SpCV setting is more close to the true predictive power of the model - whereas the result from non-spatial CV is overoptimistic and biased.\n\n\n\n\n\n\nNote\n\n\n\nThe result of the SpCV setting is by no means the absolute truth - it is also biased, but (most often) less compared to the non-spatial setting.\n\n\n\n8.3.4 Visualization of Spatiotemporal Partitions\nEvery partitioning method in mlr3spatiotempcv comes with S3 methods for plot() and autoplot() to visualize the created groups. In a 2D space ggplot2 is used in the backgroudn while for spatiotemporal methods 3D visualizations via plotly are created.\nThe following examples shows how the resampling_sp object from the previous example can be visualized. In this case I want to look at the first four partitions of the first repetition. The point size is adjusted via argument size. After the plot creation, additional scale_* calls are used to adjust the coordinate labels on the x and y axes, respectively.\n\nautoplot(resampling_sp, task, fold_id = c(1:4), size = 0.7) *\n  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *\n  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.02))\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that setting the correct CRS for the given data is important which is done during task creation. Spatial offsets of up to multiple meters may occur if the wrong CRS is supplied initially.\n\n\nThis example used a built-in mlr3 task via tsk(). In practice however, one needs to create a spatiotemporal task via TaskClassifST/TaskRegrST and set the crs argument (unless a sf object is handed over).\nmlr3spatiotempcv can also visualize non-spatial partitonings. This helps to visually compare differences. Let’s use the objects from the previous example again, this time resampling_nsp.\n\nautoplot(resampling_nsp, task, fold_id = c(1:4), size = 0.7) *\n  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *\n  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.02))\n\n\n\n\nThe visualization show very well how close train and test observations are located next to each other.\n\n8.3.4.1 Spatial Block Visualization\nThis examples showcases another SpCV method: spcv_block. This method makes use of rectangular blocks to divide the study area into equally-sized parts. {mlr3spatiotempcv} has support for visualizing the created blocks and displaying their respective fold ID to get a better understanding how the final folds were composed out of the partitions. E.g. the “Fold 1 Repetition 1” plot shows that the test set is composed out of two “blocks” with the ID “1” in this case.\n\n\n\n\n\n\nNote\n\n\n\nThe use of range = 1000L is arbitrary here and should not be copy-pasted into a real application.\n\n\n\ntask = tsk(\"ecuador\")\nresampling = rsmp(\"spcv_block\", range = 1000L)\n\n# Visualize train/test splits of multiple folds\nautoplot(resampling, task, size = 0.7,\n  fold_id = c(1, 2), show_blocks = TRUE, show_labels = TRUE) *\n  ggplot2::scale_x_continuous(breaks = seq(-79.085, -79.055, 0.02))\n\n\n\n\n\n8.3.4.2 Spatiotemporal Visualization\nWhen going spatiotemporal, two dimensions are not enough anymore. To visualize space and time together, a 3D solution is needed. {mlr3spatiotempcv} makes use of plotly for this purpose.\nThe following examples uses a modified version of the cookfarm_mlr3 task for showcasing reasons. By adjusting some levels, the individual partitions can be recognized very well.\n\n\n\n\n\n\nWarning\n\n\n\nIn practice, you should not modify your data to achieve “good looking” visualizations as done in this example. This is only done for (visual) demonstration purposes.\n\n\nIn the following we use the sptcv_cstf method after Meyer et al. (2018). Only the temporal variable is used in this first example, denoted by setting the column role “time” to variable “Date”. This column role is then picked up by the resampling method. Last, autoplot() is called with an explicit definition of plot3D = TRUE. This is because sptcv_cstf can also be visualized in 2D (which only makes sense if the “space” column role is used for partitioning).\nLast, sample_fold_n is used to take a stratified random sample from all partitions. The call to plotly::layout() only adjusts the default viewing angle of the plot - in interactive visualizations, this is not needed and the viewing angle can be adjusted with the mouse.\n\n\n\n\n\n\nWarning\n\n\n\nThis is done to reduce the final plot size and keep things small for demos like this one. We don’t recommend doing this in actual studies - unless you prominently communicate this alongside the resulting plot.\n\n\n\ndata = cookfarm_mlr3\nset.seed(42)\ndata$Date = sample(rep(c(\n  \"2020-01-01\", \"2020-02-01\", \"2020-03-01\", \"2020-04-01\",\n  \"2020-05-01\"), times = 1, each = 35768))\ntask_spt = as_task_regr_st(data,\n  id = \"cookfarm\", target = \"PHIHOX\",\n  coordinate_names = c(\"x\", \"y\"), coords_as_features = FALSE,\n  crs = 26911)\ntask_spt$set_col_roles(\"Date\", roles = \"time\")\n\nrsmp_cstf_time = rsmp(\"sptcv_cstf\", folds = 5)\n\nplot = autoplot(rsmp_cstf_time,\n  fold_id = 5, task = task_spt, plot3D = TRUE,\n  sample_fold_n = 3000L\n)\nplotly::layout(plot, scene = list(camera = list(eye = list(z = 0.58))))\n\nIf both space and time are used for partitioning in sptcv_cstf, the visualization becomes even more powerful as it allows to also show the observations which are omitted, i.e., not being used in either train and test sets for a specific fold.\n\ntask_spt$set_col_roles(\"SOURCEID\", roles = \"space\")\ntask_spt$set_col_roles(\"Date\", roles = \"time\")\n\nrsmp_cstf_space_time = rsmp(\"sptcv_cstf\", folds = 5)\n\nplot = autoplot(rsmp_cstf_space_time,\n  fold_id = 4, task = task_spt, plot3D = TRUE,\n  show_omitted = TRUE, sample_fold_n = 3000L)\n\nplotly::layout(plot, scene = list(camera =\nlist(eye = list(z = 0.58, x = -1.4, y = 1.6))))\n\nCombining multiple spatiotemporal plots with plotly::layout() is possible but somewhat cumbersome. First, a list of plots containing the individuals plots must be created. These plots can then be passed to plotly::subplot(). This return is then passed to plotly::layout().\n\npl = autoplot(rsmp_cstf_space_time, task = task_spt,\n  fold_id = c(1, 2, 3, 4), point_size = 3,\n  axis_label_fontsize = 10, plot3D = TRUE,\n  sample_fold_n = 3000L, show_omitted = TRUE\n)\n\n# Warnings can be ignored\npl_subplot = plotly::subplot(pl)\n\nplotly::layout(pl_subplot,\n  title = \"Individual Folds\",\n  scene = list(\n    domain = list(x = c(0, 0.5), y = c(0.5, 1)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(z = 0.20, x = -1.4, y = 1.6))\n  ),\n  scene2 = list(\n    domain = list(x = c(0.5, 1), y = c(0.5, 1)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(z = 0.1, x = -1.4, y = 1.6))\n  ),\n  scene3 = list(\n    domain = list(x = c(0, 0.5), y = c(0, 0.5)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(z = 0.1, x = -1.4, y = 1.6))\n  ),\n  scene4 = list(\n    domain = list(x = c(0.5, 1), y = c(0, 0.5)),\n    aspectmode = \"cube\",\n    camera = list(eye = list(z = 0.58, x = -1.4, y = 1.6))\n  )\n)\n\nUnfortunately, titles in subplots cannot be created dynamically. However, there is a manual workaround via annotations show in this RPubs post.\n\n8.3.5 Choosing a Resampling Method\nWhile the example in this section made use of the spcv_coords method, this should by no means infer that this method is the best or only method suitable for this task. Even though this method is quite popular, it was mainly chosen because of the clear visual grouping differences when being applied on the ecuador task when compared to random partitioning.\nIn fact, most often multiple spatial partitioning methods can be used for a dataset. It is recommended (required) that users familiarize themselves with each implemented method and decide which method to choose based on the specific characteristics of the dataset. For almost all methods implemented in mlr3spatiotempcv, there is a scientific publication describing the strengths and weaknesses of the respective approach (either linked in the help file of mlr3spatiotempcv or its respective dependency packages).\n\n\n\n\n\n\nTip\n\n\n\nIn the example above, a cross-validation without hyperparameter tuning was shown. If a nested CV is desired, it is recommended to use the same spatial partitioning method for the inner loop (= tuning level). See Schratz et al. (2019) for more details and chapter 11 of Geocomputation with R (Lovelace, Nowosad, and Muenchow 2019).\n\n\n\n\n\n\n\n\nTip\n\n\n\nA list of all implemented methods in mlr3spatiotempcv can be found in the Getting Started vignette of the package.\n\n\nIf you want to learn even more about the field of spatial partitioning, STAC and the problems associated with it, the works of Prof. Hanna Meyer and Prof. Alexander Brenning are very much recommended for further reference.\n\n8.3.6 Spatial Prediction\nSupport for (parallel) spatial prediction with terra, raster, stars and sf objects is available via mlr3spatial.\nmlr3spatial has two main scopes:\n\nProvide DataBackends for spatial objects (vector and raster data)\nSimplify spatial prediction tasks\n\nOverall it aims to reduce the roundtripping between spatial objects -> data.frame / data.table -> spatial object during modeling.\n\n8.3.6.1 Spatial Data Backends\nA common scenario is the existence of spatial information in (point) vector data. mlr3spatial provides as_task_* helpers to directly convert these into a spatiotemporal (ST) task:\n\nlibrary(mlr3verse)\nlibrary(mlr3spatial)\nlibrary(sf)\n\n# load sample points\nleipzig_vector = sf::read_sf(system.file(\"extdata\",\n  \"leipzig_points.gpkg\", package = \"mlr3spatial\"),\n  stringsAsFactors = TRUE)\n\n# create land cover task\ntsk_leipzig = as_task_classif_st(leipzig_vector, target = \"land_cover\")\ntsk_leipzig\n\n<TaskClassifST:leipzig_vector> (97 x 9)\n* Target: land_cover\n* Properties: multiclass\n* Features (8):\n  - dbl (8): b02, b03, b04, b06, b07, b08, b11, ndvi\n* Coordinates:\n           X       Y\n 1: 732480.1 5693957\n 2: 732217.4 5692769\n 3: 732737.2 5692469\n 4: 733169.3 5692777\n 5: 732202.2 5692644\n---                 \n93: 733018.7 5692342\n94: 732551.4 5692887\n95: 732520.4 5692589\n96: 732542.2 5692204\n97: 732437.8 5692300\n\n\nThis saves users from stripping the coordinates from the vector data or transforming the sf object to a data.frame or data.table in the first place.\nmlr3spatial adds support for the following spatial data classes:\n\n\nstars (from package stars)\n\nSpatRaster (from package terra)\n\nRasterLayer (from package raster)\n\nRasterStack (from package raster)\n\n8.3.6.2 Spatial prediction\nThe goal of spatial prediction is usually a raster image that covers a specific area. Most often this area is quite large and contains millions of pixels. The goal is to predict on each pixel and return a raster image containing these predictions.\nTo save users from having to go the extra mile of extracting the final model and using it with the respective predict() function of the spatial R package of their desired outcome class, mlr3spatial provides predict_spatial(). predict_spatial() let’s users\n\nchoose the output class of the resulting raster image\noptionally predict in parallel via the future package, using a parallel backend of their choice\n\nTo use a raster image for prediction, it must be wrapped into a TaskUnsupervised for internal mlr3 reasons. Next, the learner can be used to predict on the task.\n\nleipzig_raster = terra::rast(system.file(\"extdata\", \"leipzig_raster.tif\",\n  package = \"mlr3spatial\"))\ntsk_predict = as_task_unsupervised(leipzig_raster)\n\nlrn = lrn(\"classif.ranger\")\nlrn$train(tsk_leipzig)\n\n# plan(\"multisession\") # optional parallelization\npred = predict_spatial(tsk_predict, lrn, format = \"terra\")\nclass(pred)\n\n\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nThe resulting object can be visualized and treated as any other terra object. Thanks to the improved handling of factor variables in terra, the raster image contains the correct labeled classes from the prediction right away.\n\nlibrary(terra, exclude = \"resample\")\nplot(pred, col = c(\"#440154FF\", \"#443A83FF\", \"#31688EFF\",\n  \"#21908CFF\", \"#35B779FF\", \"#8FD744FF\", \"#FDE725FF\"))\n\n\n\n\nIf you are interested in the performance of parallel spatial prediction, have a look at the Spatial Prediction Benchmark vignette - the resulting plot is shown below.\n\n\n\n\n\n\nThe results of the shown benchmark results should be taken with care as something seems to go wrong when using all-core parallelization with terra::predict(). Also both relative and absolute numbers will vary on your local machine and results might change depending on eventual package updates in the future."
  },
  {
    "objectID": "special.html#cost-sens",
    "href": "special.html#cost-sens",
    "title": "8  Special Tasks",
    "section": "\n8.4 Cost-Sensitive Classification",
    "text": "8.4 Cost-Sensitive Classification\nIn regular classification, the aim is to minimize the misclassification rate, and thus all types of misclassification errors are deemed equally severe. A more general setting is cost-sensitive classification. Cost-sensitive classification does not assume that the costs caused by different kinds of errors are equal. The objective of cost-sensitive classification is to minimize the expected costs.\nImagine you are an analyst for a big credit institution. Let’s also assume that a correct bank decision would result in 35% of the profit at the end of a specific period. A correct decision means that the bank predicts that a customer will pay their bills (hence would obtain a loan), and the customer indeed has good credit. On the other hand, a wrong decision means that the bank predicts that the customer’s credit is in good standing, but the opposite is true. This would result in a loss of 100% of the given loan.\n\n\n\n\n\n\n\n\nGood Customer (truth)\nBad Customer (truth)\n\n\n\nGood Customer (predicted)\n+ 0.35\n- 1.0\n\n\nBad Customer (predicted)\n0\n0\n\n\n\nExpressed as costs (instead of profit), we can write down the cost-matrix as follows:\n\ncosts = matrix(c(-0.35, 0, 1, 0), nrow = 2)\ndimnames(costs) = list(response = c(\"good\", \"bad\"), truth = c(\"good\", \"bad\"))\nprint(costs)\n\n        truth\nresponse  good bad\n    good -0.35   1\n    bad   0.00   0\n\n\nAn exemplary data set for such a problem is the German Credit task:\n\nlibrary(\"mlr3verse\")\ntask = tsk(\"german_credit\")\ntable(task$truth())\n\n\ngood  bad \n 700  300 \n\n\nThe data has 70% of customers who can pay back their credit and 30% of bad customers who default on their debt. A manager, who doesn’t have any model, could decide to give either everybody credit or to give nobody credit. The resulting costs for the German credit data are:\n\n# nobody:\n(700 * costs[2, 1] + 300 * costs[2, 2]) / 1000\n\n[1] 0\n\n# everybody\n(700 * costs[1, 1] + 300 * costs[1, 2]) / 1000\n\n[1] 0.055\n\n\nIf the average loan is $20,000, the credit institute will lose more than one million dollars if it would grant everybody a credit:\n\n# average profit * average loan * number of customers\n0.055 * 20000 * 1000\n\n[1] 1100000\n\n\nOur goal is to find a model that minimizes the costs (and maximizes the expected profit).\n\n8.4.1 A First Model\nFor our first model, we choose an ordinary logistic regression (implemented in add-on package mlr3learners). We first create a classification task, then resample the model using 10-fold cross-validation and extract the resulting confusion matrix:\n\nlearner = lrn(\"classif.log_reg\")\nrr = resample(task, learner, rsmp(\"cv\"))\n\nconfusion = rr$prediction()$confusion\nprint(confusion)\n\n        truth\nresponse good bad\n    good  606 154\n    bad    94 146\n\n\nTo calculate the average costs like above, we can simply multiply the elements of the confusion matrix with the elements of the previously introduced cost matrix and sum the values of the resulting matrix:\n\navg_costs = sum(confusion * costs) / 1000\nprint(avg_costs)\n\n[1] -0.0581\n\n\nWith an average loan of $20,000, the logistic regression yields the following costs:\n\navg_costs * 20000 * 1000\n\n[1] -1162000\n\n\nInstead of losing over $1,000,000, the credit institute now can expect a profit of more than $1,000,000.\n\n8.4.2 Cost-sensitive Measure\nOur natural next step would be to further improve the modeling step in order to maximize the profit. For this purpose, we first create a cost-sensitive classification measure that calculates the costs based on our cost matrix. This allows us to conveniently quantify and compare modeling decisions. Fortunately, there already is a predefined measure Measure for this purpose: MeasureClassifCosts. The costs have to be provided as a numeric matrix whose columns and rows are named with class labels (just like the previously constructed costs matrix):\n\ncost_measure = msr(\"classif.costs\", costs = costs)\nprint(cost_measure)\n\n<MeasureClassifCosts:classif.costs>: Cost-sensitive Classification\n* Packages: mlr3\n* Range: [-Inf, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: normalize=TRUE\n* Properties: -\n* Predict type: response\n\n\nIf we now call resample() or benchmark(), the cost-sensitive measures will be evaluated. We compare the logistic regression to a simple featureless learner and to a random forest from package ranger :\n\nlearners = list(\n  lrn(\"classif.log_reg\"),\n  lrn(\"classif.featureless\"),\n  lrn(\"classif.ranger\")\n)\ncv10 = rsmp(\"cv\", folds = 10)\nbmr = benchmark(benchmark_grid(task, learners, cv10))\nautoplot(bmr, measure = cost_measure) + ggplot2::geom_hline(yintercept = 0, colour = \"red\")\n\n\n\n\nAs expected, the featureless learner is performing comparably badly. The logistic regression and the random forest both yield a profit on average.\n\n8.4.3 Thresholding\nAlthough we now correctly evaluate the models in a cost-sensitive fashion, the models themselves are unaware of the classification costs. They assume the same costs for both wrong classification decisions (false positives and false negatives). Some learners natively support cost-sensitive classification (e.g., XXX). However, we will concentrate on a more generic approach that works for all models which can predict probabilities for class labels: thresholding.\nMost learners can calculate the probability \\(p\\) for the positive class. If \\(p\\) exceeds the threshold \\(0.5\\), they predict the positive class and the negative class otherwise.\nFor our binary classification case of the credit data, we primarily want to minimize the errors where the model predicts “good”, but truth is “bad” (i.e., the number of false positives) as this is the more expensive error. If we now increase the threshold to values \\(> 0.5\\), we reduce the number of false negatives. Note that we increase the number of false positives simultaneously, or, in other words, we are trading false positives for false negatives.\n\n# fit models with probability prediction\nlearner = lrn(\"classif.log_reg\", predict_type = \"prob\")\nrr = resample(task, learner, rsmp(\"cv\"))\np = rr$prediction()\nprint(p)\n\n<PredictionClassif> for 1000 observations:\n    row_ids truth response  prob.good  prob.bad\n          2   bad      bad 0.40126720 0.5987328\n         17  good     good 0.97406360 0.0259364\n         46  good     good 0.74870171 0.2512983\n---                                            \n        973   bad      bad 0.09917846 0.9008215\n        976  good     good 0.78358973 0.2164103\n        997  good     good 0.51492948 0.4850705\n\n# helper function to try different threshold values interactively\nwith_threshold = function(p, th) {\n  p$set_threshold(th)\n  list(confusion = p$confusion, costs = p$score(measures = cost_measure))\n}\n\nwith_threshold(p, 0.5)\n\n$confusion\n        truth\nresponse good bad\n    good  598 152\n    bad   102 148\n\n$costs\nclassif.costs \n      -0.0573 \n\nwith_threshold(p, 0.75)\n\n$confusion\n        truth\nresponse good bad\n    good  462  75\n    bad   238 225\n\n$costs\nclassif.costs \n      -0.0867 \n\nwith_threshold(p, 1.0)\n\n$confusion\n        truth\nresponse good bad\n    good    0   0\n    bad   700 300\n\n$costs\nclassif.costs \n            0 \n\n\nThere is also an autoplot() method which systematically varies the threshold between 0 and 1 and calculates the corresponding scores:\n\nautoplot(p, type = \"threshold\", measure = cost_measure)\n\n\n\n\nInstead of manually or visually searching for good values, the base R function optimize() can do the job for us:\n\n# simple wrapper function that takes a threshold and returns the resulting model performance\n# this wrapper is passed to optimize() to find its minimum for thresholds in [0.5, 1]\nf = function(th) {\n  with_threshold(p, th)$costs\n}\nbest = optimize(f, c(0.5, 1))\nprint(best)\n\n$minimum\n[1] 0.6905289\n\n$objective\nclassif.costs \n     -0.08925 \n\n# optimized confusion matrix:\nwith_threshold(p, best$minimum)$confusion\n\n        truth\nresponse good bad\n    good  515  91\n    bad   185 209\n\n\nNote that the function \"optimize()\" is intended for unimodal functions and, therefore, may converge to a local optimum here. See the next section for better alternatives to tune the threshold.\n\n8.4.4 Threshold Tuning\nCurrently mlr3pipelines offers two main strategies towards adjusting classification thresholds. We can either expose the thresholds as a hyperparameter of the Learner by using PipeOpThreshold. This allows us to tune the thresholds via an outside optimizer from mlr3tuning. Alternatively, we can also use PipeOpTuneThreshold which automatically tunes the threshold after each learner is fit. Both methods are described in the following subsections.\n\n8.4.5 PipeOpThreshold\nPipeOpThreshold can be put directly after a Learner.\nA simple example would be:\n\ngr = lrn(\"classif.rpart\", predict_type = \"prob\") %>>% po(\"threshold\")\nl = as_learner(gr)\n\nNote, that predict_type = \"prob\" is required for po(\"threshold\") to have any effect.\nThe thresholds are now exposed as a hyperparameter of the GraphLearner we created:\n\nl$param_set\n\n<ParamSetCollection>\n                              id    class lower upper nlevels        default\n 1:             classif.rpart.cp ParamDbl     0     1     Inf           0.01\n 2:     classif.rpart.keep_model ParamLgl    NA    NA       2          FALSE\n 3:     classif.rpart.maxcompete ParamInt     0   Inf     Inf              4\n 4:       classif.rpart.maxdepth ParamInt     1    30      30             30\n 5:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf              5\n 6:      classif.rpart.minbucket ParamInt     1   Inf     Inf <NoDefault[3]>\n 7:       classif.rpart.minsplit ParamInt     1   Inf     Inf             20\n 8: classif.rpart.surrogatestyle ParamInt     0     1       2              0\n 9:   classif.rpart.usesurrogate ParamInt     0     2       3              2\n10:           classif.rpart.xval ParamInt     0   Inf     Inf             10\n11:         threshold.thresholds ParamUty    NA    NA     Inf <NoDefault[3]>\n1 variable not shown: [value]\n\n\nWe can now tune those thresholds from the outside as follows:\nBefore tuning, we have to define which hyperparameters we want to tune over. In this example, we only tune over the thresholds parameter of the threshold pipe operator. You can easily imagine that we can also jointly tune over additional hyperparameters, i.e., rpart’s cp parameter.\nAs the Task we aim to optimize for is a binary task, we can simply specify the threshold param:\n\nlibrary(\"paradox\")\nps = ps(threshold.thresholds = p_dbl(lower = 0, upper = 1))\n\nWe now create a AutoTuner which automatically tunes the supplied learner over the ParamSet we supplied above.\n\nat = AutoTuner$new(\n  learner = l,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = ps,\n  terminator = trm(\"evals\", n_evals = 5L),\n  tuner = tnr(\"random_search\")\n)\n\nat$train(tsk(\"german_credit\"))\n\nInside the trafo, we simply collect all set params into a named vector via map_dbl and store it in the threshold.thresholds slot expected by the learner.\nAgain, we create a AutoTuner, which automatically tunes the supplied learner over the ParamSet we supplied above.\nOne drawback of this strategy is that this requires us to fit a new model for each new threshold setting. While setting a threshold and computing performance is relatively cheap, fitting the learner is often more computationally demanding. An often better strategy is, therefore, to optimize the thresholds separately after each model fit.\n\n8.4.6 PipeOpTunethreshold\nPipeOpTuneThreshold on the other hand works together with PipeOpLearnerCV. It directly optimizes the cross-validated predictions made by this PipeOp. This is necessary to avoid over-fitting the threshold tuning.\nA simple example would be:\n\ngr = po(\"learner_cv\", lrn(\"classif.rpart\", predict_type = \"prob\")) %>>% po(\"tunethreshold\")\nl2 = as_learner(gr)\n\nNote, that predict_type = “prob” is required for po(\"tunethreshold\") to work. Additionally, note that this time no threshold parameter is exposed, and it is automatically tuned internally.\n\nl2$param_set\n\n<ParamSetCollection>\n                                        id    class lower upper nlevels\n 1:        classif.rpart.resampling.method ParamFct    NA    NA       2\n 2:         classif.rpart.resampling.folds ParamInt     2   Inf     Inf\n 3: classif.rpart.resampling.keep_response ParamLgl    NA    NA       2\n 4:                       classif.rpart.cp ParamDbl     0     1     Inf\n 5:               classif.rpart.keep_model ParamLgl    NA    NA       2\n 6:               classif.rpart.maxcompete ParamInt     0   Inf     Inf\n 7:                 classif.rpart.maxdepth ParamInt     1    30      30\n 8:             classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n 9:                classif.rpart.minbucket ParamInt     1   Inf     Inf\n10:                 classif.rpart.minsplit ParamInt     1   Inf     Inf\n11:           classif.rpart.surrogatestyle ParamInt     0     1       2\n12:             classif.rpart.usesurrogate ParamInt     0     2       3\n13:                     classif.rpart.xval ParamInt     0   Inf     Inf\n14:           classif.rpart.affect_columns ParamUty    NA    NA     Inf\n15:                  tunethreshold.measure ParamUty    NA    NA     Inf\n16:                tunethreshold.optimizer ParamUty    NA    NA     Inf\n17:                tunethreshold.log_level ParamUty    NA    NA     Inf\n2 variables not shown: [default, value]\n\n\nNote that we can set rsmp(\"intask\") as a resampling strategy for “learner_cv” in order to evaluate predictions on the “training” data. This is generally not advised, as it might lead to over-fitting on the thresholds but can significantly reduce runtime.\nFor more information, see the post on Threshold Tuning on the mlr3 gallery."
  },
  {
    "objectID": "special.html#cluster",
    "href": "special.html#cluster",
    "title": "8  Special Tasks",
    "section": "\n8.5 Cluster Analysis",
    "text": "8.5 Cluster Analysis\nCluster analysis is a type of unsupervised machine learning where the goal is to group data into clusters, where each cluster contains similar observations. The similarity is based on specified metrics that are task and application-dependent. Cluster analysis is closely related to classification in the sense that each observation needs to be assigned to a cluster or a class. However, unlike classification problems where each observation is labeled, clustering works on data sets without true labels or class assignments.\nThe package mlr3cluster extends mlr3 with the following objects for cluster analysis:\n\n\nTaskClust to define clustering tasks\n\nLearnerClust as base class for clustering learners\n\nPredictionClust as specialized class for Prediction objects\n\nMeasureClust as specialized class for performance measures\n\nSince clustering is a type of unsupervised learning, TaskClust is slightly different from TaskRegr and TaskClassif objects. More specifically:\n\n\ntruth() function is missing because observations are not labeled.\n\ntarget field is empty and will return character(0) if accessed anyway.\n\nAdditionally, LearnerClust provides two extra fields that are absent from supervised learners:\n\n\nassignments returns cluster assignments for training data. It returns NULL if accessed before training.\n\nsave_assignments is a boolean field that controls whether or not to store training set assignments in a learner.\n\nFinally, PredictionClust contains two additional fields:\n\n\npartition stores cluster partitions.\n\nprob stores cluster probabilities for each observation.\n\n\n8.5.1 Train and Predict\nClustering learners provide both train and predict methods. The analysis typically consists of building clusters using all available data. To be consistent with the rest of the library, we refer to this process as training.\nSome learners can assign new observations to existing groups with predict. However, prediction does not always make sense, as is the case for hierarchical clustering. In hierarchical clustering, the goal is to build a hierarchy of nested clusters by either splitting large clusters into smaller ones or merging smaller clusters into bigger ones. The final result is a tree or dendrogram which can change if a new data point is added. For consistency with the rest of the ecosystem, mlr3cluster offers predict method for hierarchical clusterers but it simply assigns all points to a specified number of clusters by cutting the resulting tree at a corresponding level. Moreover, some learners estimate the probability of each observation belonging to a given cluster. predict_types field gives a list of prediction types for each learner.\nAfter training, the model field stores a learner’s model that looks different for each learner depending on the underlying library. predict returns a PredictionClust object that gives a simplified view of the learned model. If the data given to the predict method is the same as the one on which the learner was trained, predict simply returns cluster assignments for the “training” observations. On the other hand, if the test set contains new data, predict will estimate cluster assignments for that data set. Some learners do not support estimating cluster partitions on new data and will instead return assignments for training data and print a warning message.\nIn the following example, a $k$-means learner is applied on the US arrest data set. The class labels are predicted and the contribution of the task features to assignment of the respective class are visualized.\n\nlibrary(\"mlr3\")\nlibrary(\"mlr3cluster\")\nlibrary(\"mlr3viz\")\nset.seed(1L)\n\n# create an example task\ntask = tsk(\"usarrests\")\nprint(task)\n\n<TaskClust:usarrests> (50 x 4): US Arrests\n* Target: -\n* Properties: -\n* Features (4):\n  - int (2): Assault, UrbanPop\n  - dbl (2): Murder, Rape\n\nautoplot(task)\n\n\n\n# create a k-means learner\nlearner = lrn(\"clust.kmeans\")\n\n# assigning each observation to one of the two clusters (default in clust.kmeans)\nlearner$train(task)\nlearner$model\n\nK-means clustering with 2 clusters of sizes 21, 29\n\nCluster means:\n   Assault    Murder     Rape UrbanPop\n1 255.0000 11.857143 28.11429 67.61905\n2 109.7586  4.841379 16.24828 64.03448\n\nClustering vector:\n [1] 1 1 1 1 1 1 2 1 1 1 2 2 1 2 2 2 2 1 2 1 2 1 2 1 2 2 2 1 2 2 1 1 1 2 2 2 2 2\n[39] 2 1 2 1 1 2 2 2 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 41636.73 54762.30\n (between_SS / total_SS =  72.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# make \"predictions\" for the same data\nprediction = learner$predict(task)\nautoplot(prediction, task)\n\n\n\n\n\n8.5.2 Measures\nThe difference between supervised and unsupervised learning is that there is no ground truth data in unsupervised learning. In a supervised setting, such as classification, we would need to compare our predictions to true labels. Since clustering is an example of unsupervised learning, there are no true labels to which we can compare. However, we can still measure the quality of cluster assignments by quantifying how closely objects within the same cluster are related (cluster cohesion) as well as how distinct different clusters are from each other (cluster separation).\nThere are a few built-in evaluation metrics available to assess the quality of clustering. One of them is within sum of squares (WSS) which calculates the sum of squared differences between observations and centroids. WSS is useful because it quantifies cluster cohesion. The range of this measure is \\([0, \\infty)\\) where a smaller value means that clusters are more compact.\nAnother measure is silhouette quality index that quantifies how well each point belongs to its assigned cluster versus neighboring cluster. Silhouette values are in \\([-1, 1]\\) range.\nPoints with silhouette closer to:\n\n1 are well clustered\n0 lie between two clusters\n-1 likely placed in the wrong cluster\n\nThe following is an example of conducting a benchmark experiment with various learners on iris data set without target variable and assessing the quality of each learner with both within sum of squares and silhouette measures.\n\ndesign = benchmark_grid(\n  tasks = TaskClust$new(\"iris\", iris[-5]),\n  learners = list(\n    lrn(\"clust.kmeans\", centers = 3L),\n    lrn(\"clust.pam\", k = 2L),\n    lrn(\"clust.cmeans\", centers = 3L)),\n  resamplings = rsmp(\"insample\"))\nprint(design)\n\n              task                  learner               resampling\n1: <TaskClust[46]> <LearnerClustKMeans[38]> <ResamplingInsample[20]>\n2: <TaskClust[46]>    <LearnerClustPAM[38]> <ResamplingInsample[20]>\n3: <TaskClust[46]> <LearnerClustCMeans[38]> <ResamplingInsample[20]>\n\n# execute benchmark\nbmr = benchmark(design)\n\n# define measure\nmeasures = list(msr(\"clust.wss\"), msr(\"clust.silhouette\"))\nbmr$aggregate(measures)\n\n   nr      resample_result task_id   learner_id resampling_id iters clust.wss\n1:  1 <ResampleResult[21]>    iris clust.kmeans      insample     1  78.85144\n2:  2 <ResampleResult[21]>    iris    clust.pam      insample     1 153.32572\n3:  3 <ResampleResult[21]>    iris clust.cmeans      insample     1  79.02617\n1 variable not shown: [clust.silhouette]\n\n\nThe experiment shows that using k-means algorithm with three centers produces a better within sum of squares score than any other learner considered. However, pam (partitioning around medoids) learner with two clusters performs the best when considering silhouette measure which takes into the account both cluster cohesion and separation.\n\n8.5.3 Visualization\nCluster analysis in mlr3 is integrated with mlr3viz which provides a number of useful plots. Some of those plots are shown below.\n\ntask = TaskClust$new(\"iris\", iris[-5])\nlearner = lrn(\"clust.kmeans\")\nlearner$train(task)\nprediction = learner$predict(task)\n\n# performing PCA on task and showing assignments\nautoplot(prediction, task, type = \"pca\")\n\n\n\n# same as above but with probability ellipse that assumes normal distribution\nautoplot(prediction, task, type = \"pca\", frame = TRUE, frame.type = \"norm\")\n\n\n\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.hclust\")\nlearner$train(task)\n\n# dendrogram for hierarchical clustering\nautoplot(learner, task = task)\n\n\n\n\nSilhouette plots can help to visually assess the quality of the analysis and help choose a number of clusters for a given data set. The red dotted line shows the mean silhouette value and each bar represents a data point. If most points in each cluster have an index around or higher than mean silhouette, the number of clusters is chosen well.\n\n# silhouette plot allows to visually inspect the quality of clustering\ntask = TaskClust$new(\"iris\", iris[-5])\nlearner = lrn(\"clust.kmeans\")\nlearner$param_set$values = list(centers = 5L)\nlearner$train(task)\nprediction = learner$predict(task)\nautoplot(prediction, task, type = \"sil\")\n\n\n\n\nThe plot shows that all points in cluster 5 and almost all points in clusters 4, 2 and 1 are below average silhouette index. This means that a lot of observations lie either on the border of clusters or are likely assigned to the wrong cluster.\n\nlearner = lrn(\"clust.kmeans\")\nlearner$param_set$values = list(centers = 2L)\nlearner$train(task)\nprediction = learner$predict(task)\nautoplot(prediction, task, type = \"sil\")\n\n\n\n\nSetting the number of centers to two improves both the average silhouette score as well as the overall quality of clustering because almost all points in cluster 1 are higher than, and a lot of points in cluster 2 are close to the mean silhouette. Hence, having two centers might be a better choice for the number of clusters.\n\n\n\n\n\n\nBrenning, Alexander. 2012. “Spatial Cross-Validation and Bootstrap for the Assessment of Prediction Rules in Remote Sensing: The R Package Sperrorest.” In 2012 IEEE International Geoscience and Remote Sensing Symposium. IEEE. https://doi.org/10.1109/igarss.2012.6352393.\n\n\nCollett, David. 2014. Modelling Survival Data in Medical Research. 3rd ed. CRC.\n\n\nHastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Springer New York. https://doi.org/10.1007/978-0-387-21606-5.\n\n\nLegendre, Pierre. 1993. “Spatial Autocorrelation: Trouble or New Paradigm?” Ecology 74 (6): 1659–73. https://doi.org/10.2307/1939924.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. CRC Press.\n\n\nMeyer, Hanna, Christoph Reudenbach, Tomislav Hengl, Marwan Katurji, and Thomas Nauss. 2018. “Improving Performance of Spatio-Temporal Machine Learning Models Using Forward Feature Selection and Target-Oriented Validation.” Environmental Modelling & Software 101 (March): 1–9. https://doi.org/10.1016/j.envsoft.2017.12.001.\n\n\nMuenchow, J., A. Brenning, and M. Richter. 2012. “Geomorphic Process Rates of Landslides Along a Humidity Gradient in the Tropical Andes.” Geomorphology 139-140: 271–84. https://doi.org/https://doi.org/10.1016/j.geomorph.2011.10.029.\n\n\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and Alexander Brenning. 2019. “Hyperparameter Tuning and Performance Assessment of Statistical and Machine-Learning Algorithms Using Spatial Data.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002.\n\n\nSilverman, Bernard W. 1986. Density Estimation for Statistics and Data Analysis. Vol. 26. CRC press.\n\n\nSonabend, Raphael, Franz J Király, Andreas Bender, Bernd Bischl, and Michel Lang. 2021. “mlr3proba: An R Package for Machine Learning in Survival Analysis.” Bioinformatics, February. https://doi.org/10.1093/bioinformatics/btab039."
  },
  {
    "objectID": "technical.html#sec-parallelization",
    "href": "technical.html#sec-parallelization",
    "title": "9  Technical",
    "section": "\n9.1 Parallelization",
    "text": "9.1 Parallelization\nParallelization refers to running multiple jobs in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes. This process allows for significant savings in computing power.\nIn general, there are many possibilities to parallelize, depending on the hardware to run the computations: If you only have a single CPU with multiple cores, threads or forks are ways to utilize all cores. If you have multiple machines, they need a way to communicate and exchange information, e.g. via protocols like network sockets or the Message Passing Interface (MPI). We don’t want to delve too deep into such details here, but want to introduce some terminology:\n\nWe call the parallelization platform together with its implementation for R the parallelization backend. As many parallelization backends have a different API, we are using the future package as an additional abstraction layer. mlr3 just interfaces future while the user can control how the code is executed.\nThe R session or process which orchestrates the computational work is called main, and it starts computational jobs.\nThe R sessions, processes, forks or machines which receive the jobs, do the calculation and then send back the result are called workers.\n\nWe distinguish between implicit parallelism and explicit parallelism. For the former, no special directives are required to enable the parallelization, everything works fully automatically. For the latter, parallelization has to be manually configured. On the one hand, this gives you full control over the execution, but on the other hand, this poses a greater obstacle for non-experts.\n\n\n\n\n\n\nNote\n\n\n\nWe don’t cover parallelization on GPUs here. mlr3 only distributes the fitting of multiple learners, e.g., during resampling, benchmarking, or tuning. On this rather abstract level, GPU parallelization doesn’t work efficiently. Some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU while fitting a single model. We refer to the respective documentation of the learner’s implementation, e.g., here for xgboost.\n\n\n\n9.1.1 Implicit Parallelization\nWe talk about implicit parallelization in the context of mlr3, if mlr3 calls external code (i.e., code from foreign CRAN packages which implements a Learner) that itself runs in parallel. Note that this definition includes GPU acceleration.\nMany machine learning algorithms can parallelize their model fit using threading, e.g., the random forest implementation in ranger or the boosting implemented in xgboost. During threading, the implementation instructs some sequential parts of the code to be executed independently of the other parts in the same process.\nFor example, while fitting a decision tree, each split that divides the data into two disjoint partitions requires a search for the best cut point on all \\(p\\) features. So instead of iterating over all features sequentially, the search can be broken down into \\(p\\) threads, each searching for the best cut point on a single feature. These threads can easily be parallelized by the scheduler of the operating system, as there is no need for communication between the threads. After all threads have finished, the results are collected and merged before terminating the threads. I.e., for our example of the decision tree, (1) the \\(p\\) best cut points per feature are collected and then (2) aggregated to the single best cut point across all features by just iterating over the \\(p\\) results sequentially.\n\n\n\n\n\n\nWarning\n\n\n\nIt does not make practical sense to actually execute in parallel every operation that can be parallelized. Starting and terminating workers (here: threads) as well as possible communication between workers comes at a price in the form of additionally required runtime which is called (parallelization) overhead. The overhead must be related to the runtime of the sequential execution. If the sequential execution is comparably fast, enabling parallelization often just introduces additional complexity and slows down the execution.\n\n\nUnfortunately, threading conflicts with certain parallel backends used during explicit parallelization, causing the system to be overutilized in the best case and causing hangs or segfaults in the worst case. For this reason, we introduced the convention that implicit parallelization is turned off per default. Hyperparameters that control the number of threads are tagged with the label \"threads\". Currently, controlling the number of threads is possible for some learners and filters from the mlr3filters package:\n\nlibrary(\"mlr3learners\") # for the ranger learner\n\nlearner = lrn(\"classif.ranger\")\nlearner$param_set$ids(tags = \"threads\")\n\n[1] \"num.threads\"\n\n\nTo enable the parallelization for this learner, we provide the helper function set_threads() which\n\n# use 4 CPUs\nset_threads(learner, n = 4)\n\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=4\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n# auto-detect cores on the local machine\nset_threads(learner)\n\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=2\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\n\n\n\n\n\n\nDanger\n\n\n\nAutomatic detection of the number of CPUs is sometimes flaky, and utilizing all available cores is occasionally counterproductive as overburdening the system often has negative effects on the overall runtime. The function which determines the number of CPUs for mlr3 is implemented in parallelly::availableCores() and comes with reasonable heuristics for many setups. See this blog post for some background information about the heuristic. However, there are still some scenarios where it is better to reduce the number of utilized CPUs manually:\n\nYou want to simultaneously work on the same system, e.g., browse the web or watch a video.\nYou are on a multi-user system and want to spare some resources for other users.\nYou have energy-efficient CPU cores, for example, the “Icestorm” cores on a Mac M1 chip. These are comparably slower than the high-performance “Firestorm” cores and not well suited for heavy computations.\nYou have linked R to a threaded BLAS implementation like OpenBLAS, and your learners make heavy use of linear algebra.\n\nYou can manually set the number of CPUs to overrule the heuristic via option \"mc.cores\":\n\noptions(mc.cores = 4)\n\nWe recommend setting this in your system’s .Rprofile file, c.f. Startup.\n\n\n\n9.1.2 Explicit Parallelization\nHere, we talk about explicit parallelization if mlr3 starts and controls the parallelization itself. For this purpose, an additional abstraction layer is used to be able to operate on a unified interface for a broad range of parallel backends: the future package. There are two operations where mlr3 calls the future package: while performing resampling via resample() and while benchmarking via benchmark(). During resampling, because all resampling iterations are independent of each other, all iterations can be executed in parallel. The same holds for benchmarking, where additionally to the independent model fits of a single resampling, all combinations in the provided design are also independent. These iterations are performed by future using the parallel backend configured with future::plan(). Extension packages like mlr3tuning internally call benchmark() during tuning and thus work in parallel, too.\n\n\n\n\n\n\nTip\n\n\n\nWhen computational problems are so easy to parallelize, they are often referred to as “embarrassingly parallel”.\nWhenever you loop over elements with a map-like function (e.g., lapply(), sapply(), mapply(), vapply() or a function from package purrr), you are facing an embarrassingly parallel problem. Such problems are straightforward to parallelize with R, e.g., with the furrr package providing map-like functions executed in parallel via the future framework. The same holds for for-loops with independent iterations, i.e., loops where the current iteration does not rely on the results of previous iterations.\n\n\nIn this section, we will use the spam task and a simple classification tree to showcase the explicit parallelization. We use the future::multisession parallel backend that should work on all systems.\n\n# select the multisession backend to use\nfuture::plan(\"multisession\")\n\n# define objects to perform a resampling\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\n\ntime = proc.time()[3]\nresample(task, learner, resampling)\ndiff = proc.time()[3] - time\n\nBy default, all CPUs of your machine are used unless you specify the argument workers in future::plan() (possible problems with this default have already been discussed for implicit parallelization). You should see a decrease in the reported elapsed time, but in practice, you cannot expect the runtime to fall linearly as the number of cores increases (Amdahl’s law). In contrast to threads, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers is quite large for the multisession backend. Therefore, it is advised to only consider parallelization for resamplings where each iteration runs at least several seconds.\nFigure 9.1 illustrates the parallelization from the above example. From left to right:\n\nThe main process calls the resample() function.\nThe task is split into 3 folds.\nThe folds are passed to three workers, each fitting a model on the respective subset of the task and predicting on the left-out observations.\nThe predictions (and trained models) are communicated back to main process which combines them into a ResampleResult.\n\n\n\n\n\n\ngraph LR\n    M[fa:fa-server Main]\n    S{\"resample()\"}\n    C{ResampleResult}\n\n    M --> S\n    S -->|Fold 1| W1[fa:fa-microchip Worker 1]\n    S -->|Fold 2| W2[fa:fa-microchip Worker 2]\n    S -->|Fold 3| W3[fa:fa-microchip Worker 3]\n    W1 -->|Prediction 1| C\n    W2 -->|Prediction 2| C\n    W3 -->|Prediction 3| C\n\n\n\n\n\nFigure 9.1: Parallelization of a resampling using a 3-fold cross-validation\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are transitioning from mlr, you might be used to selecting different parallelization levels, e.g., for resampling, benchmarking, or tuning. In mlr3, this is no longer required (except for nested resampling, briefly described in the following section). All kind of experiments are rolled out on the same level. Therefore, there is no need to decide whether you want to parallelize the tuning OR the resampling.\nJust lean back and let the machine do the work :-)\n\n\n\n9.1.3 Reproducibility\nUsually reproducibility is a major concern during parallelization as special pseudorandom number generators (PRNGs) are required. Luckily, this problem is already solved for us by the excellent future package mlr3 calls under the hood. future ensures that all workers will receive the exactly same PRNG streams. Although this alone does not guarantee full reproducibility, it is one problem less to worry about.\nYou can find more details about the used pseudo RNG in this blog post.\n\n9.1.4 Nested Resampling Parallelization\nNested resampling results in two nested resampling loops, and the user can choose which of them should be parallelized. Let’s consider the following example: You want to tune the minsplit argument of a classification tree using the AutoTuner of mlr3tuning (simplified version taken from the nested resampling section):\n\nlibrary(\"mlr3tuning\")\n\nLoading required package: paradox\n\nlearner = lrn(\"classif.rpart\",\n  minsplit  = to_tune(2, 128, logscale = TRUE)\n)\n\nat = auto_tuner(\n  method = tnr(\"random_search\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 2), # inner CV\n  measure = msr(\"classif.ce\"),\n  term_evals = 20,\n)\n\nTo evaluate the performance on an independent test set, resampling is used:\n\nresample(\n  task = tsk(\"penguins\"),\n  learner = at,\n  resampling = rsmp(\"cv\", folds = 5) # outer CV\n)\n\n<ResampleResult> of 5 iterations\n* Task: penguins\n* Learner: classif.rpart.tuned\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\nHere, we have two opportunities to tune: the inner cross-validation of the auto tuner with 2 folds, or the outer cross-validation of the resampling with 5 folds. Let’s say that we have a single CPU with four cores available.\nIf we opt to parallelize the outer CV, all four cores would be utilized first with the computation of the first 4 resampling iterations. The computation of the fifth iteration has to wait, i.e., depending on the parallelization backend and its scheduling strategy,\n\nuntil all four iterations have been finished, and the results have been collectively reported back to the main process, or\neither one of the four cores has terminated – the first core reporting back will get a new task as soon as possible.\n\n\n\n\n\n\n\nNote\n\n\n\nThe former method usually comes with less synchronization overhead and is best suited for short jobs with homogeneous runtimes. The latter yields better runtimes if the runtimes are heterogeneous, especially if the parallelization overhead is neglectable in comparison with the runtime for the computation. E.g., for parallel::mclapply(), the behavior of the scheduler can be controlled with the mc.preschedule option. For many backends, you cannot control the scheduling. However, future allows you to first chunk jobs together which combines multiple tasks into blocks that run sequentially on a worker, avoiding the intermediate synchronization steps.\n\n\nThe resulting CPU utilization of the nested resampling example on 4 CPUs is visualized in two Figures:\n\n\nFigure 9.2 as an example for parallelizing the outer 5-fold cross-validation.\n\n# Runs the outer loop in parallel and the inner loop sequentially\nfuture::plan(list(\"multisession\", \"sequential\"))\n\nWe assume that each fit during the inner resampling takes 4 seconds to compute and that there is no other significant overhead. First, each of the four workers starts with the computation of an inner 2-fold cross-validation. As there are more jobs than workers, the remaining fifth iteration of the outer resampling is queued on CPU1 after the first 4 iterations are finished after 8 secs. During the computation of the 5th outer resampling iteration, only CPU1 is utilized, the other 3 CPUs are idling.\n\n\nFigure 9.3 as an example for parallelizing the inner 2-fold cross-validation.\n\n# Runs the outer loop sequentially and the inner loop in parallel\nfuture::plan(list(\"sequential\", \"multisession\"))\n\nHere, the outer loop runs sequentially and distributes the 2 computations for the inner resampling on 2 CPUs. Meanwhile, CPU3 and CPU4 are idling.\n\n\n\n\n\n\n\ngantt\n    title CPU Utilization\n    dateFormat  s\n    axisFormat %S\n    section CPU1\n    Iteration 1-1           :0, 4s\n    Iteration 1-2           :4, 4s\n    Iteration 5-1           :8, 4s\n    Iteration 5-2           :12, 4s\n\n    section CPU2\n    Iteration 2-1           :0, 4s\n    Iteration 2-2           :4, 4s\n    Idle                    :crit, 8, 8s\n\n    section CPU3\n    Iteration 3-1           :0, 4s\n    Iteration 3-2           :4, 4s\n    Idle                    :crit, 8, 8s\n\n    Iteration 4-1           :0, 4s\n    section CPU4\n    Iteration 4-2           :4, 4s\n    Idle                    :crit, 8, 8s\n\n\n\n\n\nFigure 9.2: CPU utilization while parallelizing the outer resampling of a 2-fold cross-validation nested inside a 5-fold cross-validation on 4 CPUs.\n\n\n\n\n\n\n\n\n\ngantt\n    title CPU Utilization\n    dateFormat  s\n    axisFormat %S\n    section CPU1\n    Iteration 1-1           :0, 4s\n    Iteration 2-1           :4, 4s\n    Iteration 3-1           :8, 4s\n    Iteration 4-1           :12, 4s\n    Iteration 5-1           :16, 4s\n\n    section CPU2\n    Iteration 1-2           :0, 4s\n    Iteration 2-2           :4, 4s\n    Iteration 3-2           :8, 4s\n    Iteration 4-2           :12, 4s\n    Iteration 5-2           :16, 4s\n\n    section CPU3\n    Idle                    :crit, 0, 20s\n\n    section CPU4\n    Idle                    :crit, 0, 20s\n\n\n\n\n\nFigure 9.3: CPU utilization while parallelizing the inner resampling of a 2-fold cross-validation nested inside a 5-fold cross-validation on 4 CPUs.\n\n\n\n\nBoth possibilities for parallelization are not exploiting the full potential of the 4 CPUs. With parallelization of the outer loop, all results are computed after 16s, in contrast to parallelization of the inner loop where the results are only available after 20s.\nIf possible, the number of iterations can be adapted to the available hardware. There is no law set in stone that you have to do, e.g., 10 folds in cross-validation. If you have 4 CPUs and a reasonable variance, 8 iterations are often sufficient, or you do 12 iterations because you get the last two iterations basically for free.\nAlternatively, you can also enable parallelization for both loops for nested parallelization, even on different parallelization backends. While nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups. In this case, the number of workers must be manually tweaked so that the system does not get overburdened:\n\n# Runs both loops in parallel\nfuture::plan(list(\n  future::tweak(\"multisession\", workers = 2),\n  future::tweak(\"multisession\", workers = 4)\n))\n\nThis example would run on 8 cores (= 2 * 4) on the local machine. The vignette of the future package gives more insight into nested parallelization. For more background information about parallelization during tuning, see Section 6.7 of Bischl et al. (2021).\n\n\n\n\n\n\nDanger\n\n\n\nDuring tuning with mlr3tuning, you can often adjust the batch size of the Tuner, i.e., control how many hyperparameter configurations are evaluated in parallel. If you want full parallelization, make sure that the batch size multiplied by the number of (inner) resampling iterations is at least equal to the number of available workers. If you expect homogeneous runtimes, i.e., you are tuning over a single learner or linear pipeline and you have no hyperparameter which is likely to influence the performance, aim for a multiple of the number of workers.\nIn general, larger batches mean more parallelization, while smaller batches imply a more frequent evaluation of termination criteria. We default to a batch_size of 1 that ensures that all Terminators work as intended, i.e., you cannot exceed the computational budget."
  },
  {
    "objectID": "technical.html#sec-error-handling",
    "href": "technical.html#sec-error-handling",
    "title": "9  Technical",
    "section": "\n9.2 Error Handling",
    "text": "9.2 Error Handling\nIn ML, it is not uncommon for something to break. This is because the algorithms have to process arbitrary data, and not all eventualities can always be handled. While we try to identify obvious problems before execution, such as when missing values occur, but a learner can’t handle them, other problems are far more complex to detect. Examples include correlations or collinearity that make model fitting impossible, outliers that lead to numerical problems, or new levels of categorical variables emerging in the predict step. The learners behave quite differently when encountering such problems: some models signal a warning during the train step that they failed to fit but return a baseline model while other models stop the execution. During prediction, some learners just refuse to predict the response for observations they cannot handle while others predict a missing value. How to deal with these problems even in more complex setups like benchmarking or tuning is the topic of this section.\nFor illustration (and internal testing) of error handling, mlr3 ships with the learners classif.debug and regr.debug. Here, we will concentrate on the debug learner for classification:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.debug\")\nprint(learner)\n\n<LearnerClassifDebug:classif.debug>: Debug Learner for Classification\n* Model: -\n* Parameters: list()\n* Packages: mlr3\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_forward, missings, multiclass, twoclass\n\n\nThis learner comes with special hyperparameters that let us simulate problems frequently encountered in ML. E.g., the debug learner comes with hyperparameters to control\n\nwhat conditions should be signaled (message, warning, error, segfault) with what probability,\nduring which stage the conditions should be signaled (train or predict), and\nthe ratio of predictions being NA (predict_missing).\n\n\nlearner$param_set\n\n<ParamSet>\n                      id    class lower upper nlevels        default value\n 1:        error_predict ParamDbl     0     1     Inf              0      \n 2:          error_train ParamDbl     0     1     Inf              0      \n 3:      message_predict ParamDbl     0     1     Inf              0      \n 4:        message_train ParamDbl     0     1     Inf              0      \n 5:      predict_missing ParamDbl     0     1     Inf              0      \n 6: predict_missing_type ParamFct    NA    NA       2             na      \n 7:           save_tasks ParamLgl    NA    NA       2          FALSE      \n 8:     segfault_predict ParamDbl     0     1     Inf              0      \n 9:       segfault_train ParamDbl     0     1     Inf              0      \n10:          sleep_train ParamUty    NA    NA     Inf <NoDefault[3]>      \n11:        sleep_predict ParamUty    NA    NA     Inf <NoDefault[3]>      \n12:              threads ParamInt     1   Inf     Inf <NoDefault[3]>      \n13:      warning_predict ParamDbl     0     1     Inf              0      \n14:        warning_train ParamDbl     0     1     Inf              0      \n15:                    x ParamDbl     0     1     Inf <NoDefault[3]>      \n16:                 iter ParamInt     1   Inf     Inf              1      \n\n\nWith the learner’s default settings, the learner will do nothing special: The learner remembers a random label and constantly predicts this label:\n\ntask = tsk(\"penguins\")\nlearner$train(task)$predict(task)$confusion\n\n           truth\nresponse    Adelie Chinstrap Gentoo\n  Adelie         0         0      0\n  Chinstrap      0         0      0\n  Gentoo       152        68    124\n\n\nWe now set a hyperparameter to let the debug learner signal an error during the train step. By default, mlr3 does not catch conditions such as warnings or errors raised while calling learners:\n\n# set probability to signal an error to 1\nlearner$param_set$values = list(error_train = 1)\n\nlearner$train(tsk(\"penguins\"))\n\nError in .__LearnerClassifDebug__.train(self = self, private = private, : Error from classif.debug->train()\n\n\nIf this has been a regular learner, we could now start debugging with traceback() (or create a Minimal Reproducible Example (MRE) to file a bug report upstream).\n\n\n\n\n\n\nNote\n\n\n\nIf you start debugging, make sure you have disabled parallelization to avoid various pitfalls related to parallelization. It may also be helpful to set the option mlr3.debug to TRUE. If this flag is set, mlr3 does not call into the future package, resulting in an easier-to-interpret program flow and traceback().\n\n\n\n9.2.1 Encapsulation\nSince ML algorithms are confronted with arbitrary, often messy data, errors are not uncommon here, and we often just need to move on during benchmarking or tuning. Thus, we need a mechanism to\n\ncapture all signaled conditions such as messages, warnings and errors so that we can analyze them post-hoc (called “encapsulation”, covered in this section), and\na statistically sound way to proceed while being able to aggregate over partial results (next Section 9.2.2).\n\nEncapsulation ensures that signaled conditions (such as messages, warnings and errors) are intercepted: all conditions raised during the training or predict step are logged into the learner, and errors do not interrupt the program flow. I.e., the execution of the calling function or package (here: mlr3) continues as if there had been no error, though the result (fitted model during train(), predictions during predict()) are missing. Each Learner has a field encapsulate to control how the train or predict steps are wrapped. The easiest way to encapsulate the execution is provided by the package evaluate which evaluates R expressions while tracking conditions such as outputs, messages, warnings or errors (see the documentation of the encapsulate() helper function for more details):\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.debug\")\n\n# this learner throws a warning and then stops with an error during train()\nlearner$param_set$values = list(warning_train = 1, error_train = 1)\n\n# enable encapsulation for train() and predict()\nlearner$encapsulate = c(train = \"evaluate\", predict = \"evaluate\")\n\nlearner$train(task)\n\nAfter training the learner, one can access the recorded log via the fields log, warnings and errors:\n\nlearner$log\n\n   stage   class                                 msg\n1: train warning Warning from classif.debug->train()\n2: train   error   Error from classif.debug->train()\n\nlearner$warnings\n\n[1] \"Warning from classif.debug->train()\"\n\nlearner$errors\n\n[1] \"Error from classif.debug->train()\"\n\n\nAnother method for encapsulation is implemented in the callr package. In contrast to evaluate, the computation is taken out in a separate R process. This guards the calling session against segfaults which otherwise would tear down the complete R session. On the downside, starting new processes comes with comparably more computational overhead.\n\nlearner$encapsulate = c(train = \"callr\", predict = \"callr\")\nlearner$param_set$values = list(segfault_train = 1)\nlearner$train(task = task)\nlearner$errors\n\n[1] \"callr process exited with status -11\"\n\n\nWith either of these encapsulation methods, we can now catch errors and post-hoc analyze the messages, warnings and error messages. Unfortunately, this is only half the battle. Without a model, it is not possible to get predictions:\n\nlearner$predict(task)\n\nError: Cannot predict, Learner 'classif.debug' has not been trained yet\n\n\nTo handle the missing predictions gracefully during resample(), benchmark() or tuning, fallback learners are introduced next.\n\n9.2.2 Fallback learners\nFallback learners have the purpose of allowing scoring results in cases where a Learner failed to fit a model, refuses to provide predictions for all observations or predicts missing values.\nWe will first handle the case that a learner fails to fit a model during training, e.g., if some convergence criterion is not met or the learner ran out of memory. There are in general three possibilities to proceed:\n\nIgnore missing scores. Although this is arguably the most frequent approach in practice, it is not statistically sound. For example, consider the case where a researcher wants a specific learner to look better in a benchmark study. To do this, the researcher takes an existing learner but introduces a small adaptation: If an internal goodness-of-fit measure is not achieved, an error is thrown. In other words, the learner only fits a model if the model can be reasonably well learned on the given training data. In comparison with the learning procedure without this adaptation and a good threshold, however, we now compare the mean over only the “easy” splits with the mean over all splits - an unfair advantage.\nPenalize failing learners. If a score is missing, we can simply impute the worst possible score (as defined by the Measure) and thereby heavily penalize the learner for failing. However, this often seems too harsh for many problems, and for some measures there is no reasonable value to impute.\nImpute a value that corresponds to a (weak) baseline. Instead of imputing with the worst possible score, impute with a reasonable baseline, e.g., by just predicting the majority class or the mean of the response in the training data. Such simple baselines are implemented as featureless learners (mlr_learners_classif.featureless or mlr_learners_regr.featureless). Note that a reasonable baseline value is different in different training splits. Retrieving these values after a larger benchmark study has been conducted is possible, but tedious.\n\nWe strongly recommend option (3): it is statistically sound and very flexible. To make this procedure very convenient during resampling and benchmarking, we support fitting a proper baseline with a fallback learner. In the next example, in addition to the debug learner, we attach a simple featureless learner to the debug learner. So whenever the debug learner fails (which is every single time with the given parametrization) and encapsulation is enabled, mlr3 falls back to the predictions of the featureless learner internally:\n\ntask = tsk(\"penguins\")\n\nlearner = lrn(\"classif.debug\")\nlearner$param_set$values = list(error_train = 1)\nlearner$fallback = lrn(\"classif.featureless\")\n\nlearner$train(task)\nlearner\n\n<LearnerClassifDebug:classif.debug>: Debug Learner for Classification\n* Model: -\n* Parameters: error_train=1\n* Packages: mlr3\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_forward, missings, multiclass, twoclass\n* Errors: Error from classif.debug->train()\n\n\nNote that we don’t have to enable encapsulation explicitly; it is automatically set to \"evaluate\" for the training and the predict step while setting a fallback learner for a learner without encapsulation enabled. Furthermore, the log contains the captured error (which is also included in the print output), and although we don’t have a model, we can still get predictions:\n\nlearner$model\n\nNULL\n\nprediction = learner$predict(task)\nprediction$score()\n\nclassif.ce \n 0.5581395 \n\n\nIn this stepwise train-predict procedure, the fallback learner is of limited use. However, it is invaluable for larger benchmark studies.\nIn the following snippet, we compare the previously created debug learner with a simple classification tree. We re-parametrize the debug learner to fail in roughly 30% of the resampling iterations during the training step:\n\nlearner$param_set$values = list(error_train = 0.3)\n\nbmr = benchmark(benchmark_grid(tsk(\"penguins\"), list(learner, lrn(\"classif.rpart\")), rsmp(\"cv\")))\naggr = bmr$aggregate(conditions = TRUE)\naggr[, .(learner_id, warnings, errors, classif.ce)]\n\n      learner_id warnings errors classif.ce\n1: classif.debug        0      8 0.56915966\n2: classif.rpart        0      0 0.06403361\n\n\nEven though the debug learner occasionally failed to provide predictions, we still got a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree. It is also possible to split the benchmark up into separate ResampleResult objects which sometimes helps to get more context. E.g., if we only want to have a closer look into the debug learner, we can extract the errors from the corresponding resample results:\n\nrr = aggr[learner_id == \"classif.debug\"]$resample_result[[1L]]\nrr$errors\n\n   iteration                               msg\n1:         1 Error from classif.debug->train()\n2:         4 Error from classif.debug->train()\n3:         5 Error from classif.debug->train()\n4:         6 Error from classif.debug->train()\n5:         7 Error from classif.debug->train()\n6:         8 Error from classif.debug->train()\n7:         9 Error from classif.debug->train()\n8:        10 Error from classif.debug->train()\n\n\nA similar problem to failed model fits emerges when a learner predicts only a subset of the observations in the test set (and predicts NA or no value for others). A typical case is, e.g., when new and unseen factor levels are encountered in the test data. Imagine again that our goal is to benchmark two algorithms using cross-validation on some binary classification task:\n\nAlgorithm A is an ordinary logistic regression.\nAlgorithm B is also an ordinary logistic regression, but with a twist: If the logistic regression is rather certain about the predicted label (> 90% probability), it returns the label and returns a missing value otherwise.\n\nClearly, at its core, this is the same problem as outlined before. Algorithm B would easily outperform algorithm A, but you have not factored in that you can not generate predictions for all observations. Long story short, if a fallback learner is involved, missing predictions of the base learner will be automatically replaced with predictions from the fallback learner. This is illustrated in the following example:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.debug\")\n\n# this hyperparameter sets the ratio of missing predictions\nlearner$param_set$values = list(predict_missing = 0.5)\n\n# without fallback\np = learner$train(task)$predict(task)\ntable(p$response, useNA = \"always\")\n\n\n   Adelie Chinstrap    Gentoo      <NA> \n      172         0         0       172 \n\n# with fallback\nlearner$fallback = lrn(\"classif.featureless\")\np = learner$train(task)$predict(task)\ntable(p$response, useNA = \"always\")\n\n\n   Adelie Chinstrap    Gentoo      <NA> \n      172         0       172         0 \n\n\nSummed up, by combining encapsulation and fallback learners, it is possible to benchmark even quite unreliable or unstable learning algorithms in a convenient and statistically sound fashion.\n\n9.2.3 Actionable Errors\nAll problems demonstrated so far are artificial and non-actionable. The usefulness of encapsulation and error logging usually only really becomes apparent in large benchmarks, especially in combination with parallelization. For a fair comparison, you need to distinguish between the following cases:\n\nYou have made a mistake, e.g., forgot a required preprocessing step in your pipeline. Action: Fix problems, restart computation.\nTemporary problems related to the executing system, e.g., network hiccups. Action: Restart computation.\nIntrinsic, deterministic and reproducible problem with the model fitting. Action: Impute with fallback learner.\n\nThe package mlr3batchmark provides functionality to map jobs of a benchmark to computational jobs for the package batchtools. This provides a convenient way get fine-grained control over the execution of each single resampling iteration and then combine the results afterwards to a BenchmarkResult again to proceed with the analysis."
  },
  {
    "objectID": "technical.html#sec-backends",
    "href": "technical.html#sec-backends",
    "title": "9  Technical",
    "section": "\n9.3 Data Backends",
    "text": "9.3 Data Backends\nIn mlr3, Tasks store their data in an abstract data object, the DataBackend. A backend provides a unified API to retrieve subsets of the data or query information about it, regardless of how the data is actually stored. The default backend uses data.table via the DataBackendDataTable as a very fast and efficient in-memory database. For example, we can query some information of the mlr_tasks_penguins task:\n\ntask = tsk(\"penguins\")\nbackend = task$backend\nbackend$nrow\n\n[1] 344\n\nbackend$ncol\n\n[1] 9\n\n\nFor bigger data, or when working with many tasks simultaneously in the same R session, it can be necessary to interface out-of-memory data to reduce the memory requirements. This way, only the part of the data which is currently required by the learners will be placed in the main memory to operate on. There are multiple options to archive this:\n\n\nDataBackendDplyr which interfaces the R package dbplyr, extending dplyr to work on many popular databases like MariaDB, PostgreSQL or SQLite.\n\nDataBackendDuckDB for the impressive DuckDB database connected via duckdb: a fast, zero-configuration alternative to SQLite.\n\nDataBackendDuckDB, again, but for Parquet files. The data does not need to be converted to DuckDB’s native storage format, you can work directly on directories containing one or multiple files stored in the popular Parquet format.\n\n\n9.3.1 Databases with DataBackendDplyr\nTo demonstrate the DataBackendDplyr we use the NYC flights data set from the nycflights13 package and move it into a SQLite database. Although as_sqlite_backend() provides a convenient function to perform this step, we construct the database manually here.\n\n# load data\nrequireNamespace(\"DBI\")\nrequireNamespace(\"RSQLite\")\nrequireNamespace(\"nycflights13\")\ndata(\"flights\", package = \"nycflights13\")\nstr(flights)\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n# add column of unique row ids\nflights$row_id = 1:nrow(flights)\n\n# create sqlite database in temporary file\npath = tempfile(\"flights\", fileext = \".sqlite\")\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\ntbl = DBI::dbWriteTable(con, \"flights\", as.data.frame(flights))\nDBI::dbDisconnect(con)\n\n# remove in-memory data\nrm(flights)\n\nWith the SQLite database stored in file path, we now re-establish a connection and switch to dplyr/dbplyr for some essential preprocessing.\n\n# establish connection\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\n\n# select the \"flights\" table, enter dplyr\nlibrary(\"dplyr\")\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(\"dbplyr\")\n\n\nAttaching package: 'dbplyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\ntbl = tbl(con, \"flights\")\n\nFirst, we select a subset of columns to work on:\n\nkeep = c(\"row_id\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"dep_time\",\n  \"arr_time\", \"carrier\", \"flight\", \"air_time\", \"distance\", \"arr_delay\")\ntbl = select(tbl, all_of(keep))\n\nAdditionally, we remove those observations where the arrival delay (arr_delay) has a missing value:\n\ntbl = filter(tbl, !is.na(arr_delay))\n\nTo keep runtime reasonable for this toy example, we filter the data to only use every second row:\n\ntbl = filter(tbl, row_id %% 2 == 0)\n\nThe factor levels of the feature carrier are merged so that infrequent carriers are replaced by level “other”:\n\ntbl = mutate(tbl, carrier = case_when(\n  carrier %in% c(\"OO\", \"HA\", \"YV\", \"F9\", \"AS\", \"FL\", \"VX\", \"WN\") ~ \"other\",\n  TRUE ~ carrier))\n\nNext, the processed table is used to create a mlr3db::DataBackendDplyr from mlr3db:\n\nlibrary(\"mlr3db\")\nb = as_data_backend(tbl, primary_key = \"row_id\")\n\nWe can now use the interface of DataBackend to query some basic information about the data:\n\nb$nrow\n\n[1] 163707\n\nb$ncol\n\n[1] 13\n\nb$head()\n\n   row_id year month day hour minute dep_time arr_time carrier flight air_time\n1:      2 2013     1   1    5     29      533      850      UA   1714      227\n2:      4 2013     1   1    5     45      544     1004      B6    725      183\n3:      6 2013     1   1    5     58      554      740      UA   1696      150\n4:      8 2013     1   1    6      0      557      709      EV   5708       53\n5:     10 2013     1   1    6      0      558      753      AA    301      138\n6:     12 2013     1   1    6      0      558      853      B6     71      158\n2 variables not shown: [distance, arr_delay]\n\n\nNote that the DataBackendDplyr does not know about any rows or columns we have filtered out with dplyr before, it just operates on the view we provided.\nAs we now have constructed a backend, we can switch over to mlr3 for model fitting and create the following mlr3 objects:\n\nA regression task, based on the previously created mlr3db::DataBackendDplyr.\nA regression learner (regr.rpart).\nA resampling strategy: 3 times repeated subsampling using 2% of the observations for training (“subsampling”)\nMeasures “mse”, “time_train” and “time_predict”\n\n\ntask = as_task_regr(b, id = \"flights_sqlite\", target = \"arr_delay\")\nlearner = lrn(\"regr.rpart\")\nmeasures = mlr_measures$mget(c(\"regr.mse\", \"time_train\", \"time_predict\"))\nresampling = rsmp(\"subsampling\", repeats = 3, ratio = 0.02)\n\nWe pass all these objects to resample() to perform a simple resampling with three iterations. In each iteration, only the required subset of the data is queried from the SQLite database and passed to rpart::rpart():\n\nrr = resample(task, learner, resampling)\nprint(rr)\n\n<ResampleResult> of 3 iterations\n* Task: flights_sqlite\n* Learner: regr.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\nrr$aggregate(measures)\n\n    regr.mse   time_train time_predict \n 1246.076751     0.295000     2.017333 \n\n\nNote that we still have an active connection to the database. To properly close it, we remove the tbl object referencing the connection and then close the connection.\n\nrm(tbl)\nDBI::dbDisconnect(con)\n\n\n9.3.2 Parquet Files with DataBackendDuckDB\nWhile storing the Task’s data in memory is most efficient w.r.t. accessing it for model fitting, this has two major disadvantages:\n\nAlthough you might only need a small proportion of the data, the complete data frame sits in memory and consumes memory. This is especially a problem if you work with many tasks simultaneously.\nDuring parallelization, the complete data needs to be transferred to the workers which can cause significant overhead.\n\nA very simple way to avoid this is given by just converting the DataBackendDataTable to a DataBackendDuckDB. As we have already demonstrated how to operate on a SQLite database, and DuckDB is not different in that regard. To convert a data.frame to DuckDB, we provide the helper function as_duckdb_backend(). Only two arguments are required: the data.frame to convert, and a path to store the data.\nWhile this is useful while working with many tasks simultaneously in order to keep the memory requirements reasonable, the more frequent use case for DuckDB are nowadays Parquet files. Parquet is a popular column-oriented data storage format supporting efficient compression, making it far superior to other popular data exchange formats such as CSV.\nTo demonstrate working with Parquet files, we first query the location of an example data set shipped with mlr3db:\n\npath = system.file(file.path(\"extdata\", \"spam.parquet\"), package = \"mlr3db\")\n\nWe can then create a DataBackendDuckDB based on this file and convert the backend to a classification task, all without loading the dataset into memory:\n\nbackend = as_duckdb_backend(path)\ntask = as_task_classif(backend, target = \"type\")\nprint(task)\n\n<TaskClassif:backend> (4601 x 58)\n* Target: type\n* Properties: twoclass\n* Features (57):\n  - dbl (57): address, addresses, all, business, capitalAve,\n    capitalLong, capitalTotal, charDollar, charExclamation, charHash,\n    charRoundbracket, charSemicolon, charSquarebracket, conference,\n    credit, cs, data, direct, edu, email, font, free, george, hp, hpl,\n    internet, lab, labs, mail, make, meeting, money, num000, num1999,\n    num3d, num415, num650, num85, num857, order, original, our, over,\n    parts, people, pm, project, re, receive, remove, report, table,\n    technology, telnet, will, you, your\n\n\nAccessing the data internally triggers a query and data is fetched to be stored in an in-memory data.frame, but only the required subsets."
  },
  {
    "objectID": "technical.html#sec-paradox",
    "href": "technical.html#sec-paradox",
    "title": "9  Technical",
    "section": "\n9.4 Parameters (using paradox)",
    "text": "9.4 Parameters (using paradox)\n\n\n\n\n\n\nNote\n\n\n\nWe are currently revising the book. This section contains a lot of legacy code. You can find the new paradox syntax in the Defining a Tuning Spaces section.\n\n\nThe paradox package offers a language for the description of parameter spaces, as well as tools for useful operations on these parameter spaces. A parameter space is often useful when describing:\n\nA set of sensible input values for an R function\nThe set of possible values that slots of a configuration object can take\nThe search space of an optimization process\n\nThe tools provided by paradox therefore relate to:\n\n\nParameter checking: Verifying that a set of parameters satisfies the conditions of a parameter space\n\nParameter sampling: Generating parameter values that lie in the parameter space for systematic exploration of program behavior depending on these parameters\n\nparadox is, by nature, an auxiliary package that derives its usefulness from other packages that make use of it. It is heavily utilized in other mlr-org packages such as mlr3, mlr3pipelines, and mlr3tuning.\n\n9.4.1 Reference Based Objects\nparadox is the spiritual successor to the ParamHelpers package and was written from scratch using the R6 class system. The most important consequence of this is that all objects created in paradox are “reference-based”, unlike most other objects in R. When a change is made to a ParamSet object, for example by adding a parameter using the $add() function, all variables that point to this ParamSet will contain the changed object. To create an independent copy of a ParamSet, the $clone() method needs to be used:\n\nlibrary(\"paradox\")\n\nps = ParamSet$new()\nps2 = ps\nps3 = ps$clone(deep = TRUE)\nprint(ps) # the same for ps2 and ps3\n\n<ParamSet>\nEmpty.\n\n\n\nps$add(ParamLgl$new(\"a\"))\n\n\nprint(ps)  # ps was changed\n\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  a ParamLgl    NA    NA       2 <NoDefault[3]>      \n\nprint(ps2) # contains the same reference as ps\n\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  a ParamLgl    NA    NA       2 <NoDefault[3]>      \n\nprint(ps3) # is a \"clone\" of the old (empty) ps\n\n<ParamSet>\nEmpty.\n\n\n\n9.4.2 Defining a Parameter Space\n\n9.4.2.1 Single Parameters\nThe basic building block for describing parameter spaces is the Param class. It represents a single parameter, which usually can take a single atomic value. Consider, for example, trying to configure the rpart package’s rpart.control object. It has various components (minsplit, cp, …) that all take a single value, and that would all be represented by a different instance of a Param object.\nThe Param class has various subclasses that represent different value types:\n\n\nParamInt: Integer numbers\n\nParamDbl: Real numbers\n\nParamFct: String values from a set of possible values, similar to R factors\n\nParamLgl: Truth values (TRUE / FALSE), as logicals in R\n\nParamUty: Parameter that can take any value\n\nA particular instance of a parameter is created by calling the attached $new() function:\n\nlibrary(\"paradox\")\nparA = ParamLgl$new(id = \"A\")\nparB = ParamInt$new(id = \"B\", lower = 0, upper = 10, tags = c(\"tag1\", \"tag2\"))\nparC = ParamDbl$new(id = \"C\", lower = 0, upper = 4, special_vals = list(NULL))\nparD = ParamFct$new(id = \"D\", levels = c(\"x\", \"y\", \"z\"), default = \"y\")\nparE = ParamUty$new(id = \"E\", custom_check = function(x) checkmate::checkFunction(x))\n\nEvery parameter must have:\n\n\nid - A name for the parameter within the parameter set\n\ndefault - A default value\n\nspecial_vals - A list of values that are accepted even if they do not conform to the type\n\ntags - Tags that can be used to organize parameters\n\nThe numeric (Int and Dbl) parameters furthermore allow for specification of a lower and upper bound. Meanwhile, the Fct parameter must be given a vector of levels that define the possible states its parameter can take. The Uty parameter can also have a custom_check function that must return TRUE when a value is acceptable and may return a character(1) error description otherwise. The example above defines parE as a parameter that only accepts functions.\nAll values which are given to the constructor are then accessible from the object for inspection using $. Although all these values can be changed for a parameter after construction, this can be a bad idea and should be avoided when possible.\nInstead, a new parameter should be constructed. Besides the possible values that can be given to a constructor, there are also the $class, $nlevels, $is_bounded, $has_default, $storage_type, $is_number and $is_categ slots that give information about a parameter.\nA list of all slots can be found in ?Param.\n\nparB$lower\n\n[1] 0\n\nparA$levels\n\n[1]  TRUE FALSE\n\nparE$class\n\n[1] \"ParamUty\"\n\n\nIt is also possible to get all information of a Param as data.table by calling as.data.table().\n\nas.data.table(parA)\n\n   id    class lower upper      levels nlevels is_bounded special_vals\n1:  A ParamLgl    NA    NA  TRUE,FALSE       2       TRUE    <list[0]>\n3 variables not shown: [default, storage_type, tags]\n\n\n\n9.4.2.1.1 Type / Range Checking\nA Param object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the $test(), $check(), and $assert() functions. test() should be used within conditional checks and returns TRUE or FALSE, while check() returns an error description when a value does not conform to the parameter (and thus plays well with the \"checkmate::assert()\" function). assert() will throw an error whenever a value does not fit.\n\nparA$test(FALSE)\n\n[1] TRUE\n\nparA$test(\"FALSE\")\n\n[1] FALSE\n\nparA$check(\"FALSE\")\n\n[1] \"Must be of type 'logical flag', not 'character'\"\n\n\nInstead of testing single parameters, it is often more convenient to check a whole set of parameters using a ParamSet.\n\n9.4.2.2 Parameter Sets\nThe ordered collection of parameters is handled in a ParamSet1. It is initialized using the $new() function and optionally takes a list of Params as argument. Parameters can also be added to the constructed ParamSet using the $add() function. It is even possible to add whole ParamSets to other ParamSets.1 Although the name is suggestive of a “Set”-valued Param, this is unrelated to the other objects that follow the ParamXxx naming scheme.\n\nps = ParamSet$new(list(parA, parB))\nps$add(parC)\nps$add(ParamSet$new(list(parD, parE)))\nprint(ps)\n\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  A ParamLgl    NA    NA       2 <NoDefault[3]>      \n2:  B ParamInt     0    10      11 <NoDefault[3]>      \n3:  C ParamDbl     0     4     Inf <NoDefault[3]>      \n4:  D ParamFct    NA    NA       3              y      \n5:  E ParamUty    NA    NA     Inf <NoDefault[3]>      \n\n\nThe individual parameters can be accessed through the $params slot. It is also possible to get information about all parameters in a vectorized fashion using mostly the same slots as for individual Params (i.e. $class, $levels etc.), see ?ParamSet for details.\nIt is possible to reduce ParamSets using the $subset method. Be aware that it modifies a ParamSet in-place, so a “clone” must be created first if the original ParamSet should not be modified.\n\npsSmall = ps$clone()\npsSmall$subset(c(\"A\", \"B\", \"C\"))\nprint(psSmall)\n\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  A ParamLgl    NA    NA       2 <NoDefault[3]>      \n2:  B ParamInt     0    10      11 <NoDefault[3]>      \n3:  C ParamDbl     0     4     Inf <NoDefault[3]>      \n\n\nJust as for Params, and much more useful, it is possible to get the ParamSet as a data.table using as.data.table(). This makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by data.table.\n\nas.data.table(ps)\n\n   id    class lower upper      levels nlevels is_bounded special_vals\n1:  A ParamLgl    NA    NA  TRUE,FALSE       2       TRUE    <list[0]>\n2:  B ParamInt     0    10                  11       TRUE    <list[0]>\n3:  C ParamDbl     0     4                 Inf       TRUE    <list[1]>\n4:  D ParamFct    NA    NA       x,y,z       3       TRUE    <list[0]>\n5:  E ParamUty    NA    NA                 Inf      FALSE    <list[0]>\n3 variables not shown: [default, storage_type, tags]\n\n\n\n9.4.2.2.1 Type / Range Checking\nSimilar to individual Params, the ParamSet provides $test(), $check() and $assert() functions that allow for type and range checking of parameters. Their argument must be a named list with values that are checked against the respective parameters. It is possible to check only a subset of parameters.\n\nps$check(list(A = TRUE, B = 0, E = identity))\n\n[1] TRUE\n\nps$check(list(A = 1))\n\n[1] \"A: Must be of type 'logical flag', not 'double'\"\n\nps$check(list(Z = 1))\n\n[1] \"Parameter 'Z' not available. Did you mean 'A' / 'B' / 'C'?\"\n\n\n\n9.4.2.2.2 Values in a ParamSet\n\nAlthough a ParamSet fundamentally represents a value space, it also has a slot $values that can contain a point within that space. This is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified. The $values slot contains a named list that is always checked against parameter constraints. When trying to set parameter values, e.g. for mlr3 Learners, it is the $values slot of its $param_set that needs to be used.\n\nps$values = list(A = TRUE, B = 0)\nps$values$B = 1\nprint(ps$values)\n\n$A\n[1] TRUE\n\n$B\n[1] 1\n\n\nThe parameter constraints are automatically checked:\n\nps$values$B = 100\n\nError in self$assert(xs): Assertion on 'xs' failed: B: Element 1 is not <= 10.\n\n\n\n9.4.2.2.3 Dependencies\nIt is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters. An example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter). The second parameter would be said to depend on the first parameter having the value TRUE.\nA dependency can be added using the $add_dep method, which takes both the ids of the “depender” and “dependee” parameters as well as a Condition object. The Condition object represents the check to be performed on the “dependee”. Currently it can be created using CondEqual$new() and CondAnyOf$new(). Multiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced.\nThe consequence of dependencies are twofold: For one, the $check(), $test() and $assert() tests will not accept the presence of a parameter if its dependency is not met. Furthermore, when sampling or creating grid designs from a ParamSet, the dependencies will be respected (see Parameter Sampling, in particular Hierarchical Sampler).\nThe following example makes parameter D depend on parameter A being FALSE, and parameter B depend on parameter D being one of \"x\" or \"y\". This introduces an implicit dependency of B on A being FALSE as well, because D does not take any value if A is TRUE.\n\nps$add_dep(\"D\", \"A\", CondEqual$new(FALSE))\nps$add_dep(\"B\", \"D\", CondAnyOf$new(c(\"x\", \"y\")))\n\n\nps$check(list(A = FALSE, D = \"x\", B = 1))          # OK: all dependencies met\n\n[1] TRUE\n\nps$check(list(A = FALSE, D = \"z\", B = 1))          # B's dependency is not met\n\n[1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the current parameter value is: D=z\"\n\nps$check(list(A = FALSE, B = 1))                   # B's dependency is not met\n\n[1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\n\nps$check(list(A = FALSE, D = \"z\"))                 # OK: B is absent\n\n[1] TRUE\n\nps$check(list(A = TRUE))                           # OK: neither B nor D present\n\n[1] TRUE\n\nps$check(list(A = TRUE, D = \"x\", B = 1))           # D's dependency is not met\n\n[1] \"The parameter 'D' can only be set if the following condition is met 'A = FALSE'. Instead the current parameter value is: A=TRUE\"\n\nps$check(list(A = TRUE, B = 1))                    # B's dependency is not met\n\n[1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\n\n\nInternally, the dependencies are represented as a data.table, which can be accessed listed in the $deps slot. This data.table can even be mutated, to e.g. remove dependencies. There are no sanity checks done when the $deps slot is changed this way. Therefore it is advised to be cautious.\n\nps$deps\n\n   id on           cond\n1:  D  A <CondEqual[9]>\n2:  B  D <CondAnyOf[9]>\n\n\n\n9.4.2.3 Vector Parameters\nUnlike in the old ParamHelpers package, there are no more vectorial parameters in paradox. Instead, it is now possible to create multiple copies of a single parameter using the $rep function. This creates a ParamSet consisting of multiple copies of the parameter, which can then (optionally) be added to another ParamSet.\n\nps2d = ParamDbl$new(\"x\", lower = 0, upper = 1)$rep(2)\nprint(ps2d)\n\n<ParamSet>\n        id    class lower upper nlevels        default value\n1: x_rep_1 ParamDbl     0     1     Inf <NoDefault[3]>      \n2: x_rep_2 ParamDbl     0     1     Inf <NoDefault[3]>      \n\n\n\nps$add(ps2d)\nprint(ps)\n\n<ParamSet>\n        id    class lower upper nlevels        default parents value\n1:       A ParamLgl    NA    NA       2 <NoDefault[3]>          TRUE\n2:       B ParamInt     0    10      11 <NoDefault[3]>       D     1\n3:       C ParamDbl     0     4     Inf <NoDefault[3]>              \n4:       D ParamFct    NA    NA       3              y       A      \n5:       E ParamUty    NA    NA     Inf <NoDefault[3]>              \n6: x_rep_1 ParamDbl     0     1     Inf <NoDefault[3]>              \n7: x_rep_2 ParamDbl     0     1     Inf <NoDefault[3]>              \n\n\nIt is also possible to use a ParamUty to accept vectorial parameters, which also works for parameters of variable length. A ParamSet containing a ParamUty can be used for parameter checking, but not for sampling. To sample values for a method that needs a vectorial parameter, it is advised to use a parameter transformation function that creates a vector from atomic values.\nAssembling a vector from repeated parameters is aided by the parameter’s $tags: Parameters that were generated by the $rep() command automatically get tagged as belonging to a group of repeated parameters.\n\nps$tags\n\n$A\ncharacter(0)\n\n$B\n[1] \"tag1\" \"tag2\"\n\n$C\ncharacter(0)\n\n$D\ncharacter(0)\n\n$E\ncharacter(0)\n\n$x_rep_1\n[1] \"x_rep\"\n\n$x_rep_2\n[1] \"x_rep\"\n\n\n\n9.4.3 Parameter Sampling\nIt is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning). paradox offers a variety of functions that allow creating evenly-spaced parameter values in a “grid” design as well as random sampling. In the latter case, it is possible to influence the sampling distribution in more or less fine detail.\nA point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not ParamUty. Furthermore, for most samplers ParamInt and ParamDbl must have finite lower and upper bounds.\n\n9.4.3.1 Parameter Designs\nFunctions that sample the parameter space fundamentally return an object of the Design class. These objects contain the sampled data as a data.table under the $data slot, and also offer conversion to a list of parameter-values using the $transpose() function.\n\n9.4.3.2 Grid Design\nThe generate_design_grid() function is used to create grid designs that contain all combinations of parameter values: All possible values for ParamLgl and ParamFct, and values with a given resolution for ParamInt and ParamDbl. The resolution can be given for all numeric parameters, or for specific named parameters through the param_resolutions parameter.\n\ndesign = generate_design_grid(psSmall, 2)\nprint(design)\n\n<Design> with 8 rows:\n       A  B C\n1:  TRUE  0 0\n2:  TRUE  0 4\n3:  TRUE 10 0\n4:  TRUE 10 4\n5: FALSE  0 0\n6: FALSE  0 4\n7: FALSE 10 0\n8: FALSE 10 4\n\n\n\ngenerate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2))\n\n<Design> with 4 rows:\n   B C     A\n1: 0 0  TRUE\n2: 0 0 FALSE\n3: 0 4  TRUE\n4: 0 4 FALSE\n\n\n\n9.4.3.3 Random Sampling\nparadox offers different methods for random sampling, which vary in the degree to which they can be configured. The easiest way to get a uniformly random sample of parameters is generate_design_random(). It is also possible to create “latin hypercube” sampled parameter values using generate_design_lhs(), which utilizes the lhs package. LHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values.\n\npvrand = generate_design_random(ps2d, 500)\npvlhs = generate_design_lhs(ps2d, 500)\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.3.4 Generalized Sampling: The Sampler Class\nIt may sometimes be desirable to configure parameter sampling in more detail. paradox uses the Sampler abstract base class for sampling, which has many different sub-classes that can be parameterized and combined to control the sampling process. It is even possible to create further sub-classes of the Sampler class (or of any of its subclasses) for even more possibilities.\nEvery Sampler object has a sample() function, which takes one argument, the number of instances to sample, and returns a Design object.\n\n9.4.3.4.1 1D-Samplers\nThere is a variety of samplers that sample values for a single parameter. These are Sampler1DUnif (uniform sampling), Sampler1DCateg (sampling for categorical parameters), Sampler1DNormal (normally distributed sampling, truncated at parameter bounds), and Sampler1DRfun (arbitrary 1D sampling, given a random-function). These are initialized with a single Param, and can then be used to sample values.\n\nsampA = Sampler1DCateg$new(parA)\nsampA$sample(5)\n\n<Design> with 5 rows:\n       A\n1:  TRUE\n2:  TRUE\n3: FALSE\n4: FALSE\n5: FALSE\n\n\n\n9.4.3.4.2 Hierarchical Sampler\nThe SamplerHierarchical sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution. Its name “hierarchical” implies that it is able to respect parameter dependencies. This suggests that parameters only get sampled when their dependencies are met.\nThe following example shows how this works: The Int parameter B depends on the Lgl parameter A being TRUE. A is sampled to be TRUE in about half the cases, in which case B takes a value between 0 and 10. In the cases where A is FALSE, B is set to NA.\n\npsSmall$add_dep(\"B\", \"A\", CondEqual$new(TRUE))\nsampH = SamplerHierarchical$new(psSmall,\n  list(Sampler1DCateg$new(parA),\n    Sampler1DUnif$new(parB),\n    Sampler1DUnif$new(parC))\n)\nsampled = sampH$sample(1000)\ntable(sampled$data[, c(\"A\", \"B\")], useNA = \"ifany\")\n\n       B\nA         0   1   2   3   4   5   6   7   8   9  10 <NA>\n  FALSE   0   0   0   0   0   0   0   0   0   0   0  493\n  TRUE   51  49  50  42  47  47  44  52  35  49  41    0\n\n\n\n9.4.3.4.3 Joint Sampler\nAnother way of combining samplers is the SamplerJointIndep. SamplerJointIndep also makes it possible to combine Samplers that are not 1D. However, SamplerJointIndep currently can not handle ParamSets with dependencies.\n\nsampJ = SamplerJointIndep$new(\n  list(Sampler1DUnif$new(ParamDbl$new(\"x\", 0, 1)),\n    Sampler1DUnif$new(ParamDbl$new(\"y\", 0, 1)))\n)\nsampJ$sample(5)\n\n<Design> with 5 rows:\n           x          y\n1: 0.9980689 0.29325189\n2: 0.5696531 0.14916164\n3: 0.7999309 0.87583963\n4: 0.9762024 0.06155586\n5: 0.4334808 0.60938294\n\n\n\n9.4.3.4.4 SamplerUnif\nThe Sampler used in generate_design_random() is the SamplerUnif sampler, which corresponds to a HierarchicalSampler of Sampler1DUnif for all parameters.\n\n9.4.4 Parameter Transformation\nWhile the different Samplers allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them. This can be done by assigning a function to the $trafo slot of a ParamSet. The $trafo function is called with two parameters:\n\nThe list of parameter values to be transformed as x\n\nThe ParamSet itself as param_set\n\n\nThe $trafo function must return a list of transformed parameter values.\nThe transformation is performed when calling the $transpose function of the Design object returned by a Sampler with the trafo ParamSet to TRUE (the default). The following, for example, creates a parameter that is exponentially distributed:\n\npsexp = ParamSet$new(list(ParamDbl$new(\"par\", 0, 1)))\npsexp$trafo = function(x, param_set) {\n  x$par = -log(x$par)\n  x\n}\ndesign = generate_design_random(psexp, 2)\nprint(design)\n\n<Design> with 2 rows:\n          par\n1: 0.07302144\n2: 0.15870426\n\ndesign$transpose()  # trafo is TRUE\n\n[[1]]\n[[1]]$par\n[1] 2.617002\n\n\n[[2]]\n[[2]]$par\n[1] 1.840713\n\n\nCompare this to $transpose() without transformation:\n\ndesign$transpose(trafo = FALSE)\n\n[[1]]\n[[1]]$par\n[1] 0.07302144\n\n\n[[2]]\n[[2]]$par\n[1] 0.1587043\n\n\n\n9.4.4.1 Transformation between Types\nUsually the design created with one ParamSet is then used to configure other objects that themselves have a ParamSet which defines the values they take. The ParamSets which can be used for random sampling, however, are restricted in some ways: They must have finite bounds, and they may not contain “untyped” (ParamUty) parameters. $trafo provides the glue for these situations. There is relatively little constraint on the trafo function’s return value, so it is possible to return values that have different bounds or even types than the original ParamSet. It is even possible to remove some parameters and add new ones.\nSuppose, for example, that a certain method requires a function as a parameter. Let’s say a function that summarizes its data in a certain way. The user can pass functions like median() or mean(), but could also pass quantiles or something completely different. This method would probably use the following ParamSet:\n\nmethodPS = ParamSet$new(\n  list(\n    ParamUty$new(\"fun\",\n      custom_check = function(x) checkmate::checkFunction(x, nargs = 1))\n  )\n)\nprint(methodPS)\n\n<ParamSet>\n    id    class lower upper nlevels        default value\n1: fun ParamUty    NA    NA     Inf <NoDefault[3]>      \n\n\nIf one wanted to sample this method, using one of four functions, a way to do this would be:\n\nsamplingPS = ParamSet$new(\n  list(\n    ParamFct$new(\"fun\", c(\"mean\", \"median\", \"min\", \"max\"))\n  )\n)\n\nsamplingPS$trafo = function(x, param_set) {\n  # x$fun is a `character(1)`,\n  # in particular one of 'mean', 'median', 'min', 'max'.\n  # We want to turn it into a function!\n  x$fun = get(x$fun, mode = \"function\")\n  x\n}\n\n\ndesign = generate_design_random(samplingPS, 2)\nprint(design)\n\n<Design> with 2 rows:\n   fun\n1: max\n2: min\n\n\nNote that the Design only contains the column “fun” as a character column. To get a single value as a function, the $transpose function is used.\n\nxvals = design$transpose()\nprint(xvals[[1]])\n\n$fun\nfunction (..., na.rm = FALSE)  .Primitive(\"max\")\n\n\nWe can now check that it fits the requirements set by methodPS, and that fun it is in fact a function:\n\nmethodPS$check(xvals[[1]])\n\n[1] TRUE\n\nxvals[[1]]$fun(1:10)\n\n[1] 10\n\n\nImagine now that a different kind of parametrization of the function is desired: The user wants to give a function that selects a certain quantile, where the quantile is set by a parameter. In that case the $transpose function could generate a function in a different way. For interpretability, the parameter is called “quantile” before transformation, and the “fun” parameter is generated on the fly.\n\nsamplingPS2 = ParamSet$new(\n  list(\n    ParamDbl$new(\"quantile\", 0, 1)\n  )\n)\n\nsamplingPS2$trafo = function(x, param_set) {\n  # x$quantile is a `numeric(1)` between 0 and 1.\n  # We want to turn it into a function!\n  list(fun = function(input) quantile(input, x$quantile))\n}\n\n\ndesign = generate_design_random(samplingPS2, 2)\nprint(design)\n\n<Design> with 2 rows:\n     quantile\n1: 0.03543698\n2: 0.65563727\n\n\nThe Design now contains the column “quantile” that will be used by the $transpose function to create the fun parameter. We also check that it fits the requirement set by methodPS, and that it is a function.\n\nxvals = design$transpose()\nprint(xvals[[1]])\n\n$fun\nfunction(input) quantile(input, x$quantile)\n<environment: 0x560a6298edd0>\n\nmethodPS$check(xvals[[1]])\n\n[1] TRUE\n\nxvals[[1]]$fun(1:10)\n\n3.543698% \n 1.318933 \n\n\n\n9.4.5 Defining a Tuning Spaces\nWhen running an optimization, it is important to inform the tuning algorithm about what hyperparameters are valid. Here the names, types, and valid ranges of each hyperparameter are important. All this information is communicated with objects of the class ParamSet, which is defined in paradox. While it is possible to create ParamSet-objects using its $new-constructor, it is much shorter and readable to use the ps-shortcut, which will be presented here. For an in-depth description of paradox and its classes, see Section 9.4.\nNote, that ParamSet objects exist in two contexts. First, ParamSet-objects are used to define the space of valid parameter settings for a learner (and other objects). Second, they are used to define a search space for tuning. We are mainly interested in the latter. For example we can consider the minsplit parameter of the classif.rpart Learner. The ParamSet associated with the learner has a lower but no upper bound. However, for tuning the value, a lower and upper bound must be given because tuning search spaces need to be bounded. For Learner or PipeOp objects, typically “unbounded” ParamSets are used. Here, however, we will mainly focus on creating “bounded” ParamSets that can be used for tuning. See Section 9.4 for more details on using ParamSets to define parameter ranges for use-cases besides tuning.\n\n9.4.5.1 Creating ParamSets\nAn empty \"ParamSet\") – not yet very useful – can be constructed using just the \"ps\") call:\n\nsearch_space = ps()\nprint(search_space)\n\n<ParamSet>\nEmpty.\n\n\nps takes named Domain arguments that are turned into parameters. A possible search space for the \"classif.svm\" learner could for example be:\n\nsearch_space = ps(\n  cost = p_dbl(lower = 0.1, upper = 10),\n  kernel = p_fct(levels = c(\"polynomial\", \"radial\"))\n)\nprint(search_space)\n\n<ParamSet>\n       id    class lower upper nlevels        default value\n1:   cost ParamDbl   0.1    10     Inf <NoDefault[3]>      \n2: kernel ParamFct    NA    NA       2 <NoDefault[3]>      \n\n\nThere are five domain constructors that produce a parameters when given to ps:\n\n\nConstructor\nDescription\nIs bounded?\nUnderlying Class\n\n\n\np_dbl\nReal valued parameter (“double”)\nWhen upper and lower are given\nParamDbl\n\n\np_int\nInteger parameter\nWhen upper and lower are given\nParamInt\n\n\np_fct\nDiscrete valued parameter (“factor”)\nAlways\nParamFct\n\n\np_lgl\nLogical / Boolean parameter\nAlways\nParamLgl\n\n\np_uty\nUntyped parameter\nNever\nParamUty\n\n\n\nThese domain constructors each take some of the following arguments:\n\n\nlower, upper: lower and upper bound of numerical parameters (p_dbl and p_int). These need to be given to get bounded parameter spaces valid for tuning.\n\nlevels: Allowed categorical values for p_fct parameters. Required argument for p_fct. See below for more details on this parameter.\n\ntrafo: transformation function, see below.\n\ndepends: dependencies, see below.\n\ntags: Further information about a parameter, used for example by the hyperband tuner.\n\ndefault: Value corresponding to default behavior when the parameter is not given. Not used for tuning search spaces.\n\nspecial_vals: Valid values besides the normally accepted values for a parameter. Not used for tuning search spaces.\n\ncustom_check: Function that checks whether a value given to p_uty is valid. Not used for tuning search spaces.\n\nThe lower and upper parameters are always in the first and second position respectively, except for p_fct where levels is in the first position. It is preferred to omit the labels (ex: upper = 0.1 becomes just 0.1). This way of defining a ParamSet is more concise than the equivalent definition above. Preferred:\n\nsearch_space = ps(cost = p_dbl(0.1, 10), kernel = p_fct(c(\"polynomial\", \"radial\")))\n\n\n9.4.5.2 Transformations (trafo)\nWe can use the paradox function generate_design_grid to look at the values that would be evaluated by grid search. (We are using rbindlist() here because the result of $transpose() is a list that is harder to read. If we didn’t use $transpose(), on the other hand, the transformations that we investigate here are not applied.) In generate_design_grid(search_space, 3), search_space is the ParamSet argument and 3 is the specified resolution in the parameter space. The resolution for categorical parameters is ignored; these parameters always produce a grid over all of their valid levels. For numerical parameters the endpoints of the params are always included in the grid, so if there were 3 levels for the kernel instead of 2 there would be 9 rows, or if the resolution was 4 in this example there would be 8 rows in the resulting table.\n\nlibrary(\"data.table\")\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n    cost     kernel\n1:  0.10 polynomial\n2:  0.10     radial\n3:  5.05 polynomial\n4:  5.05     radial\n5: 10.00 polynomial\n6: 10.00     radial\n\n\nWe notice that the cost parameter is taken on a linear scale. We assume, however, that the difference of cost between 0.1 and 1 should have a similar effect as the difference between 1 and 10. Therefore it makes more sense to tune it on a logarithmic scale. This is done by using a transformation (trafo). This is a function that is applied to a parameter after it has been sampled by the tuner. We can tune cost on a logarithmic scale by sampling on the linear scale [-1, 1] and computing 10^x from that value.\n\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  1.0 polynomial\n4:  1.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n\n\nIt is even possible to attach another transformation to the ParamSet as a whole that gets executed after individual parameter’s transformations were performed. It is given through the .extra_trafo argument and should be a function with parameters x and param_set that takes a list of parameter values in x and returns a modified list. This transformation can access all parameter values of an evaluation and modify them with interactions. It is even possible to add or remove parameters. (The following is a bit of a silly example.)\n\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  .extra_trafo = function(x, param_set) {\n    if (x$kernel == \"polynomial\") {\n      x$cost = x$cost * 2\n    }\n    x\n  }\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n   cost     kernel\n1:  0.2 polynomial\n2:  0.1     radial\n3:  2.0 polynomial\n4:  1.0     radial\n5: 20.0 polynomial\n6: 10.0     radial\n\n\nThe available types of search space parameters are limited: continuous, integer, discrete, and logical scalars. There are many machine learning algorithms, however, that take parameters of other types, for example vectors or functions. These can not be defined in a search space ParamSet, and they are often given as ParamUty in the Learner’s ParamSet. When trying to tune over these hyperparameters, it is necessary to perform a Transformation that changes the type of a parameter.\nAn example is the class.weights parameter of the Support Vector Machine (SVM), which takes a named vector of class weights with one entry for each target class. The trafo that would tune class.weights for the mlr_tasks_spam, 'tsk(\"spam\") dataset could be:\n\nsearch_space = ps(\n  class.weights = p_dbl(0.1, 0.9, trafo = function(x) c(spam = x, nonspam = 1 - x))\n)\ngenerate_design_grid(search_space, 3)$transpose()\n\n[[1]]\n[[1]]$class.weights\n   spam nonspam \n    0.1     0.9 \n\n\n[[2]]\n[[2]]$class.weights\n   spam nonspam \n    0.5     0.5 \n\n\n[[3]]\n[[3]]$class.weights\n   spam nonspam \n    0.9     0.1 \n\n\n(We are omitting rbindlist() in this example because it breaks the vector valued return elements.)\n\n9.4.6 Automatic Factor Level Transformation\nA common use-case is the necessity to specify a list of values that should all be tried (or sampled from). It may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried. Or it may be that a choice of special numeric values should be tried. For this, the p_fct constructor’s level argument may be a value that is not a character vector, but something else. If, for example, only the values 0.1, 3, and 10 should be tried for the cost parameter, even when doing random search, then the following search space would achieve that:\n\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  3.0 polynomial\n4:  3.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n\n\nThis is equivalent to the following:\n\nsearch_space = ps(\n  cost = p_fct(c(\"0.1\", \"3\", \"10\"),\n    trafo = function(x) list(`0.1` = 0.1, `3` = 3, `10` = 10)[[x]]),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  3.0 polynomial\n4:  3.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n\n\nNote: Though the resolution is 3 here, in this case it doesn’t matter because both cost and kernel are factors (the resolution for categorical variables is ignored, these parameters always produce a grid over all their valid levels).\nThis may seem silly, but makes sense when considering that factorial tuning parameters are always character values:\n\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\ntypeof(search_space$params$cost$levels)\n\n[1] \"character\"\n\n\nBe aware that this results in an “unordered” hyperparameter, however. Tuning algorithms that make use of ordering information of parameters, like genetic algorithms or model based optimization, will perform worse when this is done. For these algorithms, it may make more sense to define a p_dbl or p_int with a more fitting trafo.\nThe class.weights case from above can also be implemented like this, if there are only a few candidates of class.weights vectors that should be tried. Note that the levels argument of p_fct must be named if there is no easy way for as.character() to create names:\n\nsearch_space = ps(\n  class.weights = p_fct(\n    list(\n      candidate_a = c(spam = 0.5, nonspam = 0.5),\n      candidate_b = c(spam = 0.3, nonspam = 0.7)\n    )\n  )\n)\ngenerate_design_grid(search_space)$transpose()\n\n[[1]]\n[[1]]$class.weights\n   spam nonspam \n    0.5     0.5 \n\n\n[[2]]\n[[2]]$class.weights\n   spam nonspam \n    0.3     0.7 \n\n\n\n9.4.6.1 Parameter Dependencies (depends)\nSome parameters are only relevant when another parameter has a certain value, or one of several values. The Support Vector Machine (SVM), for example, has the degree parameter that is only valid when kernel is \"polynomial\". This can be specified using the depends argument. It is an expression that must involve other parameters and be of the form <param> == <scalar>, <param> %in% <vector>, or multiple of these chained by &&. To tune the degree parameter, one would need to do the following:\n\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  degree = p_int(1, 3, depends = kernel == \"polynomial\")\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose(), fill = TRUE)\n\n    cost     kernel degree\n 1:  0.1 polynomial      1\n 2:  0.1 polynomial      2\n 3:  0.1 polynomial      3\n 4:  0.1     radial     NA\n 5:  1.0 polynomial      1\n 6:  1.0 polynomial      2\n 7:  1.0 polynomial      3\n 8:  1.0     radial     NA\n 9: 10.0 polynomial      1\n10: 10.0 polynomial      2\n11: 10.0 polynomial      3\n12: 10.0     radial     NA\n\n\n\n9.4.6.2 Creating Tuning ParamSets from other ParamSets\nHaving to define a tuning ParamSet for a Learner that already has parameter set information may seem unnecessarily tedious, and there is indeed a way to create tuning ParamSets from a Learner’s ParamSet, making use of as much information as already available.\nThis is done by setting values of a Learner’s ParamSet to so-called TuneTokens, constructed with a to_tune call. This can be done in the same way that other hyperparameters are set to specific values. It can be understood as the hyperparameters being tagged for later tuning. The resulting ParamSet used for tuning can be retrieved using the $search_space() method.\n\nlearner = lrn(\"classif.svm\")\nlearner$param_set$values$kernel = \"polynomial\" # for example\nlearner$param_set$values$degree = to_tune(lower = 1, upper = 3)\n\nprint(learner$param_set$search_space())\n\n<ParamSet>\n       id    class lower upper nlevels        default value\n1: degree ParamInt     1     3       3 <NoDefault[3]>      \n\nrbindlist(generate_design_grid(\n  learner$param_set$search_space(), 3)$transpose()\n)\n\n   degree\n1:      1\n2:      2\n3:      3\n\n\nIt is possible to omit lower here, because it can be inferred from the lower bound of the degree parameter itself. For other parameters, that are already bounded, it is possible to not give any bounds at all, because their ranges are already bounded. An example is the logical shrinking hyperparameter:\n\nlearner$param_set$values$shrinking = to_tune()\n\nprint(learner$param_set$search_space())\n\n<ParamSet>\n          id    class lower upper nlevels        default value\n1:    degree ParamInt     1     3       3 <NoDefault[3]>      \n2: shrinking ParamLgl    NA    NA       2           TRUE      \n\nrbindlist(generate_design_grid(\n  learner$param_set$search_space(), 3)$transpose()\n)\n\n   degree shrinking\n1:      1      TRUE\n2:      1     FALSE\n3:      2      TRUE\n4:      2     FALSE\n5:      3      TRUE\n6:      3     FALSE\n\n\n\"to_tune\") can also be constructed with a Domain object, i.e. something constructed with a p_*** call. This way it is possible to tune continuous parameters with discrete values, or to give trafos or dependencies. One could, for example, tune the cost as above on three given special values, and introduce a dependency of shrinking on it. Notice that a short form for to_tune(<levels>) is a short form of to_tune(p_fct(<levels>)).\n\n\n\n\n\n\nNote\n\n\n\nWhen introducing the dependency, we need to use the degree value from before the implicit trafo, which is the name or as.character() of the respective value, here \"val2\"!\n\n\n\nlearner$param_set$values$type = \"C-classification\" # needs to be set because of a bug in paradox\nlearner$param_set$values$cost = to_tune(c(val1 = 0.3, val2 = 0.7))\nlearner$param_set$values$shrinking = to_tune(p_lgl(depends = cost == \"val2\"))\n\nprint(learner$param_set$search_space())\n\n<ParamSet>\n          id    class lower upper nlevels        default parents value\n1:      cost ParamFct    NA    NA       2 <NoDefault[3]>              \n2:    degree ParamInt     1     3       3 <NoDefault[3]>              \n3: shrinking ParamLgl    NA    NA       2 <NoDefault[3]>    cost      \nTrafo is set.\n\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)\n\n   degree cost shrinking\n1:      1  0.3        NA\n2:      1  0.7      TRUE\n3:      1  0.7     FALSE\n4:      2  0.3        NA\n5:      2  0.7      TRUE\n6:      2  0.7     FALSE\n7:      3  0.3        NA\n8:      3  0.7      TRUE\n9:      3  0.7     FALSE\n\n\nThe \"search_space() picks up dependencies from the underlying ParamSet automatically. So if the kernel is tuned, then degree automatically gets the dependency on it, without us having to specify that. (Here we reset cost and shrinking to NULL for the sake of clarity of the generated output.)\n\nlearner$param_set$values$cost = NULL\nlearner$param_set$values$shrinking = NULL\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n\nprint(learner$param_set$search_space())\n\n<ParamSet>\n       id    class lower upper nlevels        default parents value\n1: degree ParamInt     1     3       3 <NoDefault[3]>  kernel      \n2: kernel ParamFct    NA    NA       2 <NoDefault[3]>              \n\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)\n\n       kernel degree\n1: polynomial      1\n2: polynomial      2\n3: polynomial      3\n4:     radial     NA\n\n\nIt is even possible to define whole ParamSets that get tuned over for a single parameter. This may be especially useful for vector hyperparameters that should be searched along multiple dimensions. This ParamSet must, however, have an .extra_trafo that returns a list with a single element, because it corresponds to a single hyperparameter that is being tuned. Suppose the class.weights hyperparameter should be tuned along two dimensions:\n\nlearner$param_set$values$class.weights = to_tune(\n  ps(spam = p_dbl(0.1, 0.9), nonspam = p_dbl(0.1, 0.9),\n    .extra_trafo = function(x, param_set) list(c(spam = x$spam, nonspam = x$nonspam))\n))\nhead(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), 3)\n\n[[1]]\n[[1]]$kernel\n[1] \"polynomial\"\n\n[[1]]$degree\n[1] 1\n\n[[1]]$class.weights\n   spam nonspam \n    0.1     0.1 \n\n\n[[2]]\n[[2]]$kernel\n[1] \"polynomial\"\n\n[[2]]$degree\n[1] 1\n\n[[2]]$class.weights\n   spam nonspam \n    0.1     0.5 \n\n\n[[3]]\n[[3]]$kernel\n[1] \"polynomial\"\n\n[[3]]$degree\n[1] 1\n\n[[3]]$class.weights\n   spam nonspam \n    0.1     0.9"
  },
  {
    "objectID": "technical.html#sec-logging",
    "href": "technical.html#sec-logging",
    "title": "9  Technical",
    "section": "\n9.5 Logging",
    "text": "9.5 Logging\nWe use the lgr package for logging and progress output.\n\n9.5.1 Changing mlr3 logging levels\nTo change the setting for mlr3 for the current session, you need to retrieve the logger (which is a R6 object) from lgr, and then change the threshold of the like this:\n\nrequireNamespace(\"lgr\")\n\nlogger = lgr::get_logger(\"mlr3\")\nlogger$set_threshold(\"<level>\")\n\nThe default log level is \"info\". All available levels can be listed as follows:\n\ngetOption(\"lgr.log_levels\")\n\nfatal error  warn  info debug trace \n  100   200   300   400   500   600 \n\n\nTo increase verbosity, set the log level to a higher value, e.g. to \"debug\" with:\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"debug\")\n\nTo reduce the verbosity, reduce the log level to warn:\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nlgr comes with a global option called \"lgr.default_threshold\" which can be set via options() to make your choice permanent across sessions.\nAlso note that the optimization packages such as mlr3tuning mlr3fselect use the logger of their base package bbotk. To disable the output from mlr3, but keep the output from mlr3tuning, reduce the verbosity for the logger mlr3 and optionally change the logger bbotk to the desired level.\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")\n\n\n9.5.2 Redirecting output\nRedirecting output is already extensively covered in the documentation and vignette of lgr. Here is just a short example that adds an additional appender to log events into a temporary file in JSON format:\n\ntf = tempfile(\"mlr3log_\", fileext = \".json\")\n\n# get the logger as R6 object\nlogger = lgr::get_logger(\"mlr\")\n\n# add Json appender\nlogger$add_appender(lgr::AppenderJson$new(tf), name = \"json\")\n\n# signal a warning\nlogger$warn(\"this is a warning from mlr3\")\n\nWARN  [19:37:26.723] this is a warning from mlr3\n\n# print the contents of the file\ncat(readLines(tf))\n\n{\"level\":300,\"timestamp\":\"2023-02-27 19:37:26\",\"logger\":\"mlr\",\"caller\":\"eval\",\"msg\":\"this is a warning from mlr3\"}\n\n# remove the appender again\nlogger$remove_appender(\"json\")\n\n\n9.5.3 Immediate Log Feedback\nmlr3 uses future and encapsulation to make evaluations fast, stable, and reproducible. However, this may lead to logs being delayed, out of order, or, in case of some errors, not present at all.\nWhen it is necessary to have immediate access to log messages, for example to investigate problems, one may therefore choose to disable future and encapsulation. This can be done by enabling the debug mode using options(mlr.debug = TRUE); the $encapsulate slot of learners should also be set to \"none\" (default) or \"evaluate\", but not \"callr\". This should only be done to investigate problems, however, and not for production use, because\n\nthis disables parallelization, and\nthis leads to different RNG behavior and therefore to results that are not reproducible when the debug mode is set.\n\n\n\n\n\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2021. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges.” https://doi.org/10.48550/ARXIV.2107.05847."
  },
  {
    "objectID": "interpretation.html#sec-penguin-task",
    "href": "interpretation.html#sec-penguin-task",
    "title": "10  Model Interpretation",
    "section": "\n10.1 Penguin Task",
    "text": "10.1 Penguin Task\nTo understand what model interpretation packages can offer, we start with a thorough example. The goal of this example is to figure out the species of penguins given a set of features. The palmerpenguins::penguins (Horst, Hill, and Gorman 2020) data set will be used which is an alternative to the iris data set. The penguins data sets contain 8 variables of 344 penguins:\n\ndata(\"penguins\", package = \"palmerpenguins\")\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nTo get started run:\n\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nset.seed(1)\n\n\npenguins = na.omit(penguins)\ntask_peng = as_task_classif(penguins, target = \"species\")\n\npenguins = na.omit(penguins) is to omit the 11 cases with missing values. If not omitted, there will be an error when running the learner from the data points that have N/A for some features.\n\nlearner = lrn(\"classif.ranger\")\nlearner$predict_type = \"prob\"\nlearner$train(task_peng)\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      333 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.01790106 \n\nx = penguins[which(names(penguins) != \"species\")]\n\nAs explained in Section Learners, specific learners can be queried with mlr_learners. In Section Train/Predict it is recommended for some classifiers to use the predict_type as prob instead of directly predicting a label. This is what is done in this example. penguins[which(names(penguins) != \"species\")] is the data of all the features and y will be the penguinsspecies. learner$train(task_peng) trains the model and learner$model stores the model from the training command. Predictor holds the machine learning model and the data. All interpretation methods in iml need the machine learning model and the data to be wrapped in the Predictor object."
  },
  {
    "objectID": "interpretation.html#sec-iml",
    "href": "interpretation.html#sec-iml",
    "title": "10  Model Interpretation",
    "section": "\n10.2 iml",
    "text": "10.2 iml\nAuthor: Shawn Storm\niml is an R package that interprets the behaviour and explains predictions of machine learning models. The functions provided in the iml package are model-agnostic which gives the flexibility to use any machine learning model.\nThis chapter provides examples of how to use iml with mlr3. For more information refer to the IML github and the IML book\nNext is the core functionality of iml. In this example, three separate interpretation methods will be used: FeatureEffects, FeatureImp and Shapley\n\nFeatureEffects computes the effects for all given features on the model prediction. Different methods are implemented: Accumulated Local Effect (ALE) plots, Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) curves.\nShapley computes feature contributions for single predictions with the Shapley value – an approach from cooperative game theory (Shapley Value).\nFeatureImp computes the importance of features by calculating the increase in the model’s prediction error after permuting the feature (more here).\n\n\n10.2.1 FeatureEffects\nIn addition to the commands above the following two need to be run:\n\nlibrary(\"iml\")\n\nmodel = Predictor$new(learner, data = x, y = penguins$species)\n\nnum_features = c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n\n\n\nPlot of the results from FeatureEffects. FeatureEffects computes and plots feature effects of prediction models\n\n\n\n\neffect stores the object from the FeatureEffect computation and the results can then be plotted. In this example, all of the features provided by the penguins data set were used.\nAll features except for year provide meaningful interpretable information. It should be clear why year doesn’t provide anything of significance. bill_length_mm shows for example that when the bill length is smaller than roughly 40mm, there is a high chance that the penguin is an Adelie.\n\n10.2.2 Shapley\n\nx = penguins[which(names(penguins) != \"species\")]\nmodel = Predictor$new(learner, data = penguins, y = \"species\")\nx.interest = data.frame(penguins[1, ])\nshapley = Shapley$new(model, x.interest = x.interest)\nplot(shapley)\n\n\n\nPlot of the results from Shapley. \\(\\phi\\) gives the increase or decrease in probability given the values on the vertical axis\n\n\n\n\nThe \\(\\phi\\) provides insight into the probability given the values on the vertical axis. For example, a penguin is less likely to be Gentoo if the bill_depth=18.7 is and much more likely to be Adelie than Chinstrap.\n\n10.2.3 FeatureImp\n\neffect = FeatureImp$new(model, loss = \"ce\")\neffect$plot(features = num_features)\n\n\n\nPlot of the results from FeatureImp. FeatureImp visualizes the importance of features given the prediction model\n\n\n\n\nFeatureImp shows the level of importance of the features when classifying penguins. It is clear to see that the bill_length_mm is of high importance and one should concentrate on the different boundaries of this feature when attempting to classify the three species.\n\n10.2.4 Independent Test Data\nIt is also interesting to see how well the model performs on a test data set. For this section, exactly as was recommended in Section Train/Predict, 80% of the penguin data set will be used for the training set and 20% for the test set:\n\ntrain_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)\ntest_set = setdiff(seq_len(task_peng$nrow), train_set)\nlearner$train(task_peng, row_ids = train_set)\nprediction = learner$predict(task_peng, row_ids = test_set)\n\nFirst, we compare the feature importance on training and test set\n\n# plot on training\nmodel = Predictor$new(learner, data = penguins[train_set, ], y = \"species\")\neffect = FeatureImp$new(model, loss = \"ce\")\nplot_train = plot(effect, features = num_features)\n\n# plot on test data\nmodel = Predictor$new(learner, data = penguins[test_set, ], y = \"species\")\neffect = FeatureImp$new(model, loss = \"ce\")\nplot_test = plot(effect, features = num_features)\n\n# combine into single plot\nlibrary(\"patchwork\")\nplot_train + plot_test\n\n\n\nFeatImp on train (left) and test (right)\n\n\n\n\nThe results of the train set for FeatureImp are very similar, which is expected. We follow a similar approach to compare the feature effects:\n\nmodel = Predictor$new(learner, data = penguins[train_set, ], y = \"species\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n\n\n\nFeatEffect train data set\n\n\n\n\n\nmodel = Predictor$new(learner, data = penguins[test_set, ], y = \"species\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n\n\n\nFeatEffect test data set\n\n\n\n\nAs is the case with FeatureImp, the test data results show either an over- or underestimate of feature importance / feature effects compared to the results where the entire penguin data set was used. This would be a good opportunity for the reader to attempt to resolve the estimation by playing with the amount of features and the amount of data used for both the test and train data sets of FeatureImp and FeatureEffects. Be sure to not change the line train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow) as it will randomly sample the data again."
  },
  {
    "objectID": "interpretation.html#sec-dalex",
    "href": "interpretation.html#sec-dalex",
    "title": "10  Model Interpretation",
    "section": "\n10.3 DALEX",
    "text": "10.3 DALEX\nThe DALEX (Biecek 2018) package belongs to DrWhy family of solutions created to support the responsible development of machine learning models. It implements the most common methods for explaining predictive models using posthoc model agnostic techniques. You can use it for any model built with the mlr3 package as well as with other frameworks in R. The counterpart in Python is the library dalex (Baniecki et al. 2021).\nThe philosophy of working with DALEX package is based on the process of explanatory model analysis described in the EMA book (Biecek and Burzykowski 2021). In this chapter, we present code snippets and a general overview of this package. For illustrative purposes, we reuse the learner model built in the Section 10.1 on palmerpenguins::penguins data.\nOnce you become familiar with the philosophy of working with the DALEX package, you can also use other packages from this family such as fairmodels (Wiśniewski and Biecek 2022) for detection and mitigation of biases, modelStudio (Baniecki and Biecek 2019) for interactive model exploration, modelDown (Romaszko et al. 2019) for the automatic generation of IML model documentation in the form of a report, survex (Krzyziński et al. 2023) for the explanation of survival models, or treeshap for the analysis of tree-based models.\n\n10.3.1 Explanatory model analysis\nThe analysis of a model is usually an interactive process starting with a shallow analysis – usually a single-number summary. Then in a series of subsequent steps, one can systematically deepen understanding of the model by exploring the importance of single variables or pairs of variables to an in-depth analysis of the relationship between selected variables to the model outcome. See Bücker et al. (2022) for a broader discussion of what the model exploration process looks like.\nThis explanatory model analysis (EMA) process can focus on a single observation, in which case we speak of local model analysis, or for a set of observations, in which case we speak of global data analysis. Below, we will present these two scenarios in separate subsections. See Figure 10.1 for an overview of key functions that will be discussed.\n\n\n\n\nFigure 10.1: Taxonomy of methods for model exploration presented in this chapter. Left part overview methods for global level exploration while the right part is related to local level model exploration.\n\n\n\n\nPredictive models in R have different internal structures. To be able to analyse them systematically, an intermediate object – a wrapper – is needed to provide a consistent interface for accessing the model. Working with explanations in the DALEX package always starts with the creation of such a wrapper with the use of the DALEX::explain() function. This function has several arguments that allow the model created by the various frameworks to be parameterised accordingly. For models created in the mlr3 package, it is more convenient to use the DALEXtra::explain_mlr3().\n\nlibrary(\"DALEX\")\nlibrary(\"DALEXtra\")\n\nranger_exp = DALEX::explain(learner,\n  data = penguins[test_set, ],\n  y = penguins[test_set, \"species\"],\n  label = \"Ranger Penguins\",\n  colorize = FALSE)\n\nPreparation of a new explainer is initiated\n  -> model label       :  Ranger Penguins \n  -> data              :  67  rows  8  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  Argument 'y' was a data frame. Converted to a vector. (  WARNING  )\n  -> target variable   :  67  values \n  -> predict function  :  yhat.LearnerClassif  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package mlr3 , ver. 0.14.1 , task multiclass (  default  ) \n  -> predicted values  :  predict function returns multiple columns:  3  (  default  ) \n  -> residual function :  difference between 1 and probability of true class (  default  )\n  -> residuals         :  numerical, min =  0 , mean =  0.07756016 , max =  0.5380321  \n  A new explainer has been created!  \n\n\nThe DALEX::explain() function performs a series of internal checks so the output is a bit verbose. Turn the verbose = FALSE argument to make it less wordy.\n\n10.3.2 Global level exploration\nThe global model analysis aims to understand how a model behaves on average on a set of observations, most commonly a test set. In the DALEX package, functions for global analysis have names starting with the prefix model_.\n\n10.3.2.1 Model Performance\nAs shown in Figure Figure 10.1, it starts by evaluating the performance of a model. This can be done with a variety of tools, in the DALEX package the default is to use the DALEX::model_performance function. Since the explain function checks what type of task is being analysed, it can select the appropriate performance measures for it. In our illustration, we have a multi-label classification, so measures such as micro-aggregated F1, macro-aggregated F1 etc. are calculated in the following snippet. One of the calculated measures is cross entropy and it will be used later in the following sections.\nEach explanation can be drawn with the generic plot() function, for multi-label classification the distribution of residuals is drawn by default.\n\nperf_penguin = model_performance(ranger_exp)\nperf_penguin\n\nMeasures for:  multiclass\nmicro_F1   : 1 \nmacro_F1   : 1 \nw_macro_F1 : 1 \naccuracy   : 1 \nw_macro_auc: 1 \ncross_entro: 6.034954\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n0.0000000000 0.0005846154 0.0036863492 0.0111489133 0.0315985873 0.0440341048 \n         60%          70%          80%          90%         100% \n0.0535907937 0.0683762754 0.0956176783 0.2191798413 0.5380321429 \n\nlibrary(\"ggplot2\")\nold_theme = set_theme_dalex(\"ema\") \nplot(perf_penguin)\n\n\n\n\n\n\n\nThe task of classifying the penguin species is rather easy, which is why there are so many values of 1 in the performance assessment of this model.\n\n10.3.2.2 Permutational Variable Importance\nA popular technique for assessing variable importance in a model-agnostic manner is the permutation variable importance. It is based on the difference (or ratio) in the selected loss function after the selected variable or set of variables has been permuted. Read more about this technique in Variable-importance Measures chapter.\nThe DALEX::model_parts() function calculates the importance of variables and its results can be visualized with the generic plot() function.\n\nranger_effect = model_parts(ranger_exp)\nhead(ranger_effect)\n\n       variable mean_dropout_loss           label\n1  _full_model_          6.034954 Ranger Penguins\n2          year          5.988560 Ranger Penguins\n3       species          6.034954 Ranger Penguins\n4           sex          7.002289 Ranger Penguins\n5   body_mass_g         12.377824 Ranger Penguins\n6 bill_depth_mm         15.617252 Ranger Penguins\n\nplot(ranger_effect, show_boxplots = FALSE) \n\n\n\n\n\n\n\nThe bars start in loss (here cross-entropy loss) for the selected data and end in a loss for the data after the permutation of the selected variable. The more important the variable, the more the model will lose after its permutation.\n\n10.3.2.3 Partial Dependence\nOnce we know which variables are most important, we can use Partial Dependence Plots to show how the model, on average, changes with changes in selected variables.\nThe DALEX::model_profile() function calculates the partial dependence profiles. The type argument of this function also allows Marginal profiles and Accumulated Local profiles to be calculated. Again, the result of the explanation can be model_profile with the generic function plot().\n\nranger_profiles = model_profile(ranger_exp)\nranger_profiles\n\nTop profiles    : \n        _vname_                   _label_    _x_    _yhat_ _ids_\n1 bill_depth_mm    Ranger Penguins.Adelie 13.500 0.2839077     0\n2 bill_depth_mm Ranger Penguins.Chinstrap 13.500 0.1908264     0\n3 bill_depth_mm    Ranger Penguins.Gentoo 13.500 0.5252659     0\n4 bill_depth_mm    Ranger Penguins.Adelie 13.566 0.2839077     0\n5 bill_depth_mm Ranger Penguins.Chinstrap 13.566 0.1908264     0\n6 bill_depth_mm    Ranger Penguins.Gentoo 13.566 0.5252659     0\n\nplot(ranger_profiles) + \n  theme(legend.position = \"top\") + \n  ggtitle(\"Partial Dependence for Penguins\",\"\")\n\n\n\n\n\n\n\nFor the multi-label classification model, profiles are drawn for each class separately by indicating them with different colours. We already know which variable is the most important, so now we can read how the model result changes with the change of this variable. In our example, based on bill_length_mm we can separate Adelie from Chinstrap and based on flipper_length_mm we can separate Adelie from Gentoo.\n\n10.3.3 Local level explanation\nThe local model analysis aims to understand how a model behaves for a single observation. In the DALEX package, functions for local analysis have names starting with the prefix predict_.\nWe will carry out the following examples using Steve the penguin of the Adelie species as an example.\n\nsteve = penguins[1,]\nsteve\n\n# A tibble: 1 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\n10.3.3.1 Model Prediction\nAs shown in Figure Figure 10.1, the local analysis starts with the calculation of a model prediction.\nFor Steve, the species was correctly predicted as Adelie with high probability.\n\npredict(ranger_exp, steve)\n\n        Adelie   Chinstrap Gentoo\n[1,] 0.9900897 0.009910317      0\n\n\n\n10.3.3.2 Break Down\nA popular technique for assessing the contributions of variables to model prediction is Break Down (see Introduction to Break Down chapter for more information about this method).\nThe function DALEX::predict_parts() function calculates the attributions of variables and its results can be visualized with the generic plot() function.\n\nranger_attributions = predict_parts(ranger_exp, new_observation = steve)\nplot(ranger_attributions) + ggtitle(\"Break Down for Steve\") \n\n\n\n\n\n\n\nLooking at the plots above, we can read that the biggest contributors to the final prediction were for Steve the variables bill length and flipper.\n\n10.3.3.3 Shapley Values\nBy far the most popular technique for local model exploration (Holzinger et al. 2022) is Shapley values and the most popular algorithm for estimating these values is the SHAP algorithm. Find a detailed description of the method and algorithm in the chapter SHapley Additive exPlanations (SHAP).\nThe function DALEX::predict_parts() calculates SHAP attributions, you just need to set type = \"shap\". Its results can be visualized with a generic plot() function.\n\nranger_shap = predict_parts(ranger_exp, new_observation = steve, \n             type = \"shap\")\nplot(ranger_shap, show_boxplots = FALSE) + \n             ggtitle(\"Shapley values for Steve\", \"\") \n\n\n\n\n\n\n\nThe results for Break Down and SHAP methods are generally similar. Differences will emerge if there are many complex interactions in the model.\n\n10.3.3.4 Ceteris Paribus\nIn the previous section, we’ve introduced a global explanation – Partial Dependence plots. Ceteris Paribus plots are the local level version of that plot. Read more about this technique in the chapter Ceteris Paribus and note that these profiles are also called Individual Conditional Expectations (ICE). They show the response of a model when only one variable is changed while others stay unchanged.\nThe function DALEX::predict_profile() calculates Ceteris paribus profiles which can be visualized with the generic plot() function.\n\nranger_ceteris = predict_profile(ranger_exp, steve)\nplot(ranger_ceteris) + ggtitle(\"Ceteris paribus for Steve\", \" \") \n\n\n\n\n\n\n\nBlue dot stands for the prediction for Steve. Only a big change in bill length could convince the model of Steve’s different species."
  },
  {
    "objectID": "interpretation.html#exercises",
    "href": "interpretation.html#exercises",
    "title": "10  Model Interpretation",
    "section": "\n10.4 Exercises",
    "text": "10.4 Exercises\nModel explanation allows us to confront our expert knowledge related to the problem with relations learned by the model. Following tasks are based on predictions of the value of football players based on data from the FIFA game. It is a graceful example, as most people have some intuition about how a footballer’s age or skill can affect their value. The latest FIFA statistics can be downloaded from kaggle.com, but also one can use the 2020 data avaliable in the DALEX packages(see DALEX::fifa dataset). The following exercises can be performed in both the iml and DALEX packages and we have provided solutions for both.\n\nPrepare a mlr3 regression task for fifa data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. regr.ranger.\nUse the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?\nUse the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?\n\n4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices.\n\nFor the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?\nFor the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectatons profiles to draw the local behaviour of the model for this variable. Is it different from the global behaviour?\n\n\n\n\n\n\n\nBaniecki, Hubert, and Przemyslaw Biecek. 2019. “modelStudio: Interactive Studio with Explanations for ML Predictive Models.” Journal of Open Source Software 4 (43): 1798. https://doi.org/10.21105/joss.01798.\n\n\nBaniecki, Hubert, Wojciech Kretowicz, Piotr Piątyszek, Jakub Wiśniewski, and Przemysław Biecek. 2021. “dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python.” Journal of Machine Learning Research 22 (214): 1–7. http://jmlr.org/papers/v22/20-1473.html.\n\n\nBiecek, Przemyslaw. 2018. “DALEX: Explainers for complex predictive models in R.” Journal of Machine Learning Research 19 (84): 1–5. http://jmlr.org/papers/v19/18-416.html.\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBücker, Michael, Gero Szepannek, Alicja Gosiewska, and Przemyslaw Biecek. 2022. “Transparency, Auditability, and Explainability of Machine Learning Models in Credit Scoring.” Journal of the Operational Research Society 73 (1): 70–90. https://doi.org/10.1080/01605682.2021.1922098.\n\n\nHolzinger, Andreas, Anna Saranti, Christoph Molnar, Przemyslaw Biecek, and Wojciech Samek. 2022. “Explainable AI Methods - a Brief Overview.” International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers, 13–38. https://doi.org/10.1007/978-3-031-04083-2_2.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nKrzyziński, Mateusz, Mikołaj Spytek, Hubert Baniecki, and Przemysław Biecek. 2023. “SurvSHAP(t): Time-dependent explanations of machine learning survival models.” Knowledge-Based Systems 262: 110234. https://doi.org/https://doi.org/10.1016/j.knosys.2022.110234.\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. New York, NY: Crown Publishing Group.\n\n\nRomaszko, Kamil, Magda Tatarynowicz, Mateusz Urbański, and Przemysław Biecek. 2019. “modelDown: Automated Website Generator with Interpretable Documentation for Predictive Machine Learning Models.” Journal of Open Source Software 4 (38): 1444. https://doi.org/10.21105/joss.01444.\n\n\nWiśniewski, Jakub, and Przemysław Biecek. 2022. “The r Journal: Fairmodels: A Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models.” The R Journal 14: 227–43. https://doi.org/10.32614/RJ-2022-019."
  },
  {
    "objectID": "extending.html#sec-extending-learners",
    "href": "extending.html#sec-extending-learners",
    "title": "11  Extending",
    "section": "\n11.1 Adding new Learners",
    "text": "11.1 Adding new Learners\nAlthough many learners are already included in the mlr3 ecosystem, there might be a situation in which your algorithm of choice is not implemented. Here, we show how to create a custom mlr3learner step-by-step using mlr3extralearners::create_learner. If you intend to add a learner to mlr3extralearners, it is strongly recommended to first open a learner request issue to inform the mlr3 team about your idea. This allows to discuss the implementation details and potential peculiarities of the learner before putting actual work in.\nThis section gives insights on how a mlr3learner is constructed and how to troubleshoot issues. See the Learner FAQ subsection for help.\nSummary of steps for adding a new learner\n\nCheck that the learner does not already exist here.\n\nInstall mlr3extralearners and if you want to create a PR, also fork and clone it.\n\nRun mlr3extralearners::create_learner().\nAdd the learner’s ParamSet.\nManually add .train and .predict private methods to the learner.\nImplement supported optional extractors and hotstarting if applicable.\nFill out the missing parts in the learner’s description. To add references you first need to create an entry in bibentries.R\nCheck that unit tests and parameter tests pass (these are automatically created).\nRun cleaning functions.\nOpen a pull request with the “new learner” template.\n\n\n\n\n\n\n\nWarning\n\n\n\nDo not copy/paste the code shown in this section. Use create_learner() to start.\n\n\n\n11.1.1 Setting-up mlr3extralearners\nIn order to use mlr3extralearners::create_learner you must have mlr3extralearners installed. Note that mlr3extralearners is not on CRAN and has to be installed from GitHub. To ensure that you have the latest version, run remotes::install_github(\"mlr-org/mlr3extralearners\") before proceeding.\nIf you want to create a pull request to mlr3extralearners you also need to\n\n\nFork the repository\n\n\nClone a local copy of your forked repository.\n\n11.1.2 Calling create_learner\nThe learner classif.rpart will be used as a running example throughout this section.\n\nlibrary(\"mlr3extralearners\")\ncreate_learner(\n  path = \"path/to/a/folder\",\n  classname = \"Rpart\",\n  type = \"classif\",\n  key = \"rpart\",\n  algorithm = \"Decision Tree\",\n  package = \"rpart\",\n  caller = \"rpart\",\n  feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\"),\n  predict_types = c(\"response\", \"prob\"),\n  properties = c(\"importance\", \"missings\", \"multiclass\", \"twoclass\", \"weights\"),\n  gh_name = \"RaphaelS1\",\n  label = \"Regression and Partition Tree\",\n  data_formats = \"data.table\"\n)\n\nThe full documentation for the function arguments is in mlr3extralearners::create_learner, in this example we are doing the following:\n\n\npath = \"path/to/a/folder\" - This determines where the templates are being generated. If the path is the root of an R package, the learner is created in the ./R directory and the test files in ./tests/testthat. Otherwise, all files are being created in the folder pointed to by path. Already existing files will not be modified.\n\nclassname = \"Rpart\" - Set the R6 class name to LearnerClassifRpart (classif is below)\n\nalgorithm = \"Decision Tree\" - Create the title as “Classification Decision Tree Learner”, where “Classification” is determined automatically from type and “Learner” is added for all learners.\n\ntype = \"classif\" - Setting the learner as a classification learner, automatically filling the title, class name, id (\"classif.rpart\") and task type.\n\nkey = \"rpart\" - Used with type to create the unique ID of the learner, \"classif.rpart\".\n\npackage = \"rpart\" - Setting the package from which the learner is implemented, this fills in things like the training function (along with caller) and the man field.\n\ncaller = \"rpart\" - This tells the .train function, and the description which function is called to run the algorithm, with package this automatically fills rpart::rpart.\n\nfeature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\") - Sets the type of features that can be handled by the learner. See meta information.\n\npredict_types = c(\"response\", \"prob\"), - Sets the available prediction types as response (pointwise prediction) and prob (probabilities). See meta information.\n\nproperties = c(\"importance\", \"missings\", \"multiclass\", \"twoclass\", \"weights\") - Sets the properties the learner supports. By including \"importance\" a public method called importance will be created that must be manually filled. See meta information and optional extractors.\n\ngh_name = \"RaphaelS1\" - Fills the ‘@author’ tag with my GitHub handle, this is required as it identifies the maintainer of the learner.\n\nThe sections below show an exemplary execution of mlr3extralearners::create_learner.\n\n11.1.3 learner_package_type_key.R\n\nThe first generated file which must be updated after running create_learner() is the one following the structure learner_<package>_<type>_<key>.R; in this example learner_rpart_classif_rpart.R.\nFor our example, the resulting script looks like this:\n\n#' @title Classification Decision Tree Learner\n#' @author RaphaelS1\n#' @name mlr_learners_classif.rpart\n#'\n#' @description\n#' FIXME: BRIEF DESCRIPTION OF THE LEARNER.\n#' Calls [rpart::rpart()] from FIXME: (CRAN VS NO CRAN): \\CRANpkg{rpart} | 'rpart'.\n#'\n#' @section Custom mlr3 parameters:\n#' FIXME: DEVIATIONS FROM UPSTREAM PARAMETERS. DELETE IF NOT APPLICABLE.\n#'\n#' @section Custom mlr3 defaults:\n#' FIXME: DEVIATIONS FROM UPSTREAM DEFAULTS. DELETE IF NOT APPLICABLE.\n#'\n#' @section Installation:\n#' FIXME: CUSTOM INSTALLATION INSTRUCTIONS. DELETE IF NOT APPLICABLE.\n#'\n#' @templateVar id classif.rpart\n#' @template learner\n#'\n#' @references\n#' FIXME: ADD REFERENCES\n#'\n#' @template seealso_learner\n#' @template example\n#' @export\nLearnerClassifRpart = R6Class(\"LearnerClassifRpart\",\n  inherit = LearnerClassif,\n  public = list(\n    #' @description\n    #' Creates a new instance of this [R6][R6::R6Class] class.\n    initialize = function() {\n      # FIXME: MANUALLY ADD PARAMETERS BELOW AND THEN DELETE THIS LINE\n      param_set = ps()\n\n      # FIXME: MANUALLY UPDATE PARAM VALUES BELOW IF APPLICABLE THEN DELETE THIS LINE.\n      param_set$values = list()\n\n      super$initialize(\n        id = \"classif.rpart\",\n        packages = \"rpart\",\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\"),\n        predict_types = c(\"response\", \"prob\"),\n        param_set = param_set,\n        properties = c(\"importance\", \"missings\", \"multiclass\", \"twoclass\", \"weights\"),\n        man = \"mlr3extralearners::mlr_learners_classif.rpart\",\n        label = \"Regression and Partition Tree\"\n      )\n    },\n    # FIXME: ADD IMPORTANCE METHOD IF APPLICABLE AND DELETE OTHERWISE\n    # SEE mlr3extralearners::LearnerRegrRandomForest FOR AN EXAMPLE\n    #' @description\n    #' The importance scores are extracted from the slot FIXME:.\n    #' @return Named `numeric()`.\n    importance = function() {\n      pars = self$param_set$get_values(tags = \"importance\")\n      # FIXME: Implement importance\n    }\n  ),\n  private = list(\n    .train = function(task) {\n      # get parameters for training\n      pars = self$param_set$get_values(tags = \"train\")\n\n      # FIXME: IF LEARNER DOES NOT HAVE 'weights' PROPERTY THEN DELETE THESE LINES.\n      if (\"weights\" %in% task$properties) {\n        # Add weights to learner\n      }\n\n      # FIXME: CREATE OBJECTS FOR THE TRAIN CALL\n      # AT LEAST \"data\" AND \"formula\" ARE REQUIRED\n      formula = task$formula()\n      data = task$data()\n\n      # FIXME: HERE IS SPACE FOR SOME CUSTOM ADJUSTMENTS BEFORE PROCEEDING TO THE\n      # TRAIN CALL. CHECK OTHER LEARNERS FOR WHAT CAN BE DONE HERE\n      # USE THE mlr3misc::invoke FUNCTION (IT'S SIMILAR TO do.call())\n\n      invoke(\n        rpart::rpart,\n        formula = formula,\n        data = data,\n        .args = pars\n      )\n    },\n    .predict = function(task) {\n      # get parameters with tag \"predict\"\n      pars = self$param_set$get_values(tags = \"predict\")\n\n      # get newdata and ensure same ordering in train and predict\n      newdata = ordered_features(task, self)\n\n      # Calculate predictions for the selected predict type.\n      type = self$predict_type\n\n      pred = invoke(predict, self$model, newdata = newdata, type = type, .args = pars)\n\n      # FIXME: ADD PREDICTIONS TO LIST BELOW\n      list()\n    }\n  )\n)\n\n.extralrns_dict$add(\"classif.rpart\", LearnerClassifRpart)\n\nNow we have to do the following (the description will be addressed later):\n\nAdd the learner’s parameters to the ParamSet.\nOptionally change default values for the parameters.\nFill in the private .train method, which takes a (filtered) Task and returns a model.\nFill in the private .predict method, which operates on the model in self$model (stored during $train()) and a (differently subsetted) Task to return a named list of predictions.\nAs we included “importance” in properties, we have to add the public method importance() which returns a named numeric vectors with the decreasingly sorted importance scores (values) for the different features (names).\n\n11.1.4 Meta-information\nIn the constructor (initialize()) the constructor of the super class (e.g. LearnerClassif) is called with meta information about the learner which should be constructed. This includes:\n\n\nid: The ID of the new learner. Usually consists of <type>.<key>, for example: \"classif.rpart\".\n\npackages: The upstream package name(s) of the implemented learner.\n\nparam_set: A set of hyperparameters and their descriptions provided as a paradox::ParamSet. For each hyperparameter the appropriate class needs to be chosen. When using the paradox::ps shortcut, a short constructor of the form p_*** can be used:\n\n\nparadox::ParamLgl / paradox::p_lgl for scalar logical hyperparameters.\n\nparadox::ParamInt / paradox::p_int for scalar integer hyperparameters.\n\nparadox::ParamDbl / paradox::p_dbl for scalar numeric hyperparameters.\n\nparadox::ParamFct / paradox::p_fct for scalar factor hyperparameters (this includes characters).\n\nparadox::ParamUty / paradox::p_uty for everything else (e.g. vector paramters or list parameters).\n\n\n\npredict_types: Set of predict types the learner supports. These differ depending on the type of the learner. See mlr_reflections$learner_predict_types for the full list of predict types supported by mlr3.\n\n\nLearnerClassif\n\n\nresponse: Only predicts a class label for each observation.\n\nprob: Also predicts the posterior probability for each class for each observation.\n\n\n\nLearnerRegr\n\n\nresponse: Only predicts a numeric response for each observation.\n\nse: Also predicts the standard error for each value of response.\n\n\n\nLearnerSurv\n\n\nlp - Linear predictor calculated as the fitted coefficients multiplied by the test data.\n\ndistr - Predicted survival distribution, either discrete or continuous. Implemented in distr6.\n\ncrank - Continuous risk ranking.\n\nresponse - Predicted survival time.\n\n\n\nLearnerDens\n\n\npdf- Predicts the probability density function.\n\ncdf - Predicts the cumulative distribution function.\n\ndistr - Predicts a distribution as implemented in distr6.\n\n\n\nLearnerClust\n\n\npartition - Assigns the observation to a partition.\n\nprob - Returns a probability for each partition.\n\n\n\n\n\nfeature_types: Set of feature types the learner is able to handle. See mlr_reflections$task_feature_types for feature types supported by mlr3.\n\nproperties: Set of properties of the learner. See mlr_reflections$learner_properties for the full list of learner properties supported by mlr3. The list of properties includes:\n\n\n\"twoclass\": The learner works on binary classification problems.\n\n\"multiclass\": The learner works on multi-class classification problems.\n\n\"missings\": The learner can natively handle missing values.\n\n\"weights\": The learner can work on tasks which have observation weights / case weights.\n\n\"importance\": The learner supports extracting importance values for features.\n\n\"selected_features\": The learner supports extracting the features which were selected by the model.\n\n\"oob_error\": The learner supports extracting the out of bag error.\n\n\"loglik\": The learner supports extracting the log-likelihood of the learner.\n\n\"hotstart_forward\": The learner allows to continue training the model e.g. by adding more trees to a random forest.\n\n\"hotstart_backward\": The learner allows to “undo” some of the training, e.g. by removing some trees from a model.\n\n\n\nman: The roxygen identifier of the learner. This is used within the $help() method of the super class to open the help page of the learner.\n\nlabel: The label of the learner. This should briefly describe the learner (similar to the description’s title) and is for example used for printing.\n\n11.1.5 ParamSet\nThe ParamSet is the set of hyperparameters used in model training and predicting, this is given as a paradox::ParamSet. The set consists of a list of hyperparameters, where each has a specific class for the hyperparameter type (see above). In addition, each parameter has one or more tags, that determine in which method they are used.\nBeyond that there are other tags that serve specific purposes:\n\nThe tag \"threads\" should be used (if applicable) to tag the parameter that determines the number of threads used for the learner’s internal parallelization. This parameter can be set using set_threads.\nThe tag \"required\" should be used to tag parameters that must be provided for the algorithm to be executable.\nIn case optional extractors are available, the can (although this is rarely the case) also have parameters and can be tagged accordingly.\nIf hotstarting is available, the fidelity parameter should be tagged with \"hotstart\".\n\nFor classif.rpart the param_set can be defined as follows\n\nparam_set = ps(\n  minsplit = p_int(lower = 1L, default = 20L, tags = \"train\"),\n  minbucket = p_int(lower = 1L, tags = \"train\"),\n  cp = p_dbl(lower = 0, upper = 1, default = 0.01, tags = \"train\"),\n  maxcompete = p_int(lower = 0L, default = 4L, tags = \"train\"),\n  maxsurrogate = p_int(lower = 0L, default = 5L, tags = \"train\"),\n  maxdepth = p_int(lower = 1L, upper = 30L, default = 30L, tags = \"train\"),\n  usesurrogate = p_int(lower = 0L, upper = 2L, default = 2L, tags = \"train\"),\n  surrogatestyle = p_int(lower = 0L, upper = 1L, default = 0L, tags = \"train\"),\n  xval = p_int(lower = 0L, default = 0L, tags = \"train\"),\n  keep_model = p_lgl(default = FALSE, tags = \"train\")\n)\nparam_set$values = list(xval = 0L)\n\nWithin mlr3 packages we suggest to stick to the shorthand notation above for consistency, however the param_set can be written with the underlying R6 classes as shown here\n\nparam_set = ParamSet$new(list(\n  ParamInt$new(id = \"minsplit\", default = 20L, lower = 1L, tags = \"train\"),\n  ParamInt$new(id = \"minbucket\", lower = 1L, tags = \"train\"),\n  ParamDbl$new(id = \"cp\", default = 0.01, lower = 0, upper = 1, tags = \"train\"),\n  ParamInt$new(id = \"maxcompete\", default = 4L, lower = 0L, tags = \"train\"),\n  ParamInt$new(id = \"maxsurrogate\", default = 5L, lower = 0L, tags = \"train\"),\n  ParamInt$new(id = \"maxdepth\", default = 30L, lower = 1L, upper = 30L, tags = \"train\"),\n  ParamInt$new(id = \"usesurrogate\", default = 2L, lower = 0L, upper = 2L, tags = \"train\"),\n  ParamInt$new(id = \"surrogatestyle\", default = 0L, lower = 0L, upper = 1L, tags = \"train\"),\n  ParamInt$new(id = \"xval\", default = 0L, lower = 0L, tags = \"train\"),\n  ParamLgl$new(id = \"keep_model\", default = FALSE, tags = \"train\")\n))\nparam_set$values = list(xval = 0L)\n\nYou should read though the learner documentation to find the full list of available parameters. Just looking at some of these in this example:\n\n\n\"cp\" is numeric, has a feasible range of [0,1] and defaults to 0.01. The parameter is used during \"train\".\n\n\"xval\" is integer has a lower bound of 0, a default of 0 and the parameter is used during \"train\".\n\n\"keep_model\" is logical with a default of FALSE and is used during \"train\".\n\nIn some rare cases you may want to change the default parameter values. You can do this by changing the param_set$values. You can see we have done this for \"classif.rpart\" where the default for xval is changed to 0. Note that the default in the ParamSet is recorded as our changed default (0), and not the original (10). It is strongly recommended to only change the defaults if absolutely required, when this is the case add the following to the learner documentation:\n#' @section Custom mlr3 defaults:\n#' - `<parameter>`:\n#'   - Actual default: <value>\n#'   - Adjusted default: <value>\n#'   - Reason for change: <text>\n\n11.1.6 Train function\nThe train function takes a Task as input and must return a model. Let’s say we want to translate the following call of rpart::rpart() into code that can be used inside the .train() method.\nFirst, we write something down that works completely without mlr3:\n\ndata = iris\nmodel = rpart::rpart(Species ~ ., data = iris, xval = 0)\n\nWe need to pass the formula notation Species ~ ., the data and the hyperparameters. To get the hyperparameters, we call self$param_set$get_values(tag = \"train\") and thereby query all parameters that are using during \"train\". Then, the dataset is extracted from the Task. Because the learner has the property \"weights\", we insert the weights of the task if there are any.\nLast, we call the upstream function rpart::rpart() with the data and pass all hyperparameters via argument .args using the mlr3misc::invoke() function. The latter is simply an optimized version of do.call() that we use within the mlr3 ecosystem.\n\n.train = function(task) {\n  pars = self$param_set$get_values(tags = \"train\")\n  if (\"weights\" %in% task$properties) {\n    pars$weights = task$weights$weight\n  }\n  formula = task$formula()\n  data = task$data()\n  invoke(\n    rpart::rpart,\n    formula = formula,\n    data = data,\n    .args = pars\n  )\n}\n\n\n11.1.7 Predict function\nThe internal predict method .predict() also operates on a Task as well as on the fitted model that has been created by the train() call previously and has been stored in self$model.\nThe return value is a Prediction object. We proceed analogously to what we did in the previous section. We start with a version without any mlr3 objects and continue to replace objects until we have reached the desired interface:\n\n# inputs:\ntask = tsk(\"iris\")\nself = list(model = rpart::rpart(task$formula(), data = task$data()))\n\ndata = iris\nresponse = predict(self$model, newdata = data, type = \"class\")\nprob = predict(self$model, newdata = data, type = \"prob\")\n\nThe \"rpart::predict.rpart()\" function predicts class labels if argument type is set to to \"class\", and class probabilities if set to \"prob\".\nNext, we transition from data to a Task again and construct a list with the return type requested by the user, this is stored in the $predict_type slot of a learner class. Note that the Task is automatically passed to the prediction object, so all you need to do is return the predictions! Make sure the list names are identical to the task predict types.\nThe final .predict() method is below, we could omit the pars line as there are no parameters with the \"predict\" tag but we keep it here to be consistent:\n\n.predict = function(task) {\n  pars = self$param_set$get_values(tags = \"predict\")\n  # get newdata and ensure same ordering in train and predict\n  newdata = ordered_features(task, self)\n  if (self$predict_type == \"response\") {\n    response = invoke(\n      predict,\n      self$model,\n      newdata = newdata,\n      type = \"class\",\n      .args = pars\n    )\n\n    return(list(response = response))\n  } else {\n    prob = invoke(\n      predict,\n      self$model,\n      newdata = newdata,\n      type = \"prob\",\n      .args = pars\n    )\n    return(list(prob = prob))\n  }\n}\n\n\n\n\n\n\n\nWarning\n\n\n\nYou cannot rely on the column order of the data returned by task$data() as the order of columns may be different from the order of the columns during $.train. The newdata line ensures the ordering is the same by calling the same order as in train!\n\n\n\n11.1.8 Optional Extractors\nSpecific learner implementations are free to implement additional getters to ease the access of certain parts of the model in the inherited subclasses. The blueprint for these methods is only included in the generated learner template, if the property is set when calling create_learner(). The comments in the templates will include references to other learners that have this property and can be used as guiding examples. To determine whether these methods are applicable, one has to determine whether the learner supports this method in principle and whether it is implemented in the upstream package.\nFor the following operations, extractors are standardized:\n\n\nimportance(...) - Returns the feature importance score as numeric vector. The higher the score, the more important the variable. The returned vector is named with feature names and sorted in decreasing order. Note that the model might omit features it has not used at all. The learner must be tagged with property \"importance\".\n\nselected_features(...) - Returns a subset of selected features as character(). The learner must be tagged with property \"selected_features\".\n\noob_error(...) - Returns the out-of-bag error of the model as numeric(1). The learner must be tagged with property \"oob_error\".\n\nloglik(...) - Extracts the log-likelihood (c.f. stats::logLik()). The learner must be tagged with property \"loglik\".\n\nIn this case, we only have to implement the importance() method.\n\nimportance = function() {\n  if (is.null(self$model)) {\n    stopf(\"No model stored\")\n  }\n\n  importance = sort(self$model$variable.importance, decreasing = TRUE)\n  if (is.null(importance)) {\n    importance = mlr3misc::set_names(numeric())\n  }\n  return(importance)\n}\n\n\n11.1.9 Hotstarting\nSome learners support resuming or continuing from an already fitted model. We assume that hotstarting is only possible if a single hyperparameter (also called the fidelity parameter, usually controlling the complexity or expensiveness) is altered and all other hyperparameters are identical. The fidelity parameters should be tagged with \"hotstart\". Examples are:\n\nRandom Forest: Starting from a model with n trees, a random forest with n + k trees can be obtained by adding k trees (\"hotstart_forward\") and a random forest with n - k trees can be obtained by removing k trees (\"hotstart_backward\").\nGradient Boosting: When having fitted a model with n iterations, we only need k iterations to obtain a model with n + k iterations. (\"hotstart_forward\").\n\nFor more information see HotstartStack.\n\n11.1.10 Control objects/functions of learners\nSome learners rely on a “control” object/function such as glmnet::glmnet.control(). Accounting for such depends on how the underlying package works:\n\nIf the package forwards the control parameters via ... and makes it possible to just pass control parameters as additional parameters directly to the train call, there is no need to distinguish both \"train\" and \"control\" parameters.\nIf the control parameters need to be passed via a separate argument, one can e.g. use formalArgs(glmnet::glmnet.control) to get the names of the control parameters and then extract them from the pars like shown below.\n\n\npars = self$param_set$get_values(tags = \"train\")\nii = names(pars) %in% formalArgs(glmnet::glmnet.control)\npars_ctrl = pars[ii]\npars_train = pars[!ii]\n\ninvoke([...], .args = pars_train, control = pars_ctrl)\n\n\n11.1.11 Adding the description\nOnce the learner is implemented - and is not only intended for personal use - it’s description should be filled out. Most steps should be clear from the instructions given in the template. For the section ‘@references’, the entry first has to be added to the file bibentries.R, essentially by converting the bibtex file to a R bibentry function call.\n\n11.1.12 Testing the learner\nOnce your learner is created, you are ready to start testing if it works, there are three types of tests: manual, unit and parameter.\n\n11.1.12.1 Train and Predict\nFor a bare-bone check you can just try to run a simple train() call locally.\n\ntask = tsk(\"iris\") # assuming a Classif learner\nlrn = lrn(\"classif.rpart\")\nlrn$train(task)\np = lrn$predict(task)\np$confusion\n\nIf it runs without erroring, that’s a very good start!\n\n11.1.12.2 Autotest\nTo ensure that your learner is able to handle all kinds of different properties and feature types, we have written an “autotest” that checks the learner for different combinations of such.\nThe “autotest” setup is generated automatically by create_learner(). It will have a name with the form test_package_type_key.R, in our case this will actually be test_rpart_classif_rpart.R. This will create the following script, for which no changes are required to pass (assuming the learner was correctly created):\n\ntest_that(\"autotest\", {\n  learner = lrn(\"classif.rpart\")\n  expect_learner(learner)\n  # note that you can skip tests using the exclude argument\n  result = run_autotest(learner)\n  expect_true(result, info = result$error)\n})\n\nFor some learners that have required parameters, it is needed to set some values for required parameters after construction so that the learner can be run in the first place.\nYou can also exclude some specific test arrangements within the “autotest” via the argument exclude in the run_autotest() function. Currently the run_autotest() function lives in inst/testthat of the mlr3 and still lacks documentation. This should change in the near future.\nTo finally run the test suite, call devtools::test() or hit CTRL + Shift + T if you are using RStudio.\n\n11.1.12.3 Checking Parameters\nSome learners have a high number of parameters and it is easy to miss out on some during the creation of a new learner. In addition, if the maintainer of the upstream package changes something with respect to the arguments of the algorithm, the learner is in danger to break. Also, new arguments could be added upstream and manually checking for new additions all the time is tedious.\nTherefore we have written a “Parameter Check” that runs regularly for every learner. This “Parameter Check” compares the parameters of the mlr3 ParamSet against all arguments available in the upstream function that is called during $train() and $predict(). Again the file is automatically created by create_learner(). This will be named like test_paramtest_package_type_key.R, so in our example test_paramtest_rpart_classif_rpart.R. When the .train function calls multiple functions (e.g. a control function as described above), a list of functions can be passed to the parameter test.\nThe test comes with an exclude argument that should be used to exclude and explain why certain arguments of the upstream function are not within the ParamSet of the mlr3learner. This will likely be required for all learners as common arguments like x, target or data are handled by the mlr3 interface and are therefore not included within the ParamSet.\nHowever, there might be more parameters that need to be excluded, for example:\n\nType dependent parameters, i.e. parameters that only apply for classification or regression learners.\nParameters that are actually deprecated by the upstream package and which were therefore not included in the mlr3 ParamSet.\n\nAll excluded parameters should have a comment justifying their exclusion.\nIn our example, the final paramtest script looks like:\n\ntest_that(\"classif.rpart train\", {\n  learner = lrn(\"classif.rpart\")\n  # this can also be a list of functions\n  fun = rpart::rpart\n  exclude = c(\n    \"formula\", # handled internally\n    \"model\", # handled internally\n    \"data\", # handled internally\n    \"weights\", # handled by task\n    \"subset\", # handled by task\n    \"na.action\", # handled internally\n    \"method\", # handled internally\n    \"x\", # handled internally\n    \"y\", # handled internally\n    \"parms\", # handled internally\n    \"control\", # handled internally\n    \"cost\" # handled internally\n  )\n\n  paramtest = run_paramtest(learner, fun, exclude, tag = \"train\")\n  expect_paramtest(paramtest)\n})\n\ntest_that(\"classif.rpart predict\", {\n  learner = lrn(\"classif.rpart\")\n  fun = rpart:::predict.rpart\n  exclude = c(\n    \"object\", # handled internally\n    \"newdata\", # handled internally\n    \"type\", # handled internally\n    \"na.action\" # handled internally\n  )\n\n  paramtest = run_paramtest(learner, fun, exclude, tag = \"predict\")\n  expect_paramtest(paramtest)\n})\n\n\n11.1.13 Package Cleaning\nOnce all tests are passing, run the following functions to ensure that the package remains clean and tidy\n\ndevtools::document(roclets = c('rd', 'collate', 'namespace'))\nIf you haven’t done this before run: remotes::install_github('mlr-org/styler.mlr')\n\nstyler::style_pkg(style = styler.mlr::mlr_style)\nusethis::use_tidy_description()\nlintr::lint_package()\n\nPlease fix any errors indicated by lintr before creating a pull request. Finally ensure that all FIXME are resolved and deleted in the generated files.\nYou are now ready to add your learner to the mlr3 ecosystem! Simply open a pull request to with the new learner template and complete the checklist in there. Creating this request will trigger an automated workflow that checks whether various conditions (such as rcmdcheck::rcmdcheck()) are satisfied. Once the pull request is approved and merged, your learner will automatically appear on the package website.\n\n11.1.14 Thanks and Maintenance\nThank you for contributing to the mlr3 ecosystem!\nWhen you created the learner you would have given your GitHub handle, meaning that you are now listed as the learner author and maintainer. This means that if the learner breaks it is your responsibility to fix the learner - you can view the status of your learner here.\n\n11.1.15 Learner FAQ\nQuestion 1\nHow to deal with Parameters which have no default?\nAnswer\nIf the learner does not work without providing a value, set a reasonable default in param_set$values, add tag \"required\" to the parameter and document your default properly.\nQuestion 2\nWhere to add the package of the upstream package in the DESCRIPTION file?\nAdd it to the “Suggests” section.\nQuestion 3\nHow to handle arguments from external “control” functions such as glmnet::glmnet_control()?\nAnswer\nSee “Control objects/functions of learners”.\nQuestion 4\nHow to document if my learner uses a custom default value that differs to the default of the upstream package?\nAnswer\nIf you set a custom default for the mlr3learner that does not cope with the one of the upstream package (think twice if this is really needed!), add this information to the help page of the respective learner.\nYou can use the following skeleton for this:\n#' @section Custom mlr3 defaults:\n#' * `<parameter>`:\n#'   * Actual default: <value>\n#'   * Adjusted default: <value>\n#'   * Reason for change: <text>\nQuestion 5\nWhen should the \"required\" tag be used when defining Params and what is its purpose?\nAnswer\nThe \"required\" tag should be used when the following conditions are met:\n\nThe upstream function cannot be run without setting this parameter, i.e. it would throw an error.\nThe parameter has no default in the upstream function.\n\nIn mlr3 we follow the principle that every learner should be constructable without setting custom parameters. Therefore, if a parameter has no default in the upstream function, a custom value is usually set for this parameter in the mlr3learner (remember to document such changes in the help page of the learner).\nEven though this practice ensures that no parameter is unset in an mlr3learner and partially removes the usefulness of the \"required\" tag, the tag is still useful in the following scenario:\nIf a user sets custom parameters after construction of the learner\nlrn = lrn(\"<id>\")\nlrn$param_set$values = list(\"<param>\" = <value>)\nHere, all parameters besides the ones set in the list would be unset. See paradox::ParamSet for more information. If a parameter is tagged as \"required\" in the ParamSet, the call above would error and prompt the user that required parameters are missing.\nQuestion 6\nWhat is this error when I run devtools::load_all()\n> devtools::load_all(\".\")\nLoading mlr3extralearners\nWarning message:\n.onUnload failed in unloadNamespace() for 'mlr3extralearners', details:\n  call: vapply(hooks, function(x) environment(x)$pkgname, NA_character_)\n  error: values must be length 1,\n but FUN(X[[1]]) result is length 0\nAnswer\nThis is not an error but a warning and you can safely ignore it!"
  },
  {
    "objectID": "extending.html#extending-measures",
    "href": "extending.html#extending-measures",
    "title": "11  Extending",
    "section": "\n11.2 Adding new Measures",
    "text": "11.2 Adding new Measures\nIn this section we showcase how to implement a custom performance measure.\nA good starting point is writing down the loss function independently of mlr3 (we also did this in the mlr3measures package). Here, we illustrate writing measure by implementing the root of the mean squared error for regression problems:\n\nroot_mse = function(truth, response) {\n  mse = mean((truth - response)^2)\n  sqrt(mse)\n}\n\nroot_mse(c(0, 0.5, 1), c(0.5, 0.5, 0.5))\n\n[1] 0.4082483\n\n\nIn the next step, we embed the root_mse() function into a new R6 class inheriting from base classes MeasureRegr/Measure. For classification measures, use MeasureClassif. We keep it simple here and only explain the most important parts of the Measure class:\n\nMeasureRootMSE = R6::R6Class(\"MeasureRootMSE\",\n  inherit = mlr3::MeasureRegr,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        # custom id for the measure\n        id = \"root_mse\",\n\n        # additional packages required to calculate this measure\n        packages = character(),\n\n        # properties, see below\n        properties = character(),\n\n        # required predict type of the learner\n        predict_type = \"response\",\n\n        # feasible range of values\n        range = c(0, Inf),\n\n        # minimize during tuning?\n        minimize = TRUE\n      )\n    }\n  ),\n\n  private = list(\n    # custom scoring function operating on the prediction object\n    .score = function(prediction, ...) {\n      root_mse = function(truth, response) {\n        mse = mean((truth - response)^2)\n        sqrt(mse)\n      }\n\n      root_mse(prediction$truth, prediction$response)\n    }\n  )\n)\n\nThis class can be used as template for most performance measures. If something is missing, you might want to consider having a deeper dive into the following arguments:\n\n\nproperties: If you tag you measure with the property \"requires_task\", the Task is automatically passed to your .score() function (don’t forget to add the argument Task in the signature). The same is possible with \"requires_learner\" if you need to operate on the Learner and \"requires_train_set\" if you want to access the set of training indices in the score function.\n\naggregator: This function (defaulting to mean()) controls how multiple performance scores, i.e. from different resampling iterations, are aggregated into a single numeric value if average is set to micro averaging. This is ignored for macro averaging.\n\npredict_sets: Prediction sets (subset of (\"train\", \"test\")) to operate on. Defaults to the “test” set.\n\nFinally, if you want to use your custom measure just like any other measure shipped with mlr3 and access it via the mlr_measures dictionary, you can easily add it:\n\nmlr3::mlr_measures$add(\"root_mse\", MeasureRootMSE)\n\nTypically it is a good idea to put the measure together with the call to mlr_measures$add() in a new R file and just source it in your project.\n\n## source(\"measure_root_mse.R\")\nmsr(\"root_mse\")\n\n<MeasureRootMSE:root_mse>\n* Packages: mlr3\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response"
  },
  {
    "objectID": "extending.html#extending-pipeops",
    "href": "extending.html#extending-pipeops",
    "title": "11  Extending",
    "section": "\n11.3 Adding new PipeOps",
    "text": "11.3 Adding new PipeOps\nThis section showcases how the mlr3pipelines package can be extended to include custom PipeOps. To run the following examples, we will need a Task; we are using the well-known “Iris” task:\n\nlibrary(\"mlr3\")\ntask = tsk(\"iris\")\ntask$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0\n\n\nmlr3pipelines is fundamentally built around R6. When planning to create custom PipeOp objects, it can only help to familiarize yourself with it.\nIn principle, all a PipeOp must do is inherit from the PipeOp R6 class and implement the .train() and .predict() functions. There are, however, several auxiliary subclasses that can make the creation of certain operations much easier.\n\n11.3.1 General Case Example: PipeOpCopy\n\nA very simple yet useful PipeOp is PipeOpCopy, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data. It is a simple example that showcases the important steps in defining a custom PipeOp. We will show a simplified version here, PipeOpCopyTwo, that creates exactly two copies of its input data.\nThe following figure visualizes how our PipeOp is situated in the Pipeline and the significant in- and outputs.\n\n\n\n\n\n\n11.3.1.1 First Steps: Inheriting from PipeOp\n\nThe first part of creating a custom PipeOp is inheriting from PipeOp. We make a mental note that we need to implement a .train() and a .predict() function, and that we probably want to have an initialize() as well:\n\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      ....\n    },\n  ),\n  private == list(\n    .train = function(inputs) {\n      ....\n    },\n\n    .predict = function(inputs) {\n      ....\n    }\n  )\n)\n\nNote, that private methods, e.g. .train and .predict etc are prefixed with a ..\n\n11.3.1.2 Channel Definitions\nWe need to tell the PipeOp the layout of its channels: How many there are, what their names are going to be, and what types are acceptable. This is done on initialization of the PipeOp (using a super$initialize call) by giving the input and output data.table objects. These must have three columns: a \"name\" column giving the names of input and output channels, and a \"train\" and \"predict\" column naming the class of objects we expect during training and prediction as input / output. A special value for these classes is \"*\", which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels.\nBy convention, we name a single channel \"input\" or \"output\", and a group of channels [\"input1\", \"input2\", …], unless there is a reason to give specific different names. Therefore, our input data.table will have a single row <\"input\", \"*\", \"*\">, and our output table will have two rows, <\"output1\", \"*\", \"*\"> and <\"output2\", \"*\", \"*\">.\nAll of this is given to the PipeOp creator. Our initialize() will thus look as follows:\n\ninitialize = function(id = \"copy.two\") {\n  input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\")\n  # the following will create two rows and automatically fill the `train`\n  # and `predict` cols with \"*\"\n  output = data.table::data.table(\n    name = c(\"output1\", \"output2\"),\n    train = \"*\", predict = \"*\"\n  )\n  super$initialize(id,\n    input = input,\n    output = output\n  )\n}\n\n\n11.3.1.3 Train and Predict\nBoth .train() and .predict() will receive a list as input and must give a list in return. According to our input and output definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using c(inputs, inputs).\nTwo things to consider:\n\nThe .train() function must always modify the self$state variable to something that is not NULL or NO_OP. This is because the $state slot is used as a signal that PipeOp has been trained on data, even if the state itself is not important to the PipeOp (as in our case). Therefore, our .train() will set self$state = list().\nIt is not necessary to “clone” our input or make deep copies, because we don’t modify the data. However, if we were changing a reference-passed object, for example by changing data in a Task, we would have to make a deep copy first. This is because a PipeOp may never modify its input object by reference.\n\nOur .train() and .predict() functions are now:\n\n.train = function(inputs) {\n  self$state = list()\n  c(inputs, inputs)\n}\n\n\n.predict = function(inputs) {\n  c(inputs, inputs)\n}\n\n\n11.3.1.4 Putting it Together\nThe whole definition thus becomes\n\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      super$initialize(id,\n        input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\"),\n        output = data.table::data.table(name = c(\"output1\", \"output2\"),\n                            train = \"*\", predict = \"*\")\n      )\n    }\n  ),\n  private = list(\n    .train = function(inputs) {\n      self$state = list()\n      c(inputs, inputs)\n    },\n\n    .predict = function(inputs) {\n      c(inputs, inputs)\n    }\n  )\n)\n\nWe can create an instance of our PipeOp, put it in a graph, and see what happens when we train it on something:\n\nlibrary(\"mlr3pipelines\")\npoct = PipeOpCopyTwo$new()\ngr = Graph$new()\ngr$add_pipeop(poct)\n\nprint(gr)\n\nGraph with 1 PipeOps:\n       ID         State sccssors prdcssors\n copy.two <<UNTRAINED>>                   \n\nresult = gr$train(task)\n\nstr(result)\n\nList of 2\n $ copy.two.output1:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n $ copy.two.output2:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n\n\n\n11.3.2 Special Case: Preprocessing\nMany PipeOps perform an operation on exactly one Task, and return exactly one Task. They may even not care about the “Target” / “Outcome” variable of that task, and only do some modification of some input data. However, it is usually important to them that the Task on which they perform prediction has the same data columns as the Task on which they train. For these cases, the auxiliary base class PipeOpTaskPreproc exists. It inherits from PipeOp itself, and other PipeOps should use it if they fall in the kind of use-case named above.\nWhen inheriting from PipeOpTaskPreproc, one must either implement the private methods .train_task() and .predict_task(), or the methods .train_dt(), .predict_dt(), depending on whether wants to operate on a Task object or on its data as data.tables. In the second case, one can optionally also overload the .select_cols() method, which chooses which of the incoming Task’s features are given to the .train_dt() / .predict_dt() functions.\nThe following will show two examples: PipeOpDropNA, which removes a Task’s rows with missing values during training (and implements .train_task() and .predict_task()), and PipeOpScale, which scales a Task’s numeric columns (and implements .train_dt(), .predict_dt(), and .select_cols()).\n\n11.3.2.1 Example: PipeOpDropNA\n\nDropping rows with missing values may be important when training a model that can not handle them.\nBecause mlr3 \"Task\", text = \"Tasks\") only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values. Instead, the rows can be removed using the Task’s $filter method, which modifies the Task in-place. This is done in the private method .train_task(). We take care that we also set the $state slot to signal that the PipeOp was trained.\nThe private method .predict_task() does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows. Furthermore, mlr3 expects a Learner to always return just as many predictions as it was given input rows, so a PipeOp that removes Task rows during training can not be used inside a GraphLearner.\nWhen we inherit from PipeOpTaskPreproc, it sets the input and output data.tables for us to only accept a single Task. The only thing we do during initialize() is therefore to set an id (which can optionally be changed by the user).\nThe complete PipeOpDropNA can therefore be written as follows. Note that it inherits from PipeOpTaskPreproc, unlike the PipeOpCopyTwo example from above:\n\nPipeOpDropNA = R6::R6Class(\"PipeOpDropNA\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"drop.na\") {\n      super$initialize(id)\n    }\n  ),\n\n  private = list(\n    .train_task = function(task) {\n      self$state = list()\n      featuredata = task$data(cols = task$feature_names)\n      exclude = apply(is.na(featuredata), 1, any)\n      task$filter(task$row_ids[!exclude])\n    },\n\n    .predict_task = function(task) {\n      # nothing to be done\n      task\n    }\n  )\n)\n\nTo test this PipeOp, we create a small task with missing values:\n\nsmalliris = iris[(1:5) * 30, ]\nsmalliris[1, 1] = NA\nsmalliris[2, 2] = NA\nsitask = as_task_classif(smalliris, target = \"Species\")\nprint(sitask$data())\n\n      Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:     setosa          1.6         0.2           NA         3.2\n2: versicolor          3.9         1.4          5.2          NA\n3: versicolor          4.0         1.3          5.5         2.5\n4:  virginica          5.0         1.5          6.0         2.2\n5:  virginica          5.1         1.8          5.9         3.0\n\n\nWe test this by feeding it to a new Graph that uses PipeOpDropNA.\n\ngr = Graph$new()\ngr$add_pipeop(PipeOpDropNA$new())\n\nfiltered_task = gr$train(sitask)[[1]]\nprint(filtered_task$data())\n\n      Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1: versicolor          4.0         1.3          5.5         2.5\n2:  virginica          5.0         1.5          6.0         2.2\n3:  virginica          5.1         1.8          5.9         3.0\n\n\n\n11.3.2.2 Example: PipeOpScaleAlways\n\nAn often-applied preprocessing step is to simply center and/or scale the data to mean \\(0\\) and standard deviation \\(1\\). This fits the PipeOpTaskPreproc pattern quite well. Because it always replaces all columns that it operates on, and does not require any information about the task’s target, it only needs to overload the .train_dt() and .predict_dt() functions. This saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification.\nBecause scaling only makes sense on numeric features, we want to instruct PipeOpTaskPreproc to give us only these numeric columns. We do this by overloading the .select_cols() function: It is called by the class to determine which columns to pass to .train_dt() and .predict_dt(). Its input is the Task that is being transformed, and it should return a character vector of all features to work with. When it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns. Because the levels() of the data table given to .train_dt() and .predict_dt() may be different from the Task’s levels, these functions must also take a levels argument that is a named list of column names indicating their levels. When working with numeric data, this argument can be ignored, but it should be used instead of levels(dt[[column]]) for factorial or character columns.\nThis is the first PipeOp where we will be using the $state slot for something useful: We save the centering offset and scaling coefficient and use it in $.predict()!\nFor simplicity, we are not using hyperparameters and will always scale and center all data. Compare this PipeOpScaleAlways operator to the one defined inside the mlr3pipelines package, PipeOpScale.\n\nPipeOpScaleAlways = R6::R6Class(\"PipeOpScaleAlways\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"scale.always\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .train_dt = function(dt, levels, target) {\n      sc = scale(as.matrix(dt))\n      self$state = list(\n        center = attr(sc, \"scaled:center\"),\n        scale = attr(sc, \"scaled:scale\")\n      )\n      sc\n    },\n\n    .predict_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\n\n(Note for the observant: If you check PipeOpScale.R from the mlr3pipelines package, you will notice that is uses “get(\"type\")” and “get(\"id\")” instead of “type” and “id”, because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a “problem” with data.table and not exclusive to mlr3pipelines.)\nWe can, again, create a new Graph that uses this PipeOp to test it. Compare the resulting data to the original “iris” Task data printed at the beginning:\n\ngr = Graph$new()\ngr$add_pipeop(PipeOpScaleAlways$new())\n\nresult = gr$train(task)\n\nresult[[1]]$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n\n\n\n11.3.3 Special Case: Preprocessing with Simple Train\nIt is possible to make even further simplifications for many PipeOps that perform mostly the same operation during training and prediction. The point of Task preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that may depend on training data).\nConsider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level. However, what features get removed must be decided during training, and may only depend on training data. Furthermore, the actual process of removing features is the same during training and prediction.\nA simplification to make is therefore to have a private method .get_state(task) which sets the $state slot during training, and a private method .transform(task), which gets called both during training and prediction. This is done in the PipeOpTaskPreprocSimple class. Just like PipeOpTaskPreproc, one can inherit from this and overload these functions to get a PipeOp that performs preprocessing with very little boilerplate code.\nJust like PipeOpTaskPreproc, PipeOpTaskPreprocSimple offers the possibility to instead overload the .get_state_dt(dt, levels) and .transform_dt(dt, levels) methods (and optionally, again, the .select_cols(task) function) to operate on data.table feature data instead of the whole Task.\nEven some methods that do not use PipeOpTaskPreprocSimple could work in a similar way: The PipeOpScaleAlways example from above will be shown to also work with this paradigm.\n\n11.3.3.1 Example: PipeOpDropConst\n\nA typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training. One simple example of this is dropping constant features. Because the mlr3 Task class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its $select() function, so the .get_state_dt(dt, levels) / .transform_dt(dt, levels) functions will not get used; instead we overload the .get_state(task) and .transform(task) methods.\nThe .get_state() function’s result is saved to the $state slot, so we want to return something that is useful for dropping features. We choose to save the names of all the columns that have nonzero variance. For brevity, we use length(unique(column)) > 1 to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other.\nThe .transform() method is evaluated both during training and prediction, and can rely on the $state slot being present. All it does here is call the Task$select function with the columns we chose to keep.\nThe full PipeOp could be written as follows:\n\nPipeOpDropConst = R6::R6Class(\"PipeOpDropConst\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"drop.const\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .get_state = function(task) {\n      data = task$data(cols = task$feature_names)\n      nonconst = sapply(data, function(column) length(unique(column)) > 1)\n      list(cnames = colnames(data)[nonconst])\n    },\n\n    .transform = function(task) {\n      task$select(self$state$cnames)\n    }\n  )\n)\n\nThis can be tested using the first five rows of the “Iris” Task, for which one feature (\"Petal.Width\") is constant:\n\nirishead = task$clone()$filter(1:5)\nirishead$data()\n\n   Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:  setosa          1.4         0.2          5.1         3.5\n2:  setosa          1.4         0.2          4.9         3.0\n3:  setosa          1.3         0.2          4.7         3.2\n4:  setosa          1.5         0.2          4.6         3.1\n5:  setosa          1.4         0.2          5.0         3.6\n\n\n\ngr = Graph$new()$add_pipeop(PipeOpDropConst$new())\ndropped_task = gr$train(irishead)[[1]]\n\ndropped_task$data()\n\n   Species Petal.Length Sepal.Length Sepal.Width\n1:  setosa          1.4          5.1         3.5\n2:  setosa          1.4          4.9         3.0\n3:  setosa          1.3          4.7         3.2\n4:  setosa          1.5          4.6         3.1\n5:  setosa          1.4          5.0         3.6\n\n\nWe can also see that the $state was correctly set. Calling $.predict() with this graph, even with different data (the whole Iris Task!) will still drop the \"Petal.Width\" column, as it should.\n\ngr$pipeops$drop.const$state\n\n$cnames\n[1] \"Petal.Length\" \"Sepal.Length\" \"Sepal.Width\" \n\n$affected_cols\n[1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Length\" \"Sepal.Width\" \n\n$intasklayout\n             id    type\n1: Petal.Length numeric\n2:  Petal.Width numeric\n3: Sepal.Length numeric\n4:  Sepal.Width numeric\n\n$outtasklayout\n             id    type\n1: Petal.Length numeric\n2: Sepal.Length numeric\n3:  Sepal.Width numeric\n\n$outtaskshell\nEmpty data.table (0 rows and 4 cols): Species,Petal.Length,Sepal.Length,Sepal.Width\n\n\n\ndropped_predict = gr$predict(task)[[1]]\n\ndropped_predict$data()\n\n       Species Petal.Length Sepal.Length Sepal.Width\n  1:    setosa          1.4          5.1         3.5\n  2:    setosa          1.4          4.9         3.0\n  3:    setosa          1.3          4.7         3.2\n  4:    setosa          1.5          4.6         3.1\n  5:    setosa          1.4          5.0         3.6\n ---                                                \n146: virginica          5.2          6.7         3.0\n147: virginica          5.0          6.3         2.5\n148: virginica          5.2          6.5         3.0\n149: virginica          5.4          6.2         3.4\n150: virginica          5.1          5.9         3.0\n\n\n\n11.3.3.2 Example: PipeOpScaleAlwaysSimple\n\nThis example will show how a PipeOpTaskPreprocSimple can be used when only working on feature data in form of a data.table. Instead of calling the scale() function, the center and scale values are calculated directly and saved to the $state slot. The .transform_dt() function will then perform the same operation during both training and prediction: subtract the center and divide by the scale value. As in the PipeOpScaleAlways example above, we use .select_cols() so that we only work on numeric columns.\n\nPipeOpScaleAlwaysSimple = R6::R6Class(\"PipeOpScaleAlwaysSimple\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"scale.always.simple\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .get_state_dt = function(dt, levels, target) {\n      list(\n        center = sapply(dt, mean),\n        scale = sapply(dt, sd)\n      )\n    },\n\n    .transform_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\n\nWe can compare this PipeOp to the one above to show that it behaves the same.\n\ngr = Graph$new()$add_pipeop(PipeOpScaleAlways$new())\nresult_posa = gr$train(task)[[1]]\n\ngr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new())\nresult_posa_simple = gr$train(task)[[1]]\n\n\nresult_posa$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n\n\n\nresult_posa_simple$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n\n\n\n11.3.4 Hyperparameters\nmlr3pipelines uses the [paradox](https://paradox.mlr-org.com) package to define parameter spaces for PipeOps. Parameters for PipeOps can modify their behavior in certain ways, e.g. switch centering or scaling off in the PipeOpScale operator. The unified interface makes it possible to have parameters for whole Graphs that modify the individual PipeOp’s behavior. The Graphs, when encapsulated in GraphLearners, can even be tuned using the tuning functionality in mlr3tuning.\nHyperparameters are declared during initialization, when calling the PipeOp’s $initialize() function, by giving a param_set argument. The param_set must be a ParamSet from the paradox package; see the tuning chapter or Section 9.4 for more information on how to define parameter spaces. After construction, the ParamSet can be accessed through the $param_set slot. While it is possible to modify this ParamSet, using e.g. the $add() and $add_dep() functions, after adding it to the PipeOp, it is strongly advised against.\nHyperparameters can be set and queried through the $values slot. When setting hyperparameters, they are automatically checked to satisfy all conditions set by the $param_set, so it is not necessary to type check them. Be aware that it is always possible to remove hyperparameter values.\nWhen a PipeOp is initialized, it usually does not have any parameter values—$values takes the value list(). It is possible to set initial parameter values in the $initialize() constructor; this must be done after the super$initialize() call where the corresponding ParamSet must be supplied. This is because setting $values checks against the current $param_set, which would fail if the $param_set was not set yet.\nWhen using an underlying library function (the scale function in PipeOpScale, say), then there is usually a “default” behaviour of that function when a parameter is not given. It is good practice to use this default behaviour whenever a parameter is not set (or when it was removed). This can easily be done when using the mlr3misc library’s mlr3misc::invoke() function, which has functionality similar to \"do.call()\".\n\n11.3.4.1 Hyperparameter Example: PipeOpScale\n\nHow to use hyperparameters can best be shown through the example of PipeOpScale, which is very similar to the example above, PipeOpScaleAlways. The difference is made by the presence of hyperparameters. PipeOpScale constructs a ParamSet in its $initialize function and passes this on to the super$initialize function:\n\nPipeOpScale$public_methods$initialize\n\nfunction (id = \"scale\", param_vals = list()) \n.__PipeOpScale__initialize(self = self, private = private, super = super, \n    id = id, param_vals = param_vals)\n<environment: namespace:mlr3pipelines>\n\n\nThe user has access to this and can set and get parameters. Types are automatically checked:\n\npss = po(\"scale\")\nprint(pss$param_set)\n\n<ParamSet:scale>\n               id    class lower upper nlevels        default value\n1:         center ParamLgl    NA    NA       2           TRUE      \n2:          scale ParamLgl    NA    NA       2           TRUE      \n3:         robust ParamLgl    NA    NA       2 <NoDefault[3]> FALSE\n4: affect_columns ParamUty    NA    NA     Inf  <Selector[1]>      \n\n\n\npss$param_set$values$center = FALSE\nprint(pss$param_set$values)\n\n$robust\n[1] FALSE\n\n$center\n[1] FALSE\n\n\n\npss$param_set$values$scale = \"TRUE\" # bad input is checked!\n\nError in self$assert(xs): Assertion on 'xs' failed: scale: Must be of type 'logical flag', not 'character'.\n\n\nHow PipeOpScale handles its parameters can be seen in its $.train_dt method: It gets the relevant parameters from its $values slot and uses them in the mlr3misc::invoke() call. This has the advantage over calling scale() directly that if a parameter is not given, its default value from the \"scale()\" function will be used.\n\nPipeOpScale$private_methods$.train_dt\n\nfunction (dt, levels, target) \n.__PipeOpScale__.train_dt(self = self, private = private, super = super, \n    dt = dt, levels = levels, target = target)\n<environment: namespace:mlr3pipelines>\n\n\nAnother change that is necessary compared to PipeOpScaleAlways is that the attributes \"scaled:scale\" and \"scaled:center\" are not always present, depending on parameters, and possibly need to be set to default values \\(1\\) or \\(0\\), respectively.\nIt is now even possible (if a bit pointless) to call PipeOpScale with both scale and center set to FALSE, which returns the original dataset, unchanged.\n\npss$param_set$values$scale = FALSE\npss$param_set$values$center = FALSE\n\ngr = Graph$new()\ngr$add_pipeop(pss)\n\nresult = gr$train(task)\n\nresult[[1]]$data()\n\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0"
  },
  {
    "objectID": "extending.html#extending-tuners",
    "href": "extending.html#extending-tuners",
    "title": "11  Extending",
    "section": "\n11.4 Adding new Tuners",
    "text": "11.4 Adding new Tuners\nIn this section, we show how to implement a custom tuner for mlr3tuning. The main task of a tuner is to iteratively propose new hyperparameter configurations that we want to evaluate for a given task, learner and validation strategy. The second task is to decide which configuration should be returned as a tuning result - usually it is the configuration that led to the best observed performance value. If you want to implement your own tuner, you have to implement an R6-Object that offers an .optimize method that implements the iterative proposal and you are free to implement .assign_result to differ from the before-mentioned default process of determining the result.\nBefore you start with the implementation make yourself familiar with the main R6-Objects in bbotk (Black-Box Optimization Toolkit). This package does not only provide basic black box optimization algorithms and but also the objects that represent the optimization problem (OptimInstance) and the log of all evaluated configurations (Archive).\nThere are two ways to implement a new tuner: a ) If your new tuner can be applied to any kind of optimization problem it should be implemented as a Optimizer. Any Optimizer can be easily transformed to a Tuner. b) If the new custom tuner is only usable for hyperparameter tuning, for example because it needs to access the task, learner or resampling objects it should be directly implemented in mlr3tuning as a Tuner.\n\n11.4.1 Adding a new Tuner\nThis is a summary of steps for adding a new tuner. The fifth step is only required if the new tuner is added via bbotk.\n\nCheck the tuner does not already exist as a [Optimizer](https://bbotk.mlr-org.com/reference/Optimizer.html) or [Tuner](https://mlr3tuning.mlr-org.com/reference/Tuner.html) in the GitHub repositories.\nUse one of the existing optimizers / tuners as a template.\nOverwrite the .optimize private method of the optimizer / tuner.\nOptionally, overwrite the default .assign_result private method.\nUse the mlr3tuning::TunerFromOptimizer class to transform the Optimizer to a Tuner.\nAdd unit tests for the tuner and optionally for the optimizer.\nOpen a new pull request for the [Tuner](https://mlr3tuning.mlr-org.com/reference/Tuner.html) and optionally a second one for the [Optimizer](https://bbotk.mlr-org.com/reference/Optimizer.html).\n\n11.4.2 Template\nIf the new custom tuner is implemented via bbotk, use one of the existing optimizer as a template e.g. bbotk::OptimizerRandomSearch. There are currently only two tuners that are not based on a Optimizer: mlr3hyperband::TunerHyperband and mlr3tuning::TunerIrace. Both are rather complex but you can still use the documentation and class structure as a template. The following steps are identical for optimizers and tuners.\nRewrite the meta information in the documentation and create a new class name. Scientific sources can be added in R/bibentries.R which are added under @source in the documentation. The example and dictionary sections of the documentation are auto-generated based on the @templateVar id <tuner_id>. Change the parameter set of the optimizer / tuner and document them under @section Parameters. Do not forget to change mlr_optimizers$add() / mlr_tuners$add() in the last line which adds the optimizer / tuner to the dictionary.\n\n11.4.3 Optimize method\nThe $.optimize() private method is the main part of the tuner. It takes an instance, proposes new points and calls the $eval_batch() method of the instance to evaluate them. Here you can go two ways: Implement the iterative process yourself or call an external optimization function that resides in another package.\n\n11.4.3.1 Writing a custom iteration\nUsually, the proposal and evaluation is done in a repeat-loop which you have to implement. Please consider the following points:\n\nYou can evaluate one or multiple points per iteration\nYou don’t have to care about termination, as $eval_batch() won’t allow more evaluations then allowed by the bbotk::Terminator. This implies, that code after the repeat-loop will not be executed.\nYou don’t have to care about keeping track of the evaluations as every evaluation is automatically stored in inst$archive.\nIf you want to log additional information for each evaluation of the Objective in the Archive you can simply add columns to the data.table object that is passed to $eval_batch().\n\n11.4.3.2 Calling an external optimization function\nOptimization functions from external packages usually take an objective function as an argument. In this case, you can pass inst$objective_function which internally calls $eval_batch(). Check out OptimizerGenSA for an example.\n\n11.4.4 Assign result method\nThe default $.assign_result() private method simply obtains the best performing result from the archive. The default method can be overwritten if the new tuner determines the result of the optimization in a different way. The new function must call the $assign_result() method of the instance to write the final result to the instance. See mlr3tuning::TunerIrace for an implementation of $.assign_result().\n\n11.4.5 Transform optimizer to tuner\nThis step is only needed if you implement via bbotk. The mlr3tuning::TunerFromOptimizer class transforms a Optimizer to a Tuner. Just add the Optimizer to the optimizer field. See mlr3tuning::TunerRandomSearch for an example.\n\n11.4.6 Add unit tests\nThe new custom tuner should be thoroughly tested with unit tests. Tuners can be tested with the test_tuner() helper function. If you added the Tuner via a Optimizer, you should additionally test the Optimizer with the test_optimizer() helper function."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baniecki, Hubert, and Przemyslaw Biecek. 2019. “modelStudio: Interactive Studio with Explanations for ML\nPredictive Models.” Journal of Open Source\nSoftware 4 (43): 1798. https://doi.org/10.21105/joss.01798.\n\n\nBaniecki, Hubert, Wojciech Kretowicz, Piotr Piątyszek, Jakub Wiśniewski,\nand Przemysław Biecek. 2021. “dalex:\nResponsible Machine Learning with Interactive Explainability and\nFairness in Python.” Journal of Machine Learning\nResearch 22 (214): 1–7. http://jmlr.org/papers/v22/20-1473.html.\n\n\nBengio, Yoshua, and Yves Grandvalet. 2003. “No Unbiased Estimator\nof the Variance of k-Fold Cross-Validation.” Advances in\nNeural Information Processing Systems 16.\n\n\nBergstra, James, and Yoshua Bengio. 2012. “Random Search for\nHyper-Parameter Optimization.” Journal of Machine Learning\nResearch 13 (10): 281–305. http://jmlr.org/papers/v13/bergstra12a.html.\n\n\nBiecek, Przemyslaw. 2018. “DALEX: Explainers\nfor complex predictive models in R.” Journal of\nMachine Learning Research 19 (84): 1–5. http://jmlr.org/papers/v19/18-416.html.\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory\nModel Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBinder, Martin, Florian Pfisterer, Michel Lang, Lennart Schneider, Lars\nKotthoff, and Bernd Bischl. 2021. “mlr3pipelines - Flexible Machine Learning\nPipelines in R.” Journal of Machine Learning\nResearch 22 (184): 1–7. http://jmlr.org/papers/v22/21-0281.html.\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter,\nStefan Coors, Janek Thomas, et al. 2021. “Hyperparameter\nOptimization: Foundations, Algorithms, Best Practices and Open\nChallenges.” https://doi.org/10.48550/ARXIV.2107.05847.\n\n\nBischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob\nRichter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones.\n2016. “mlr: Machine\nLearning in R.” Journal of Machine\nLearning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html.\n\n\nBischl, Bernd, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012.\n“Resampling Methods for Meta-Model Validation with Recommendations\nfor Evolutionary Computation.” Evolutionary Computation\n20 (2): 249–75.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Springer.\n\n\nBommert, Andrea, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel\nLang. 2020. “Benchmark for Filter Methods for Feature Selection in\nHigh-Dimensional Classification Data.” Computational\nStatistics & Data Analysis 143: 106839. https://doi.org/https://doi.org/10.1016/j.csda.2019.106839.\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine\nLearning 24 (2): 123–40.\n\n\nBrenning, Alexander. 2012. “Spatial Cross-Validation and Bootstrap\nfor the Assessment of Prediction Rules in Remote Sensing: The\nR Package Sperrorest.” In 2012 IEEE\nInternational Geoscience and Remote Sensing Symposium.\nIEEE. https://doi.org/10.1109/igarss.2012.6352393.\n\n\nBücker, Michael, Gero Szepannek, Alicja Gosiewska, and Przemyslaw\nBiecek. 2022. “Transparency, Auditability, and Explainability of\nMachine Learning Models in Credit Scoring.” Journal of the\nOperational Research Society 73 (1): 70–90. https://doi.org/10.1080/01605682.2021.1922098.\n\n\nChandrashekar, Girish, and Ferat Sahin. 2014. “A Survey on Feature\nSelection Methods.” Computers and Electrical Engineering\n40 (1): 16–28. https://doi.org/https://doi.org/10.1016/j.compeleceng.2013.11.024.\n\n\nCollett, David. 2014. Modelling Survival Data\nin Medical Research. 3rd ed. CRC.\n\n\nDavis, Jesse, and Mark Goadrich. 2006. “The Relationship Between\nPrecision-Recall and ROC Curves.” In Proceedings of the 23rd\nInternational Conference on Machine Learning, 233–40.\n\n\nDemšar, Janez. 2006. “Statistical Comparisons of Classifiers over\nMultiple Data Sets.” Journal of Machine Learning\nResearch 7 (1): 1–30. https://jmlr.org/papers/v7/demsar06a.html.\n\n\nFeurer, Matthias, and Frank Hutter. 2019. “Hyperparameter\nOptimization.” In Automated Machine Learning: Methods,\nSystems, Challenges, edited by Frank Hutter, Lars Kotthoff, and\nJoaquin Vanschoren, 3–33. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_1.\n\n\nGuyon, Isabelle, and André Elisseeff. 2003. “An Introduction to\nVariable and Feature Selection.” Journal of Machine Learning\nResearch 3 (Mar): 1157–82.\n\n\nHand, David J, and Robert J Till. 2001. “A Simple Generalisation\nof the Area Under the ROC Curve for Multiple Class Classification\nProblems.” Machine Learning 45: 171–86.\n\n\nHansen, Nikolaus, and Anne Auger. 2011. “CMA-ES: Evolution\nStrategies and Covariance Matrix Adaptation.” In Proceedings\nof the 13th Annual Conference Companion on Genetic and Evolutionary\nComputation, 991–1010.\n\n\nHastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. The\nElements of Statistical Learning. Springer New York. https://doi.org/10.1007/978-0-387-21606-5.\n\n\nHolzinger, Andreas, Anna Saranti, Christoph Molnar, Przemyslaw Biecek,\nand Wojciech Samek. 2022. “Explainable AI Methods - a Brief\nOverview.” International Workshop on Extending Explainable AI\nBeyond Deep Models and Classifiers, 13–38. https://doi.org/10.1007/978-3-031-04083-2_2.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.\npalmerpenguins: Palmer Archipelago (Antarctica)\npenguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning: With Applications in\nr. Springer Publishing Company, Incorporated.\n\n\nJapkowicz, Nathalie, and Mohak Shah. 2011. Evaluating Learning\nAlgorithms: A Classification Perspective. Cambridge University\nPress.\n\n\nKarl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan\nCoors, Martin Binder, Lennart Schneider, et al. 2022.\n“Multi-Objective Hyperparameter Optimization - an\nOverview.” https://doi.org/10.48550/ARXIV.2206.07438.\n\n\nKim, Ji-Hyun. 2009. “Estimating Classification Error Rate:\nRepeated Cross-Validation, Repeated Hold-Out and Bootstrap.”\nComputational Statistics & Data Analysis 53 (11): 3735–45.\n\n\nKrzyziński, Mateusz, Mikołaj Spytek, Hubert Baniecki, and Przemysław\nBiecek. 2023. “SurvSHAP(t): Time-dependent\nexplanations of machine learning survival models.”\nKnowledge-Based Systems 262: 110234. https://doi.org/https://doi.org/10.1016/j.knosys.2022.110234.\n\n\nLang, Michel. 2017. “checkmate: Fast Argument\nChecks for Defensive R Programming.” The R\nJournal 9 (1): 437–45. https://doi.org/10.32614/RJ-2017-028.\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian\nPfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff,\nand Bernd Bischl. 2019. “mlr3: A\nModern Object-Oriented Machine Learning Framework in\nR.” Journal of Open Source Software,\nDecember. https://doi.org/10.21105/joss.01903.\n\n\nLegendre, Pierre. 1993. “Spatial Autocorrelation: Trouble or New\nParadigm?” Ecology 74 (6): 1659–73. https://doi.org/10.2307/1939924.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and\nAmeet Talwalkar. 2017. “Hyperband: A Novel Bandit-Based Approach\nto Hyperparameter Optimization.” The Journal of Machine\nLearning Research 18 (1): 6765–6816.\n\n\nLópez-Ibáñez, Manuel, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres,\nMauro Birattari, and Thomas Stützle. 2016. “The Irace Package:\nIterated Racing for Automatic Algorithm Configuration.”\nOperations Research Perspectives 3: 43–58.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. CRC Press.\n\n\nMeyer, Hanna, Christoph Reudenbach, Tomislav Hengl, Marwan Katurji, and\nThomas Nauss. 2018. “Improving Performance of Spatio-Temporal\nMachine Learning Models Using Forward Feature Selection and\nTarget-Oriented Validation.” Environmental Modelling &\nSoftware 101 (March): 1–9. https://doi.org/10.1016/j.envsoft.2017.12.001.\n\n\nMolinaro, Annette M, Richard Simon, and Ruth M Pfeiffer. 2005.\n“Prediction Error Estimation: A Comparison of Resampling\nMethods.” Bioinformatics 21 (15): 3301–7.\n\n\nMuenchow, J., A. Brenning, and M. Richter. 2012. “Geomorphic\nProcess Rates of Landslides Along a Humidity Gradient in the Tropical\nAndes.” Geomorphology 139-140: 271–84.\nhttps://doi.org/https://doi.org/10.1016/j.geomorph.2011.10.029.\n\n\nO’Neil, Cathy. 2016. Weapons of Math\nDestruction: How Big Data Increases Inequality and Threatens\nDemocracy. New York, NY: Crown Publishing Group.\n\n\nR Core Team. 2019. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRomaszko, Kamil, Magda Tatarynowicz, Mateusz Urbański, and Przemysław\nBiecek. 2019. “modelDown: Automated Website Generator with\nInterpretable Documentation for Predictive Machine Learning\nModels.” Journal of Open Source Software 4 (38): 1444.\nhttps://doi.org/10.21105/joss.01444.\n\n\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and\nAlexander Brenning. 2019. “Hyperparameter Tuning and Performance\nAssessment of Statistical and Machine-Learning Algorithms Using Spatial\nData.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002.\n\n\nSilverman, Bernard W. 1986. Density Estimation for Statistics and\nData Analysis. Vol. 26. CRC press.\n\n\nSimon, Richard. 2007. “Resampling Strategies for Model Assessment\nand Selection.” In Fundamentals of Data Mining in Genomics\nand Proteomics, edited by Werner Dubitzky, Martin Granzow, and\nDaniel Berrar, 173–86. Boston, MA: Springer\nUS. https://doi.org/10.1007/978-0-387-47509-7_8.\n\n\nSonabend, Raphael, Franz J Király, Andreas Bender, Bernd Bischl, and\nMichel Lang. 2021. “mlr3proba: An\nR Package for Machine Learning in Survival\nAnalysis.” Bioinformatics, February. https://doi.org/10.1093/bioinformatics/btab039.\n\n\nTsallis, Constantino, and Daniel A Stariolo. 1996. “Generalized\nSimulated Annealing.” Physica A: Statistical Mechanics and\nIts Applications 233 (1-2): 395–406.\n\n\nWiśniewski, Jakub, and Przemysław Biecek. 2022. “The r Journal:\nFairmodels: A Flexible Tool for Bias Detection, Visualization, and\nMitigation in Binary Classification Models.” The R\nJournal 14: 227–43. https://doi.org/10.32614/RJ-2022-019.\n\n\nWolpert, David H. 1992. “Stacked Generalization.”\nNeural Networks 5 (2): 241–59. https://doi.org/https://doi.org/10.1016/S0893-6080(05)80023-1.\n\n\nXiang, Yang, Sylvain Gubian, Brian Suomela, and Julia Hoeng. 2013.\n“Generalized Simulated Annealing for Global Optimization: The\nGenSA Package.” R J. 5 (1): 13."
  },
  {
    "objectID": "solutions.html#solutions-to-sec-basics",
    "href": "solutions.html#solutions-to-sec-basics",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.1 Solutions to Chapter 2\n",
    "text": "A.1 Solutions to Chapter 2\n\n\nUse the built in sonar task and the classif.rpart learner along with the partition function to train a model.\n\n\nset.seed(124)\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nmeasure = msr(\"classif.ce\")\nsplits = partition(task, ratio=0.8)\n\nlearner$train(task, splits$train)\n\nOnce the model is trained, generate the predictions on the test set, define the performance measure (classif.ce), and score the predictions.\n\npreds = learner$predict(task, splits$test)\n\nmeasure = msr(\"classif.ce\")\npreds$score(measure)\n\nclassif.ce \n 0.2195122 \n\n\n\nGenerate a confusion matrix from the built in function.\n\n\npreds$confusion\n\n        truth\nresponse  M  R\n       M 20  7\n       R  2 12\n\n\n\n\n\nSince the rows represent predictions (response) and the columns represent the ground truth values, the TP, FP, TN, and FN rates are as follows:\n\nTrue Positive (TP) = 20\nFalse Positive (FP) = 2\nTrue Negative (TN) = 12\nFalse Positive (FN) = 7\n\n\nSince in this case we want the model to predict the negative class more often, we will raise the threshold (note the predict_type for the learner must be prob for this to work).\n\n\n# raise threshold from 0.5 default to 0.6\npreds$set_threshold(0.6)\n\npreds$confusion\n\n        truth\nresponse  M  R\n       M 14  4\n       R  8 15\n\n\nOne reason we might want the false positive rate to be lower than the false negative rate is if we felt it was worse for a positive prediction to be incorrect (meaning the true label was the negative label) than it was for a negative prediction to be incorrect (meaning the true label was the positive label)."
  },
  {
    "objectID": "solutions.html#solutions-to-sec-performance",
    "href": "solutions.html#solutions-to-sec-performance",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.2 Solutions to Chapter 3\n",
    "text": "A.2 Solutions to Chapter 3\n\n\nUse the spam task and 5-fold cross-validation to benchmark Random Forest (classif.ranger), Logistic Regression (classif.log_reg), and XGBoost (classif.xgboost) with regards to AUC. Which learner appears to do best? How confident are you in your conclusion? How would you improve upon this?\n\n\ngrid = benchmark_grid(\n  tasks = tsk(\"spam\"),\n  learners = lrns(c(\"classif.ranger\", \"classif.log_reg\", \"classif.xgboost\"), predict_type = \"prob\"),\n  resamplings = rsmp(\"cv\", folds = 5)\n)\n\nbmr = benchmark(grid)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nmlr3viz::autoplot(bmr, measure = msr(\"classif.auc\"))\n\n\n\n\nThis is only a small example for a benchmark workflow, but without tuning (see Chapter 4), the results are naturally not suitable to make any broader statements about the superiority of either learner for this task.\n\nA colleague claims to have achieved a 93.1% classification accuracy using the classif.rpart learner on the penguins_simple task. You want to reproduce their results and ask them about their resampling strategy. They said they used 3-fold cross-validation, and they assigned rows using the task’s row_id modulo 3 to generate three evenly sized folds. Reproduce their results using the custom CV strategy.\n\n\ntask = tsk(\"penguins_simple\")\n\nresampling_customcv = rsmp(\"custom_cv\")\n\nresampling_customcv$instantiate(task = task, f = factor(task$row_ids %% 3))\n\nrr = resample(\n  task = task,\n  learner = lrn(\"classif.rpart\"),\n  resampling = resampling_customcv\n)\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9309309"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-optimization",
    "href": "solutions.html#solutions-to-sec-optimization",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.3 Solutions to Chapter 4\n",
    "text": "A.3 Solutions to Chapter 4\n\n\nTune the mtry, sample.fraction, num.trees hyperparameters of a random forest model (regr.ranger) on the Motor Trend data set (mtcars). Use a simple random search with 50 evaluations and select a suitable batch size. Evaluate with a 3-fold cross-validation and the root mean squared error.\n\n\nset.seed(4)\nlearner = lrn(\"regr.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n\ninstance = ti(\n  task = tsk(\"mtcars\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"regr.rmse\"),\n  terminator = trm(\"evals\", n_evals = 50)\n)\n\ntuner = tnr(\"random_search\", batch_size = 10)\n\ntuner$optimize(instance)\n\n   mtry.ratio sample.fraction num.trees learner_param_vals  x_domain regr.rmse\n1:  0.3558625       0.9108463       192          <list[4]> <list[3]>  2.757883\n\n\n\nEvaluate the performance of the model created in Question 1 with nested resampling. Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling. Print the unbiased performance estimate of the model.\n\n\nset.seed(4)\nlearner = lrn(\"regr.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n\nat = auto_tuner(\n  method = tnr(\"random_search\", batch_size = 10),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"regr.rmse\"),\n  terminator = trm(\"evals\", n_evals = 50)\n)\n\ntask = tsk(\"mtcars\")\nouter_resampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n\nrr$aggregate()\n\nregr.mse \n12.16805"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-feature-selection",
    "href": "solutions.html#solutions-to-sec-feature-selection",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.4 Solutions to Chapter 5\n",
    "text": "A.4 Solutions to Chapter 5\n\n\nCalculate a correlation filter on the Motor Trend data set (mtcars).\n\n\nlibrary(\"mlr3verse\")\nfilter = flt(\"correlation\")\n\ntask = tsk(\"mtcars\")\nfilter$calculate(task)\n\nas.data.table(filter)\n\n    feature     score\n 1:      wt 0.8676594\n 2:     cyl 0.8521620\n 3:    disp 0.8475514\n 4:      hp 0.7761684\n 5:    drat 0.6811719\n 6:      vs 0.6640389\n 7:      am 0.5998324\n 8:    carb 0.5509251\n 9:    gear 0.4802848\n10:    qsec 0.4186840\n\n\n\nUse the filter from the first exercise to select the five best features in the mtcars data set.\n\n\nkeep = names(head(filter$scores, 5))\ntask$select(keep)\ntask$feature_names\n\n[1] \"cyl\"  \"disp\" \"drat\" \"hp\"   \"wt\"  \n\n\n\nApply a backward selection to the penguins data set with a classification tree learner \"classif.rpart\" and holdout resampling by the measure classification accuracy. Compare the results with those in Section 5.2.1.\n\n\nlibrary(\"mlr3fselect\")\n\ninstance = fselect(\n  method = \"sequential\",\n  strategy = \"sbs\",\n  task =  tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\")\n)\nas.data.table(instance$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]\n\n   bill_depth bill_length body_mass classif.acc\n1:       TRUE        TRUE      TRUE   0.9565217\n\ninstance$result_feature_set\n\n[1] \"bill_depth\"  \"bill_length\" \"body_mass\"   \"island\"      \"sex\"        \n[6] \"year\"       \n\n\nAnswer the following questions:\n\nDo the selected features differ?\n\nYes, the backward selection selects more features.\n\nWhich feature selection method achieves a higher classification accuracy?\n\nIn this example, the backwards example performs slightly better, but this depends heavily on the random seed and could look different in another run.\n\nAre the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?\n\nNo, they are not comparable because the holdout sampling called with rsmp(\"holdout\") creates a different holdout set for the two runs. A fair comparison would create a single resampling instance and use it for both feature selections (see Chapter 3 for details):\n\nresampling = rsmp(\"holdout\")\nresampling$instantiate(tsk(\"penguins\"))\n\nsfs = fselect(\n  method = \"sequential\",\n  strategy = \"sfs\",\n  task =  tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = resampling,\n  measure = msr(\"classif.acc\")\n)\nsbs = fselect(\n  method = \"sequential\",\n  strategy = \"sbs\",\n  task =  tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = resampling,\n  measure = msr(\"classif.acc\")\n)\nas.data.table(sfs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]\n\n   bill_depth bill_length body_mass classif.acc\n1:      FALSE        TRUE     FALSE    0.973913\n\nas.data.table(sbs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]\n\n   bill_depth bill_length body_mass classif.acc\n1:       TRUE        TRUE      TRUE    0.973913\n\n\nAlternatively, one could automate the feature selection and perform a benchmark between the two wrapped learners.\n\nAutomate the feature selection as in Section 5.2.6 with the spam data set and a logistic regression learner (\"classif.log_reg\"). Hint: Remember to call library(\"mlr3learners\") for the logistic regression learner.\n\n\nlibrary(\"mlr3fselect\")\nlibrary(\"mlr3learners\")\n\nat = auto_fselector(\n  method = fs(\"random_search\"),\n  learner = lrn(\"classif.log_reg\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 50)\n)\n\ngrid = benchmark_grid(\n  task = tsk(\"spam\"),\n  learner = list(at, lrn(\"classif.log_reg\")),\n  resampling = rsmp(\"cv\", folds = 3)\n)\n\nbmr = benchmark(grid)\n\naggr = bmr$aggregate(msrs(c(\"classif.acc\", \"time_train\")))\nas.data.table(aggr)[, .(learner_id, classif.acc, time_train)]\n\n                  learner_id classif.acc time_train\n1: classif.log_reg.fselector   0.9219717  5.5736667\n2:           classif.log_reg   0.9243633  0.1223333"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-pipelines",
    "href": "solutions.html#solutions-to-sec-pipelines",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.5 Solutions to Chapter 6\n",
    "text": "A.5 Solutions to Chapter 6"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-special",
    "href": "solutions.html#solutions-to-sec-special",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.6 Solutions to Chapter 8\n",
    "text": "A.6 Solutions to Chapter 8"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-technical",
    "href": "solutions.html#solutions-to-sec-technical",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.7 Solutions to Chapter 9\n",
    "text": "A.7 Solutions to Chapter 9"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-interpretation",
    "href": "solutions.html#solutions-to-sec-interpretation",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.8 Solutions to Chapter 10\n",
    "text": "A.8 Solutions to Chapter 10\n\n\nPrepare a mlr3 regression task for fifa data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. regr.ranger.\n\n\nlibrary(\"DALEX\")\nlibrary(\"ggplot2\")\ndata(\"fifa\", package = \"DALEX\")\nold_theme = set_theme_dalex(\"ema\") \n\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nset.seed(1)\n\nfifa20 <- fifa[,5:42]\ntask_fifa = as_task_regr(fifa20, target = \"value_eur\", id = \"fifa20\")\n\nlearner = lrn(\"regr.ranger\")\nlearner$train(task_fifa)\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      5000 \nNumber of independent variables:  37 \nMtry:                             6 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       1.022805e+13 \nR squared (OOB):                  0.869943 \n\n\n\nUse the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?\n\nWith iml\n\nlibrary(iml)\nmodel = Predictor$new(learner, \n                data = fifa20, \n                y = fifa$value_eur)\n\neffect = FeatureImp$new(model, \n                loss = \"rmse\")\neffect$plot()\n\n\n\n\nWith DALEX\n\nlibrary(\"DALEX\")\nranger_exp = DALEX::explain(learner,\n  data = fifa20,\n  y = fifa$value_eur,\n  label = \"Fifa 2020\",\n  verbose = FALSE)\n\nranger_effect = model_parts(ranger_exp, B = 5)\nhead(ranger_effect)\n\n             variable mean_dropout_loss     label\n1        _full_model_           1402526 Fifa 2020\n2           value_eur           1402526 Fifa 2020\n3           weight_kg           1471865 Fifa 2020\n4 goalkeeping_kicking           1472795 Fifa 2020\n5           height_cm           1474859 Fifa 2020\n6    movement_balance           1475618 Fifa 2020\n\nplot(ranger_effect) \n\n\n\n\n\nUse the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?\n\nWith iml\n\nnum_features = c(\"movement_reactions\", \"skill_ball_control\", \"age\")\n\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n\n\n\n\nWith DALEX\n\nnum_features = c(\"movement_reactions\", \"skill_ball_control\", \"age\")\n\nranger_profiles = model_profile(ranger_exp, variables = num_features)\nplot(ranger_profiles) \n\n\n\n\n4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices.\n\nplayer_1 <- fifa[\"R. Lewandowski\", 5:42]\n\n\nFor the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?\n\nWith iml\n\nshapley = Shapley$new(model, x.interest = player_1)\nplot(shapley)\n\n\n\n\nWith DALEX\n\nranger_shap = predict_parts(ranger_exp, \n             new_observation = player_1, \n             type = \"shap\", B = 1)\nplot(ranger_shap, show_boxplots = FALSE) \n\n\n\n\n\nFor the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectation profiles to draw the local behavior of the model for this variable. Is it different from the global behavior?\n\nWith DALEX\n\nnum_features = c(\"movement_reactions\", \"skill_ball_control\", \"age\")\n\nranger_ceteris = predict_profile(ranger_exp, player_1)\nplot(ranger_ceteris, variables = num_features) + \n  ggtitle(\"Ceteris paribus for R. Lewandowski\", \" \")"
  },
  {
    "objectID": "solutions.html#solutions-to-sec-extending",
    "href": "solutions.html#solutions-to-sec-extending",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.9 Solutions to Chapter 11\n",
    "text": "A.9 Solutions to Chapter 11"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Appendix B — Glossary",
    "section": "",
    "text": "Term\nDefinition\n\n\nResampling\nRepeatedly splitting data into training and test sets."
  },
  {
    "objectID": "tasks.html#regression-tasks",
    "href": "tasks.html#regression-tasks",
    "title": "Appendix C — Tasks",
    "section": "\nC.1 Regression Tasks",
    "text": "C.1 Regression Tasks\n\nC.1.1 mtcars\n\n\ntsk(\"mtcars\")\n\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\ntsk(\"mtcars\")$head()\n\n    mpg am carb cyl disp drat gear  hp  qsec vs    wt\n1: 21.0  1    4   6  160 3.90    4 110 16.46  0 2.620\n2: 21.0  1    4   6  160 3.90    4 110 17.02  0 2.875\n3: 22.8  1    1   4  108 3.85    4  93 18.61  1 2.320\n4: 21.4  0    1   6  258 3.08    3 110 19.44  1 3.215\n5: 18.7  0    2   8  360 3.15    3 175 17.02  0 3.440\n6: 18.1  0    1   6  225 2.76    3 105 20.22  1 3.460\n\nautoplot(tsk(\"mtcars\"))\n\n\n\n\nSee more at ?mlr_tasks_mtcars."
  },
  {
    "objectID": "tasks.html#classification-tasks",
    "href": "tasks.html#classification-tasks",
    "title": "Appendix C — Tasks",
    "section": "\nC.2 Classification Tasks",
    "text": "C.2 Classification Tasks\n\nC.2.1 penguins\n\n\ntsk(\"penguins\")\n\n<TaskClassif:penguins> (344 x 8): Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (7):\n  - int (3): body_mass, flipper_length, year\n  - dbl (2): bill_depth, bill_length\n  - fct (2): island, sex\n\ntsk(\"penguins\")$head()\n\n   species bill_depth bill_length body_mass flipper_length    island    sex\n1:  Adelie       18.7        39.1      3750            181 Torgersen   male\n2:  Adelie       17.4        39.5      3800            186 Torgersen female\n3:  Adelie       18.0        40.3      3250            195 Torgersen female\n4:  Adelie         NA          NA        NA             NA Torgersen   <NA>\n5:  Adelie       19.3        36.7      3450            193 Torgersen female\n6:  Adelie       20.6        39.3      3650            190 Torgersen   male\n1 variable not shown: [year]\n\nautoplot(tsk(\"penguins\"))\n\n\n\n\nSee more at ?mlr_tasks_penguins.\n\nC.2.2 penguins_simple\n\n\ntsk(\"penguins_simple\")\n\n<TaskClassif:penguins> (333 x 11): Simplified Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (10):\n  - dbl (7): bill_depth, bill_length, island.Biscoe, island.Dream,\n    island.Torgersen, sex.female, sex.male\n  - int (3): body_mass, flipper_length, year\n\ntsk(\"penguins_simple\")$head()\n\n   species bill_depth bill_length body_mass flipper_length island.Biscoe\n1:  Adelie       18.7        39.1      3750            181             0\n2:  Adelie       17.4        39.5      3800            186             0\n3:  Adelie       18.0        40.3      3250            195             0\n4:  Adelie       19.3        36.7      3450            193             0\n5:  Adelie       20.6        39.3      3650            190             0\n6:  Adelie       17.8        38.9      3625            181             0\n5 variables not shown: [island.Dream, island.Torgersen, sex.female, sex.male, year]\n\nautoplot(tsk(\"penguins_simple\"))\n\n\n\n\nSee more at ?mlr3data::mlr_tasks_penguins_simple.\n\nC.2.3 sonar\n\n\ntsk(\"sonar\")\n\n<TaskClassif:sonar> (208 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n    V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31,\n    V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43,\n    V44, V45, V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55,\n    V56, V57, V58, V59, V6, V60, V7, V8, V9\n\ntsk(\"sonar\")$head()\n\n   Class     V1    V10    V11    V12    V13    V14    V15    V16    V17    V18\n1:     R 0.0200 0.2111 0.1609 0.1582 0.2238 0.0645 0.0660 0.2273 0.3100 0.2999\n2:     R 0.0453 0.2872 0.4918 0.6552 0.6919 0.7797 0.7464 0.9444 1.0000 0.8874\n3:     R 0.0262 0.6194 0.6333 0.7060 0.5544 0.5320 0.6479 0.6931 0.6759 0.7551\n4:     R 0.0100 0.1264 0.0881 0.1992 0.0184 0.2261 0.1729 0.2131 0.0693 0.2281\n5:     R 0.0762 0.4459 0.4152 0.3952 0.4256 0.4135 0.4528 0.5326 0.7306 0.6193\n6:     R 0.0286 0.3039 0.2988 0.4250 0.6343 0.8198 1.0000 0.9988 0.9508 0.9025\n50 variables not shown: [V19, V2, V20, V21, V22, V23, V24, V25, V26, V27, ...]\n\nautoplot(tsk(\"sonar\"))\n\n\n\n\nSee more at ?mlr_tasks_sonar.\n\nC.2.4 spam\n\n\ntsk(\"spam\")\n\n<TaskClassif:spam> (4601 x 58): HP Spam Detection\n* Target: type\n* Properties: twoclass\n* Features (57):\n  - dbl (57): address, addresses, all, business, capitalAve,\n    capitalLong, capitalTotal, charDollar, charExclamation, charHash,\n    charRoundbracket, charSemicolon, charSquarebracket, conference,\n    credit, cs, data, direct, edu, email, font, free, george, hp, hpl,\n    internet, lab, labs, mail, make, meeting, money, num000, num1999,\n    num3d, num415, num650, num85, num857, order, original, our, over,\n    parts, people, pm, project, re, receive, remove, report, table,\n    technology, telnet, will, you, your\n\ntsk(\"spam\")$head()\n\n   type address addresses  all business capitalAve capitalLong capitalTotal\n1: spam    0.64      0.00 0.64     0.00      3.756          61          278\n2: spam    0.28      0.14 0.50     0.07      5.114         101         1028\n3: spam    0.00      1.75 0.71     0.06      9.821         485         2259\n4: spam    0.00      0.00 0.00     0.00      3.537          40          191\n5: spam    0.00      0.00 0.00     0.00      3.537          40          191\n6: spam    0.00      0.00 0.00     0.00      3.000          15           54\n50 variables not shown: [charDollar, charExclamation, charHash, charRoundbracket, charSemicolon, charSquarebracket, conference, credit, cs, data, ...]\n\nautoplot(tsk(\"spam\"))\n\n\n\n\nSee more at ?mlr_tasks_spam."
  },
  {
    "objectID": "tasks.html#survival-tasks",
    "href": "tasks.html#survival-tasks",
    "title": "Appendix C — Tasks",
    "section": "\nC.3 Survival Tasks",
    "text": "C.3 Survival Tasks\n\nC.3.1 rats\n\n\ntsk(\"rats\")\n\n<TaskSurv:rats> (300 x 5): Rats\n* Target: time, status\n* Properties: -\n* Features (3):\n  - int (2): litter, rx\n  - fct (1): sex\n\ntsk(\"rats\")$head()\n\n   time status litter rx sex\n1:  101      0      1  1   f\n2:   49      1      1  0   f\n3:  104      0      1  0   f\n4:   91      0      2  1   m\n5:  104      0      2  0   m\n6:  102      0      2  0   m\n\nautoplot(tsk(\"rats\"))\n\n\n\n\nSee more at ?mlr3proba::mlr_tasks_rats."
  },
  {
    "objectID": "tasks.html#density-tasks",
    "href": "tasks.html#density-tasks",
    "title": "Appendix C — Tasks",
    "section": "\nC.4 Density Tasks",
    "text": "C.4 Density Tasks\n\nC.4.1 precip\n\n\ntsk(\"precip\")\n\n<TaskDens:precip> (70 x 1): Annual Precipitation\n* Target: -\n* Properties: -\n* Features (1):\n  - dbl (1): precip\n\ntsk(\"precip\")$head()\n\n   precip\n1:   67.0\n2:   54.7\n3:    7.0\n4:   48.5\n5:   14.0\n6:   17.2\n\nautoplot(tsk(\"precip\"))\n\n\n\n\nSee more at ?mlr3proba::mlr_tasks_precip."
  },
  {
    "objectID": "tasks.html#spatiotemporal-tasks",
    "href": "tasks.html#spatiotemporal-tasks",
    "title": "Appendix C — Tasks",
    "section": "\nC.5 Spatiotemporal Tasks",
    "text": "C.5 Spatiotemporal Tasks\n\nC.5.1 ecuador\n\n\ntsk(\"ecuador\")\n\n<TaskClassifST:ecuador> (751 x 11): Ecuador landslides\n* Target: slides\n* Properties: twoclass\n* Features (10):\n  - dbl (10): carea, cslope, dem, distdeforest, distroad,\n    distslidespast, hcurv, log.carea, slope, vcurv\n* Coordinates:\n            x       y\n  1: 712882.5 9560002\n  2: 715232.5 9559582\n  3: 715392.5 9560172\n  4: 715042.5 9559312\n  5: 715382.5 9560142\n ---                 \n747: 714472.5 9558482\n748: 713142.5 9560992\n749: 713322.5 9560562\n750: 715392.5 9557932\n751: 713802.5 9560862\n\ntsk(\"ecuador\")$head()\n\n   slides       carea   cslope     dem distdeforest distroad distslidespast\n1:   TRUE   5577.3916 34.42789 1911.52        15.00      300              9\n2:   TRUE   1399.2329 30.71569 2198.66       300.00      300             21\n3:   TRUE 351155.1250 32.81444 1988.71       300.00      300             40\n4:   TRUE    500.5027 33.90592 2320.49       300.00      300            100\n5:   TRUE    671.1807 41.60017 2021.07       300.00      300             21\n6:   TRUE    634.3320 30.29457 1838.40         9.15      300              2\n4 variables not shown: [hcurv, log.carea, slope, vcurv]\n\nautoplot(tsk(\"ecuador\"))\n\n\n\n\nSee more at ?mlr3spatiotempcv::mlr_tasks_ecuador."
  },
  {
    "objectID": "tasks.html#clustering-tasks",
    "href": "tasks.html#clustering-tasks",
    "title": "Appendix C — Tasks",
    "section": "\nC.6 Clustering Tasks",
    "text": "C.6 Clustering Tasks\n\nC.6.1 usarrests\n\n\ntsk(\"usarrests\")\n\n<TaskClust:usarrests> (50 x 4): US Arrests\n* Target: -\n* Properties: -\n* Features (4):\n  - int (2): Assault, UrbanPop\n  - dbl (2): Murder, Rape\n\ntsk(\"usarrests\")$head()\n\n   Assault Murder Rape UrbanPop\n1:     236   13.2 21.2       58\n2:     263   10.0 44.5       48\n3:     294    8.1 31.0       80\n4:     190    8.8 19.5       50\n5:     276    9.0 40.6       91\n6:     204    7.9 38.7       78\n\nautoplot(tsk(\"usarrests\"))\n\n\n\n\nSee more at ?mlr3cluster::mlr_tasks_usarrests."
  },
  {
    "objectID": "overview-tables.html",
    "href": "overview-tables.html",
    "title": "Appendix D — Overview Tables",
    "section": "",
    "text": "Our homepage provides overviews and tables of the following objects:\n\n\nDescription\nLink\n\n\n\nPackages overview\nhttps://mlr-org.com/packages.html\n\n\nTask overview\nhttps://mlr-org.com/tasks.html\n\n\nLearner overview\nhttps://mlr-org.com/learners.html\n\n\nResampling overview\nhttps://mlr-org.com/resamplings.html\n\n\nMeasure overview\nhttps://mlr-org.com/measures.html\n\n\nPipeOp overview\nhttps://mlr-org.com/pipeops.html\n\n\nGraph overview\nhttps://mlr-org.com/graphs.html\n\n\nTuner overview\nhttps://mlr-org.com/tuners.html\n\n\nTerminator overview\nhttps://mlr-org.com/terminators.html\n\n\nTuning space overview\nhttps://mlr-org.com/tuning_spaces.html\n\n\nFilter overview\nhttps://mlr-org.com/filters.html\n\n\nFSelector overview\nhttps://mlr-org.com/fselectors.html\n\n\n\n\nlibrary(mlr3verse)\nlibrary(mlr3proba)\nlibrary(mlr3spatiotempcv)\nlibrary(mlr3spatial)\nlibrary(mlr3extralearners)\nlibrary(mlr3hyperband)\nlibrary(mlr3mbo)\n\n\nmlr_tasks\n\n<DictionaryTask> with 32 stored values\nKeys: actg, bike_sharing, boston_housing, breast_cancer, cookfarm_mlr3,\n  diplodia, ecuador, faithful, gbcs, german_credit, grace, ilpd, iris,\n  kc_housing, leipzig, lung, moneyball, mtcars, optdigits, penguins,\n  penguins_simple, pima, precip, rats, sonar, spam, titanic,\n  unemployment, usarrests, whas, wine, zoo\n\nmlr_learners\n\n<DictionaryLearner> with 145 stored values\nKeys: classif.abess, classif.AdaBoostM1, classif.bart, classif.C50,\n  classif.catboost, classif.cforest, classif.ctree, classif.cv_glmnet,\n  classif.debug, classif.earth, classif.featureless, classif.fnn,\n  classif.gam, classif.gamboost, classif.gausspr, classif.gbm,\n  classif.glmboost, classif.glmer, classif.glmnet, classif.IBk,\n  classif.imbalanced_rfsrc, classif.J48, classif.JRip, classif.kknn,\n  classif.ksvm, classif.lda, classif.liblinear, classif.lightgbm,\n  classif.LMT, classif.log_reg, classif.lssvm, classif.mob,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.OneR,\n  classif.PART, classif.priority_lasso, classif.qda,\n  classif.randomForest, classif.ranger, classif.rfsrc, classif.rpart,\n  classif.svm, classif.xgboost, clust.agnes, clust.ap, clust.cmeans,\n  clust.cobweb, clust.dbscan, clust.diana, clust.em, clust.fanny,\n  clust.featureless, clust.ff, clust.hclust, clust.kkmeans,\n  clust.kmeans, clust.MBatchKMeans, clust.mclust, clust.meanshift,\n  clust.pam, clust.SimpleKMeans, clust.xmeans, dens.hist, dens.kde,\n  dens.kde_ks, dens.locfit, dens.logspline, dens.mixed, dens.nonpar,\n  dens.pen, dens.plug, dens.spline, regr.abess, regr.bart,\n  regr.catboost, regr.cforest, regr.ctree, regr.cubist, regr.cv_glmnet,\n  regr.debug, regr.earth, regr.featureless, regr.fnn, regr.gam,\n  regr.gamboost, regr.gausspr, regr.gbm, regr.glm, regr.glmboost,\n  regr.glmnet, regr.IBk, regr.kknn, regr.km, regr.ksvm, regr.liblinear,\n  regr.lightgbm, regr.lm, regr.lmer, regr.M5Rules, regr.mars, regr.mob,\n  regr.nnet, regr.priority_lasso, regr.randomForest, regr.ranger,\n  regr.rfsrc, regr.rpart, regr.rsm, regr.rvm, regr.svm, regr.xgboost,\n  surv.akritas, surv.aorsf, surv.blackboost, surv.cforest,\n  surv.coxboost, surv.coxph, surv.coxtime, surv.ctree,\n  surv.cv_coxboost, surv.cv_glmnet, surv.deephit, surv.deepsurv,\n  surv.dnnsurv, surv.flexible, surv.gamboost, surv.gbm, surv.glmboost,\n  surv.glmnet, surv.kaplan, surv.loghaz, surv.mboost, surv.nelson,\n  surv.obliqueRSF, surv.parametric, surv.pchazard, surv.penalized,\n  surv.priority_lasso, surv.ranger, surv.rfsrc, surv.rpart, surv.svm,\n  surv.xgboost\n\nmlr_resamplings\n\n<DictionaryResampling> with 24 stored values\nKeys: bootstrap, custom, custom_cv, cv, holdout, insample, loo,\n  repeated_cv, repeated_spcv_block, repeated_spcv_coords,\n  repeated_spcv_disc, repeated_spcv_env, repeated_spcv_tiles,\n  repeated_sptcv_cluto, repeated_sptcv_cstf, spcv_block, spcv_buffer,\n  spcv_coords, spcv_disc, spcv_env, spcv_tiles, sptcv_cluto,\n  sptcv_cstf, subsampling\n\nmlr_measures\n\n<DictionaryMeasure> with 93 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n  classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n  classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mauc_au1p, classif.mauc_au1u,\n  classif.mauc_aunp, classif.mauc_aunu, classif.mbrier, classif.mcc,\n  classif.npv, classif.ppv, classif.prauc, classif.precision,\n  classif.recall, classif.sensitivity, classif.specificity, classif.tn,\n  classif.tnr, classif.tp, classif.tpr, clust.ch, clust.db, clust.dunn,\n  clust.silhouette, clust.wss, debug, dens.logloss, oob_error,\n  regr.bias, regr.ktau, regr.logloss, regr.mae, regr.mape, regr.maxae,\n  regr.medae, regr.medse, regr.mse, regr.msle, regr.pbias, regr.rae,\n  regr.rmse, regr.rmsle, regr.rrse, regr.rse, regr.rsq, regr.sae,\n  regr.smape, regr.srho, regr.sse, selected_features, sim.jaccard,\n  sim.phi, surv.brier, surv.calib_alpha, surv.calib_beta,\n  surv.chambless_auc, surv.cindex, surv.dcalib, surv.graf,\n  surv.hung_auc, surv.intlogloss, surv.logloss, surv.mae, surv.mse,\n  surv.nagelk_r2, surv.oquigley_r2, surv.rcll, surv.rmse, surv.schmid,\n  surv.song_auc, surv.song_tnr, surv.song_tpr, surv.uno_auc,\n  surv.uno_tnr, surv.uno_tpr, surv.xu_r2, time_both, time_predict,\n  time_train\n\nmlr_pipeops\n\n<DictionaryPipeOp> with 74 stored values\nKeys: boxcox, branch, chunk, classbalancing, classifavg, classweights,\n  colapply, collapsefactors, colroles, compose_crank, compose_distr,\n  compose_probregr, copy, crankcompose, datefeatures, distrcompose,\n  encode, encodeimpact, encodelmer, featureunion, filter, fixfactors,\n  histbin, ica, imputeconstant, imputehist, imputelearner, imputemean,\n  imputemedian, imputemode, imputeoor, imputesample, kernelpca,\n  learner, learner_cv, missind, modelmatrix, multiplicityexply,\n  multiplicityimply, mutate, nmf, nop, ovrsplit, ovrunite, pca, proxy,\n  quantilebin, randomprojection, randomresponse, regravg,\n  removeconstants, renamecolumns, replicate, scale, scalemaxabs,\n  scalerange, select, smote, spatialsign, subsample, survavg,\n  targetinvert, targetmutate, targettrafoscalerange, textvectorizer,\n  threshold, trafopred_regrsurv, trafopred_survregr,\n  trafotask_regrsurv, trafotask_survregr, tunethreshold, unbranch,\n  vtreat, yeojohnson\n\nmlr_graphs\n\n<DictionaryGraph> with 13 stored values\nKeys: bagging, branch, crankcompositor, distrcompositor, greplicate,\n  ovr, probregr, robustify, stacking, survaverager, survbagging,\n  survtoregr, targettrafo\n\nmlr_tuners\n\n<DictionaryTuner> with 10 stored values\nKeys: cmaes, design_points, gensa, grid_search, hyperband, irace, mbo,\n  nloptr, random_search, successive_halving\n\nmlr_terminators\n\n<DictionaryTerminator> with 8 stored values\nKeys: clock_time, combo, evals, none, perf_reached, run_time,\n  stagnation, stagnation_batch\n\nmlr_tuning_spaces\n\n<DictionaryTuningSpaces> with 24 stored values\nKeys: classif.glmnet.default, classif.glmnet.rbv2,\n  classif.kknn.default, classif.kknn.rbv2, classif.ranger.default,\n  classif.ranger.rbv2, classif.rpart.default, classif.rpart.rbv2,\n  classif.svm.default, classif.svm.rbv2, classif.xgboost.default,\n  classif.xgboost.rbv2, regr.glmnet.default, regr.glmnet.rbv2,\n  regr.kknn.default, regr.kknn.rbv2, regr.ranger.default,\n  regr.ranger.rbv2, regr.rpart.default, regr.rpart.rbv2,\n  regr.svm.default, regr.svm.rbv2, regr.xgboost.default,\n  regr.xgboost.rbv2\n\nmlr_filters\n\n<DictionaryFilter> with 21 stored values\nKeys: anova, auc, carscore, carsurvscore, cmim, correlation, disr,\n  find_correlation, importance, information_gain, jmi, jmim,\n  kruskal_test, mim, mrmr, njmim, performance, permutation, relief,\n  selected_features, variance\n\nmlr_fselectors\n\n<DictionaryFSelector> with 8 stored values\nKeys: design_points, exhaustive_search, genetic_search, random_search,\n  rfe, rfecv, sequential, shadow_variable_search"
  },
  {
    "objectID": "session.html",
    "href": "session.html",
    "title": "Appendix E — Session Info",
    "section": "",
    "text": "If you would like to reproduce the results in this book, note the seed set at the top of each chapter (which is the same as the chapter number) and the sessionInfo at the time of publication:\n\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3\nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] digest_0.6.30     lifecycle_1.0.3   jsonlite_1.8.4    magrittr_2.0.3   \n [5] evaluate_0.18     rlang_1.0.6       stringi_1.7.8     cli_3.4.1        \n [9] vctrs_0.5.1       rmarkdown_2.18    tools_4.2.2       stringr_1.5.0    \n[13] glue_1.6.2        htmlwidgets_1.5.4 xfun_0.35         fastmap_1.1.0    \n[17] compiler_4.2.2    htmltools_0.5.3   knitr_1.41"
  }
]