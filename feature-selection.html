<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Marvin N. Wright">
<title>Flexible and Robust Machine Learning Using mlr3 in R - 5&nbsp; Feature Selection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./pipelines.html" rel="next">
<link href="./optimization.html" rel="prev">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature Selection</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Flexible and Robust Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="./Flexible-and-Robust-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Getting Started</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./performance.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Resampling and Benchmarking</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./feature-selection.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature Selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pipelines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preprocessing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Preprocessing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./special.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Special Tasks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./technical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Technical</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interpretation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Interpretation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extending.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Extending</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">Appendices</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solutions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tasks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Tasks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./overview-tables.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Overview Tables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Session Info</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-fs-filter" id="toc-sec-fs-filter" class="nav-link active" data-scroll-target="#sec-fs-filter"><span class="toc-section-number">5.1</span>  Filters</a>
  <ul class="collapse">
<li><a href="#sec-fs-calc" id="toc-sec-fs-calc" class="nav-link" data-scroll-target="#sec-fs-calc"><span class="toc-section-number">5.1.1</span>  Calculating Filter Values</a></li>
  <li><a href="#sec-fs-var-imp-filters" id="toc-sec-fs-var-imp-filters" class="nav-link" data-scroll-target="#sec-fs-var-imp-filters"><span class="toc-section-number">5.1.2</span>  Feature Importance Filters</a></li>
  <li><a href="#sec-fs-embedded-methods" id="toc-sec-fs-embedded-methods" class="nav-link" data-scroll-target="#sec-fs-embedded-methods"><span class="toc-section-number">5.1.3</span>  Embedded Methods</a></li>
  <li><a href="#filter-based-feature-selection" id="toc-filter-based-feature-selection" class="nav-link" data-scroll-target="#filter-based-feature-selection"><span class="toc-section-number">5.1.4</span>  Filter-based Feature Selection</a></li>
  </ul>
</li>
  <li>
<a href="#sec-fs-wrapper" id="toc-sec-fs-wrapper" class="nav-link" data-scroll-target="#sec-fs-wrapper"><span class="toc-section-number">5.2</span>  Wrapper Methods</a>
  <ul class="collapse">
<li><a href="#sec-fs-wrapper-example" id="toc-sec-fs-wrapper-example" class="nav-link" data-scroll-target="#sec-fs-wrapper-example"><span class="toc-section-number">5.2.1</span>  Simple Forward Selection Example</a></li>
  <li><a href="#the-fselectinstance-classes" id="toc-the-fselectinstance-classes" class="nav-link" data-scroll-target="#the-fselectinstance-classes"><span class="toc-section-number">5.2.2</span>  The <code>FSelectInstance</code> Classes</a></li>
  <li><a href="#the-fselector-class" id="toc-the-fselector-class" class="nav-link" data-scroll-target="#the-fselector-class"><span class="toc-section-number">5.2.3</span>  The <code>FSelector</code> Class</a></li>
  <li><a href="#starting-the-feature-selection" id="toc-starting-the-feature-selection" class="nav-link" data-scroll-target="#starting-the-feature-selection"><span class="toc-section-number">5.2.4</span>  Starting the Feature Selection</a></li>
  <li><a href="#sec-multicrit-featsel" id="toc-sec-multicrit-featsel" class="nav-link" data-scroll-target="#sec-multicrit-featsel"><span class="toc-section-number">5.2.5</span>  Optimizing Multiple Performance Measures</a></li>
  <li><a href="#sec-autofselect" id="toc-sec-autofselect" class="nav-link" data-scroll-target="#sec-autofselect"><span class="toc-section-number">5.2.6</span>  Automating the Feature Selection</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">5.3</span>  Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">5.4</span>  Exercises</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/feature-selection.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/feature-selection.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-feature-selection" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    Marvin N. Wright <a href="https://orcid.org/0000-0002-8542-6291" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Leibniz Institute for Prevention Research and Epidemiology – BIPS, Bremen, Germany
          </p>
        <p class="affiliation">
            University of Bremen, Germany
          </p>
        <p class="affiliation">
            University of Copenhagen, Denmark
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    Feature selection is a key component of data analysis and is part of most machine learning applications. This chapter introduces two concepts of feature selection and how to use them in mlr3. Filter methods are preprocessing steps that are independent of the model to be fitted, whereas wrapper methods work by fitting models on feature subsets and evaluating their performance. We start with simple examples such as univariate filters and sequential forward selection but also cover more advanced topics such as optimizing multiple performance measures simultaneously. To integrate feature selection into complex machine learning pipelines and the mlr3 ecosystem, we show how to combine feature selection with mlr3 pipelines and how to automate feature selection by wrapping it in mlr3 learners.
  </div>
</div>

</header><p>Feature selection, also known as variable or descriptor selection, is the process of finding a subset of features to use with a given task and learner. Using an <em>optimal set</em> of features can have several benefits:</p>
<ul>
<li>improved predictive performance, since we reduce overfitting on irrelevant features,</li>
<li>robust models that do not rely on noisy features,</li>
<li>simpler models that are easier to interpret,</li>
<li>faster model fitting, e.g.&nbsp;for model updates,</li>
<li>faster prediction, and</li>
<li>no need to collect potentially expensive features.</li>
</ul>
<p>However, these objectives will not necessarily be optimized by the same <em>optimal set</em> of features and thus feature selection is inherently multi-objective. In this chapter, we mostly focus on feature selection as a means of improving predictive performance, but also briefly cover optimization of multiple criteria (<a href="#sec-multicrit-featsel"><span>Section&nbsp;5.2.5</span></a>).</p>
<p>Reducing the amount of features can improve models across many scenarios, but it can be especially helpful in datasets that have a high number of features in comparison to the number of datapoints. Many learners perform implicit, also called embedded, feature selection, e.g.&nbsp;via the choice of variables used for splitting in a decision tree. Most other feature selection methods are model agnostic, i.e.&nbsp;they can be used together with any learner. Of the many different approaches to identifying relevant features, we will focus on two general concepts, which are described in detail below: Filter and Wrapper methods <span class="citation" data-cites="guyon2003 chandrashekar2014">(<a href="references.html#ref-guyon2003" role="doc-biblioref">Guyon and Elisseeff 2003</a>; <a href="references.html#ref-chandrashekar2014" role="doc-biblioref">Chandrashekar and Sahin 2014</a>)</span>.</p>
<p>For this chapter, the reader should know the basic concepts of mlr3 (<a href="basics.html"><span>Chapter&nbsp;2</span></a>), i.e.&nbsp;know about tasks (<a href="tasks.html"><span>Appendix&nbsp;C</span></a>) and learners (<a href="basics.html#sec-learners"><span>Section&nbsp;2.2</span></a>). Basics about performance evaluation (<a href="performance.html"><span>Chapter&nbsp;3</span></a>), i.e.&nbsp;resampling (<a href="performance.html#sec-resampling"><span>Section&nbsp;3.2</span></a>) and benchmarking (<a href="performance.html#sec-benchmarking"><span>Section&nbsp;3.3</span></a>) are helpful but not strictly necessary.</p>
<section id="sec-fs-filter" class="level2 page-columns page-full" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="sec-fs-filter">
<span class="header-section-number">5.1</span> Filters</h2>
<p>Filter methods are preprocessing steps that can be applied before training a model. A very simple filter approach could look like this:</p>
<ol type="1">
<li>calculate the correlation coefficient <span class="math inline">\(\rho\)</span> between each feature and a numeric target variable, and</li>
<li>select all features with <span class="math inline">\(\rho &gt; 0.2\)</span> for further modelling steps.</li>
</ol>
<p>This approach is a <em>univariate</em> filter because it only considers the univariate relationship between each feature and the target variable. Further, it can only be applied to regression tasks with continuous features and the threshold of <span class="math inline">\(\rho &gt; 0.2\)</span> is quite arbitrary. Thus, more advanced filter methods, e.g.&nbsp;<em>multivariate</em> filters based on feature importance, usually perform better <span class="citation" data-cites="bommert2020">(<a href="references.html#ref-bommert2020" role="doc-biblioref">Bommert et al. 2020</a>)</span>. On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods. In the following, it is described how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.</p>
<p>Filter algorithms select features by assigning numeric scores to each feature, e.g.&nbsp;correlation between feature and target variables, use these to rank the features and select a feature subset based on the ranking. Features that are assigned lower scores can then be omitted in subsequent modeling steps. All filters are implemented via the package <a href="https://mlr3filters.mlr-org.com"><code>mlr3filters</code></a>. Below, we cover how to</p>
<ul>
<li>instantiate a <code>Filter</code> object,</li>
<li>calculate scores for a given task, and</li>
<li>use calculated scores to select or drop features.</li>
</ul>
<p>One special case of filters are feature importance filters (<a href="#sec-fs-var-imp-filters"><span>Section&nbsp;5.1.2</span></a>). They select features that are important according to the model induced by a selected <code>Learner</code>. Feature importance filters rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (<a href="interpretation.html"><span>Chapter&nbsp;10</span></a>) values for each feature.</p>
<p>Many filter methods are implemented in <a href="https://mlr3filters.mlr-org.com"><code>mlr3filters</code></a>, for example:</p>
<ul>
<li>Correlation, calculating Pearson or Spearman correlation between numeric features and numeric targets (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_correlation.html"><code>FilterCorrelation</code></a>)</li>
<li>Information gain, i.e.&nbsp;mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_information_gain.html"><code>FilterInformationGain</code></a>)</li>
<li>Minimal joint mutual information maximization, minimizing the joint information between selected features to avoid redundancy (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_jmim.html"><code>FilterJMIM</code></a>)</li>
<li>Permutation score, which calculates permutation feature importance (see <a href="interpretation.html"><span>Chapter&nbsp;10</span></a>) with a given learner for each feature (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_permutation.html"><code>FilterPermutation</code></a>)</li>
<li>Area under the ROC curve calculated for each feature separately (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_auc.html"><code>FilterAUC</code></a>)</li>
</ul>
<div class="page-columns page-full"><p>Most of the filter methods have some limitations, e.g.&nbsp;the correlation filter can only be calculated for regression tasks with numeric features. For a full list of all implemented filter methods we refer the reader to the <a href="https://mlr3filters.mlr-org.com">mlr3filters website</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, which also shows the supported task and features types. A benchmark of filter methods was performed by <span class="citation" data-cites="bommert2020">Bommert et al. (<a href="references.html#ref-bommert2020" role="doc-biblioref">2020</a>)</span>, who recommend to not rely on a single filter method but try several ones if the available computational resources allow. If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see <a href="#sec-fs-var-imp-filters"><span>Section&nbsp;5.1.2</span></a>), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<a href="https://mlr3filters.mlr-org.com">https://mlr3filters.mlr-org.com</a></p></li></div></div>
<section id="sec-fs-calc" class="level3" data-number="5.1.1"><h3 data-number="5.1.1" class="anchored" data-anchor-id="sec-fs-calc">
<span class="header-section-number">5.1.1</span> Calculating Filter Values</h3>
<p>The first step is to create a new R object using the class of the desired filter method. Similar to other instances in <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, these are registered in a dictionary (<a href="https://mlr3filters.mlr-org.com/reference/mlr_filters.html"><code>mlr_filters</code></a>) with an associated shortcut function <a href="https://mlr3filters.mlr-org.com/reference/flt.html"><code>flt()</code></a>. Each object of class <code>Filter</code> has a <code>$calculate()</code> method which computes the filter values and ranks them in a descending order. For example, we can use the information gain filter described above:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-001_2b25248cf16ce1e77ec38dc74747f13e">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">library</span>(<span class="st">"mlr3verse"</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Such a <code>Filter</code> object can now be used to calculate the filter on the penguins data and get the results:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-002_6113e2f96457c6ec9a5e6d5df8c18a6b">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="fu">as.data.table</span>(filter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature       score
1: flipper_length 0.581167901
2:    bill_length 0.544896584
3:     bill_depth 0.538718879
4:         island 0.520157171
5:      body_mass 0.442879511
6:            sex 0.007244168
7:           year 0.000000000</code></pre>
</div>
</div>
<p>Some filters have hyperparameters, which can be changed similar to setting hyperparameters of a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> using <code>$param_set$values</code>. For example, to calculate <code>"spearman"</code> instead of <code>"pearson"</code> correlation with the correlation filter:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-003_1707e6ccd3d02a721e6184f916c898fd">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>filter_cor <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"correlation"</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a>filter_cor<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> <span class="fu">list</span>(<span class="at">method =</span> <span class="st">"spearman"</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>filter_cor<span class="sc">$</span>param_set</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ParamSet&gt;
       id    class lower upper nlevels    default    value
1:    use ParamFct    NA    NA       5 everything         
2: method ParamFct    NA    NA       3    pearson spearman</code></pre>
</div>
</div>
</section><section id="sec-fs-var-imp-filters" class="level3 page-columns page-full" data-number="5.1.2"><h3 data-number="5.1.2" class="anchored" data-anchor-id="sec-fs-var-imp-filters">
<span class="header-section-number">5.1.2</span> Feature Importance Filters</h3>
<p>To use feature importance filters, we can use a learner with integrated feature importance methods. All learners with the property “importance” have this functionality. A list of all learners with this property can be found with</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-004_8fdb31120693ecaa1950bbfeeba2ebdf">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu">as.data.table</span>(mlr_learners)[<span class="fu">sapply</span>(properties, <span class="cf">function</span>(x) <span class="st">"importance"</span> <span class="sc">%in%</span> x)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                         key                              label task_type
 1:         classif.catboost                  Gradient Boosting   classif
 2:      classif.featureless Featureless Classification Learner   classif
 3:              classif.gbm                  Gradient Boosting   classif
 4: classif.imbalanced_rfsrc           Imbalanced Random Forest   classif
 5:         classif.lightgbm                  Gradient Boosting   classif
---                                                                      
22:                 surv.gbm                  Gradient Boosting      surv
23:              surv.mboost Boosted Generalized Additive Model      surv
24:              surv.ranger                      Random Forest      surv
25:               surv.rfsrc                      Random Forest      surv
26:             surv.xgboost                  Gradient Boosting      surv
4 variables not shown: [feature_types, packages, properties, predict_types]</code></pre>
</div>
</div>
<div class="page-columns page-full"><p>or on the <a href="https://mlr-org.com/learners.html">mlr3 website</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;<a href="https://mlr-org.com/learners.html">https://mlr-org.com/learners.html</a></p></li></div></div>
<p>For some learners, the desired filter method needs to be set during learner creation. For example, learner <a href="https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html"><code>classif.ranger</code></a> comes with multiple integrated methods, c.f. the help page of <a href="https://www.rdocumentation.org/packages/ranger/topics/ranger"><code>ranger::ranger()</code></a>. To use the feature importance method “impurity”, select it during learner construction:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-005_0ae3306da82abd6a0f67c835f6793878">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>lrn <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">importance =</span> <span class="st">"impurity"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use the <a href="https://mlr3filters.mlr-org.com/reference/mlr_filters_importance.html"><code>FilterImportance</code></a> filter class:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-006_ee748e91efab68ac1e81b8b28902e2ac">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co"># Remove observations with missing data</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>task<span class="sc">$</span><span class="fu">filter</span>(<span class="fu">which</span>(<span class="fu">complete.cases</span>(task<span class="sc">$</span><span class="fu">data</span>())))</span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"importance"</span>, <span class="at">learner =</span> lrn)</span>
<span id="cb9-7"><a href="#cb9-7"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="fu">as.data.table</span>(filter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature     score
1:    bill_length 76.374739
2: flipper_length 45.348924
3:     bill_depth 36.305939
4:      body_mass 26.457564
5:         island 24.077990
6:            sex  1.597289
7:           year  1.215536</code></pre>
</div>
</div>
</section><section id="sec-fs-embedded-methods" class="level3" data-number="5.1.3"><h3 data-number="5.1.3" class="anchored" data-anchor-id="sec-fs-embedded-methods">
<span class="header-section-number">5.1.3</span> Embedded Methods</h3>
<p>Many learners internally select a subset of the features which they find helpful for prediction, but ignore other features. For example, a decision tree might never select some features for splitting. These subsets can be used for feature selection, which we call embedded methods because the feature selection is embedded in the learner. The selected features (and those not selected) can be queried if the learner has the <code>"selected_features"</code> property, as the following example demonstrates:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-007_1a5c5057a92e20e15dcfe3b8b95c10b5">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># ensure that the learner selects features</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="fu">stopifnot</span>(<span class="st">"selected_features"</span> <span class="sc">%in%</span> learner<span class="sc">$</span>properties)</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a>learner <span class="ot">=</span> learner<span class="sc">$</span><span class="fu">train</span>(task)</span>
<span id="cb11-8"><a href="#cb11-8"></a>learner<span class="sc">$</span><span class="fu">selected_features</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "flipper_length" "bill_length"    "island"        </code></pre>
</div>
</div>
<p>The features selected by the model can be extracted by a <code>Filter</code> object, where <code>$calculate()</code> corresponds to training the learner on the given task:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-008_d0dd72fab2c53cea315e8d1dd3dc5e75">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"selected_features"</span>, <span class="at">learner =</span> learner)</span>
<span id="cb13-2"><a href="#cb13-2"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="fu">as.data.table</span>(filter)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature score
1:         island     1
2: flipper_length     1
3:    bill_length     1
4:     bill_depth     0
5:            sex     0
6:           year     0
7:      body_mass     0</code></pre>
</div>
</div>
<p>Contrary to other filter methods, embedded methods just return value of 1 (selected features) and 0 (dropped feature).</p>
</section><section id="filter-based-feature-selection" class="level3" data-number="5.1.4"><h3 data-number="5.1.4" class="anchored" data-anchor-id="filter-based-feature-selection">
<span class="header-section-number">5.1.4</span> Filter-based Feature Selection</h3>
<p>After calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modelling steps. For the <code>"selected_features"</code> filter described in embedded methods (<a href="#sec-fs-embedded-methods"><span>Section&nbsp;5.1.3</span></a>), this step is straight-forward since the methods assigns either a value of 1 for a feature to be kept or 0 for a feature to be dropped. With <code>task$select()</code> the features with a value of 1 can be selected:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-009_35e20365a5acc773d965e9f6c2ac07f9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb15-2"><a href="#cb15-2"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb15-3"><a href="#cb15-3"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"selected_features"</span>, <span class="at">learner =</span> learner)</span>
<span id="cb15-4"><a href="#cb15-4"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="co"># select all features used by rpart</span></span>
<span id="cb15-7"><a href="#cb15-7"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">which</span>(filter<span class="sc">$</span>scores <span class="sc">==</span> <span class="dv">1</span>))</span>
<span id="cb15-8"><a href="#cb15-8"></a>task<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb15-9"><a href="#cb15-9"></a>task<span class="sc">$</span>feature_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_length"    "flipper_length" "island"        </code></pre>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>To select features, we use the function <code>task$select()</code> and not <code>task$filter()</code>, which is used to filter rows (not columns) of the data matrix, see task mutators (<a href="basics.html#sec-tasks-mutators"><span>Section&nbsp;2.1.3</span></a>).</p>
</div>
</div>
<p>For filter methods which assign continuous scores, there are essentially two ways to select features:</p>
<ul>
<li>select the top <span class="math inline">\(k\)</span> features, or</li>
<li>select all features with a score above a threshold <span class="math inline">\(\tau\)</span>.</li>
</ul>
<p>Where the first option is equivalent to dropping the bottom <span class="math inline">\(p-k\)</span> features. For both options, one has to decide on a threshold, which is often quite arbitrary. For example, to implement the first option with the information gain filter:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-010_ef97db3063bd0160b9473d87299f2f51">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb17-2"><a href="#cb17-2"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb17-3"><a href="#cb17-3"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb17-4"><a href="#cb17-4"></a></span>
<span id="cb17-5"><a href="#cb17-5"></a><span class="co"># select top 3 features from information gain filter</span></span>
<span id="cb17-6"><a href="#cb17-6"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">head</span>(filter<span class="sc">$</span>scores, <span class="dv">3</span>))</span>
<span id="cb17-7"><a href="#cb17-7"></a>task<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb17-8"><a href="#cb17-8"></a>task<span class="sc">$</span>feature_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_depth"     "bill_length"    "flipper_length"</code></pre>
</div>
</div>
<p>Or, the second option with <span class="math inline">\(\tau = 0.5\)</span>:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-011_912b4b064c57a73191a896c3fe74f7dd">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb19-2"><a href="#cb19-2"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb19-3"><a href="#cb19-3"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="co"># select all features with score &gt;0.5 from information gain filter</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">which</span>(filter<span class="sc">$</span>scores <span class="sc">&gt;</span> <span class="fl">0.5</span>))</span>
<span id="cb19-7"><a href="#cb19-7"></a>task<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb19-8"><a href="#cb19-8"></a>task<span class="sc">$</span>feature_names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_depth"     "bill_length"    "flipper_length" "island"        </code></pre>
</div>
</div>
<p>Filters can be integrated into pipelines. While pipelines are described in detail in <a href="pipelines.html"><span>Chapter&nbsp;6</span></a>, here is a brief preview:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-012_bc9836925599898b86d724b45a6e1afe">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="fu">library</span>(mlr3pipelines)</span>
<span id="cb21-2"><a href="#cb21-2"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="co"># combine filter (keep top 3 features) with learner</span></span>
<span id="cb21-5"><a href="#cb21-5"></a>graph <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"filter"</span>, <span class="at">filter =</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>), <span class="at">filter.nfeat =</span> <span class="dv">3</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb21-6"><a href="#cb21-6"></a>  <span class="fu">po</span>(<span class="st">"learner"</span>, <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span>
<span id="cb21-7"><a href="#cb21-7"></a></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="co"># now it can be used as any learner, but it includes the feature selection</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>learner <span class="ot">=</span> <span class="fu">as_learner</span>(graph)</span>
<span id="cb21-10"><a href="#cb21-10"></a>learner<span class="sc">$</span><span class="fu">train</span>(task)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Pipelines can also be used to apply hyperparameter optimization (<a href="optimization.html"><span>Chapter&nbsp;4</span></a>) to the filter, i.e.&nbsp;tune the filter threshold to optimize the feature selection regarding prediction performance. We first combine a filter with a learner,</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-013_4ed1b9edb19530a72096e12944208f32">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>graph <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"filter"</span>, <span class="at">filter =</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>  <span class="fu">po</span>(<span class="st">"learner"</span>, <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span>
<span id="cb22-3"><a href="#cb22-3"></a>learner <span class="ot">=</span> <span class="fu">as_learner</span>(graph)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and tune how many feature to include</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-014_48accbf33e9dea9aea8b0cf1af97453d">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="fu">library</span>(<span class="st">"mlr3tuning"</span>)</span>
<span id="cb23-2"><a href="#cb23-2"></a>ps <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">information_gain.filter.nfeat =</span> <span class="fu">p_int</span>(<span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="dv">7</span>))</span>
<span id="cb23-3"><a href="#cb23-3"></a>instance <span class="ot">=</span> TuningInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(</span>
<span id="cb23-4"><a href="#cb23-4"></a>  <span class="at">task =</span> task,</span>
<span id="cb23-5"><a href="#cb23-5"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb23-6"><a href="#cb23-6"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb23-7"><a href="#cb23-7"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb23-8"><a href="#cb23-8"></a>  <span class="at">search_space =</span> ps,</span>
<span id="cb23-9"><a href="#cb23-9"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb23-10"><a href="#cb23-10"></a>)</span>
<span id="cb23-11"><a href="#cb23-11"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>)</span>
<span id="cb23-12"><a href="#cb23-12"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   information_gain.filter.nfeat learner_param_vals  x_domain classif.acc
1:                             5          &lt;list[2]&gt; &lt;list[1]&gt;   0.9391304</code></pre>
</div>
</div>
<p>The output above shows only the best result. To show the results of all tuning steps, retrieve them from the archive of the tuning instance:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-015_1e7913789eba075974e83500f90925d7">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   information_gain.filter.nfeat classif.acc
1:                             2   0.9304348
2:                             5   0.9391304
3:                             1   0.7478261
4:                             7   0.9391304
5:                             3   0.9391304
6:                             6   0.9391304
7:                             4   0.9391304
7 variables not shown: [x_domain_information_gain.filter.nfeat, runtime_learners, timestamp, batch_nr, warnings, errors, resample_result]</code></pre>
</div>
</div>
<p>We can also plot the tuning results:</p>
<div class="cell" data-hash="feature-selection_cache/html/fig-tunefilter_7ca0743eede1d4637ef55d701bab02f5">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="fu">autoplot</span>(instance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-tunefilter" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="feature-selection_files/figure-html/fig-tunefilter-1.png" class="img-fluid figure-img" alt="Plot showing model performance in filter-based feature selection, showing that adding a second and third feature to the model improves performance, while adding more feature achieves no further performance gain." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.1: Model performance with different numbers of features, selected by an information gain filter.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For more details, see Pipelines (<a href="pipelines.html"><span>Chapter&nbsp;6</span></a>) and Hyperparameter Optimization (<a href="optimization.html"><span>Chapter&nbsp;4</span></a>).</p>
</section></section><section id="sec-fs-wrapper" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="sec-fs-wrapper">
<span class="header-section-number">5.2</span> Wrapper Methods</h2>
<p>Wrapper methods work by fitting models on selected feature subsets and evaluating their performance. This can be done in a sequential fashion, e.g.&nbsp;by iteratively adding features to the model in the so-called sequential forward selection, or in a parallel fashion, e.g.&nbsp;by evaluating random feature subsets in a random search. Below, the use of these simple approaches is described in a common framework along with more advanced methods such as genetic search. It is further shown how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.</p>
<p>In more detail, wrapper methods iteratively select features that optimize a performance measure. Instead of ranking features, a model is fit on a selected subset of features in each iteration and evaluated in resampling with respect to a selected performance measure. The strategy that determines which feature subset is used in each iteration is given by the <code>FSelector</code> object. A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively adds the feature that leads to the largest performance improvement. Wrapper methods can be used with any learner but need to train the learner potentially many times, leading to a computationally intensive method. All wrapper methods are implemented via the package <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a>. In this chapter, we cover how to</p>
<ul>
<li>instantiate an <code>FSelector</code> object,</li>
<li>configure it, to e.g.&nbsp;respect a runtime limit or for different objectives,</li>
<li>run it or fuse it with a <code>Learner</code> via an <code>AutoFSelector</code>.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Wrapper-based feature selection is very similar to hyperparameter optimization (<a href="optimization.html"><span>Chapter&nbsp;4</span></a>). The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations. We will see below, that we can even use the same terminators, that some feature selection algorithms are similar to tuners and that we can also optimize multiple performance measures with feature selection.</p>
</div>
</div>
<section id="sec-fs-wrapper-example" class="level3" data-number="5.2.1"><h3 data-number="5.2.1" class="anchored" data-anchor-id="sec-fs-wrapper-example">
<span class="header-section-number">5.2.1</span> Simple Forward Selection Example</h3>
<p>We start with the simple example from above and do sequential forward selection with the penguins data:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-017_848313add82ce96b457d7793dd932ad7">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="fu">library</span>(<span class="st">"mlr3fselect"</span>)</span>
<span id="cb28-2"><a href="#cb28-2"></a></span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="co"># subset features to ease visualization</span></span>
<span id="cb28-4"><a href="#cb28-4"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb28-5"><a href="#cb28-5"></a>task<span class="sc">$</span><span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"bill_depth"</span>, <span class="st">"bill_length"</span>, <span class="st">"body_mass"</span>, <span class="st">"flipper_length"</span>))</span>
<span id="cb28-6"><a href="#cb28-6"></a></span>
<span id="cb28-7"><a href="#cb28-7"></a>instance <span class="ot">=</span> <span class="fu">fselect</span>(</span>
<span id="cb28-8"><a href="#cb28-8"></a>  <span class="at">method =</span> <span class="st">"sequential"</span>,</span>
<span id="cb28-9"><a href="#cb28-9"></a>  <span class="at">task =</span>  task,</span>
<span id="cb28-10"><a href="#cb28-10"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb28-11"><a href="#cb28-11"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb28-12"><a href="#cb28-12"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>)</span>
<span id="cb28-13"><a href="#cb28-13"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To show all analyzed feature subsets and the corresponding performance, we use <code>as.data.table(instance$archive)</code>.</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-018_53e0d659d2b60f56f6f335f1bb0cbcb2">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a>dt <span class="ot">=</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)</span>
<span id="cb29-2"><a href="#cb29-2"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_depth bill_length body_mass flipper_length classif.acc
1:       TRUE       FALSE     FALSE          FALSE   0.6956522
2:      FALSE        TRUE     FALSE          FALSE   0.7652174
3:      FALSE       FALSE      TRUE          FALSE   0.7043478
4:      FALSE       FALSE     FALSE           TRUE   0.7913043</code></pre>
</div>
</div>
<p>We see that the feature <code>flipper_length</code> achieved the highest prediction performance in the first iteration and is thus selected. In the second round, adding <code>bill_length</code> improves performance to over 90%:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-018-2_38c2ec7d9b9420fc39c68ab799d57d64">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_depth bill_length body_mass flipper_length classif.acc
1:       TRUE       FALSE     FALSE           TRUE   0.7652174
2:      FALSE        TRUE     FALSE           TRUE   0.9391304
3:      FALSE       FALSE      TRUE           TRUE   0.8173913</code></pre>
</div>
</div>
<p>However, adding a third feature does not improve performance</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-018-3_97e94fa3f35db49ad38a6627203d3f4f">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">3</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_depth bill_length body_mass flipper_length classif.acc
1:       TRUE        TRUE     FALSE           TRUE   0.9391304
2:      FALSE        TRUE      TRUE           TRUE   0.9391304</code></pre>
</div>
</div>
<p>and the algorithm terminates. To directly show the best feature set, we can use:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-019_9823424128660ae4476bc73cd5e65a76">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a>instance<span class="sc">$</span>result_feature_set</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_length"    "flipper_length"</code></pre>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>instance$result_feature_set</code> shows features in alphabetical order and not in the order selected.</p>
</div>
</div>
<p>Internally, the <code>fselect</code> function creates an <code>FSelectInstanceSingleCrit</code> object and executes the feature selection with an <code>FSelector</code> object, based on the selected method, in this example an <code>FSelectorSequential</code> object. It uses the supplied resampling and measure to evaluate all feature subsets provided by the <code>FSelector</code> on the task.</p>
<p>At the heart of <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a> are the R6 classes:</p>
<ul>
<li>
<code>FSelectInstanceSingleCrit</code>, <code>FSelectInstanceMultiCrit</code>: These two classes describe the feature selection problem and store the results.</li>
<li>
<code>FSelector</code>: This class is the base class for implementations of feature selection algorithms.</li>
</ul>
<p>In the following two sections, these classes will be created manually, to learn more about the <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a> package.</p>
</section><section id="the-fselectinstance-classes" class="level3" data-number="5.2.2"><h3 data-number="5.2.2" class="anchored" data-anchor-id="the-fselectinstance-classes">
<span class="header-section-number">5.2.2</span> The <code>FSelectInstance</code> Classes</h3>
<p>To create an <code>FSelectInstanceSingleCrit</code> object, we use the sugar function <a href="https://mlr3fselect.mlr-org.com/reference/fsi.html"><code>fsi</code></a>, which is short for <code>FSelectInstanceSingleCrit$new()</code> or <code>FSelectInstanceMultiCrit$new()</code>, depending on the selected measure(s):</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-020_21b34ff96c0e503acaf06b01aa35a8c6">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a>instance <span class="ot">=</span> <span class="fu">fsi</span>(</span>
<span id="cb37-2"><a href="#cb37-2"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>),</span>
<span id="cb37-3"><a href="#cb37-3"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb37-4"><a href="#cb37-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb37-5"><a href="#cb37-5"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb37-6"><a href="#cb37-6"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>)</span>
<span id="cb37-7"><a href="#cb37-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we have not selected a feature selection algorithm and thus did not select any features, yet. We have also supplied a so-called <code>Terminator</code>, which is used to stop the feature selection. For the forward selection in the example above, we did not need a terminator because we simply tried all remaining features until the full model or no further performance improvement. However, for other feature selection algorithms such as <a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_random_search.html"><code>random search</code></a>, a terminator is required. The following terminator are available:</p>
<ul>
<li>Terminate after a given time (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_clock_time.html"><code>TerminatorClockTime</code></a>)</li>
<li>Terminate after a given amount of iterations (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_evals.html"><code>TerminatorEvals</code></a>)</li>
<li>Terminate after a specific performance is reached (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_perf_reached.html"><code>TerminatorPerfReached</code></a>)</li>
<li>Terminate when feature selection does not improve (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_stagnation.html"><code>TerminatorStagnation</code></a>)</li>
<li>A combination of the above in an <em>ALL</em> or <em>ANY</em> fashion (<a href="https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html"><code>TerminatorCombo</code></a>)</li>
</ul>
<p>Above we used the sugar function <a href="https://bbotk.mlr-org.com/reference/trm.html"><code>trm</code></a> to select <a href="https://bbotk.mlr-org.com/reference/mlr_terminators_evals.html"><code>TerminatorEvals</code></a> with 20 evaluations.</p>
<p>To start the feature selection, we still need to select an algorithm which are defined via the <code>FSelector</code> class, described in the next section.</p>
</section><section id="the-fselector-class" class="level3" data-number="5.2.3"><h3 data-number="5.2.3" class="anchored" data-anchor-id="the-fselector-class">
<span class="header-section-number">5.2.3</span> The <code>FSelector</code> Class</h3>
<p>The <code>FSelector</code> class is the base class for different feature selection algorithms. The following algorithms are currently implemented in <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a>:</p>
<ul>
<li>Random search, trying random feature subsets until termination (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_random_search.html"><code>FSelectorRandomSearch</code></a>)</li>
<li>Exhaustive search, trying all possible feature subsets (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_exhaustive_search.html"><code>FSelectorExhaustiveSearch</code></a>)</li>
<li>Sequential search, i.e.&nbsp;sequential forward or backward selection (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_sequential.html"><code>FSelectorSequential</code></a>)</li>
<li>Recursive feature elimination, which uses learner’s importance scores to iteratively remove features with low feature importance (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html"><code>FSelectorRFE</code></a>)</li>
<li>Design points, trying all user-supplied feature sets (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_design_points.html"><code>FSelectorDesignPoints</code></a>)</li>
<li>Genetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_genetic_search.html"><code>FSelectorGeneticSearch</code></a>)</li>
<li>Shadow variable search, which adds permuted copies of all features (shadow variables) and stops when a shadow variable is selected (<a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html"><code>FSelectorShadowVariableSearch</code></a>)</li>
</ul>
<p>In this example, we will use a simple random search and retrieve it from the dictionary <a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors.html"><code>mlr_fselectors</code></a> with the <a href="https://mlr3fselect.mlr-org.com/reference/fs.html"><code>fs()</code></a> sugar function, which is short for <code>FSelectorRandomSearch$new()</code>:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-021_af8c31ec8a8b3ab8f3d90608f67c8779">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a>fselector <span class="ot">=</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="starting-the-feature-selection" class="level3" data-number="5.2.4"><h3 data-number="5.2.4" class="anchored" data-anchor-id="starting-the-feature-selection">
<span class="header-section-number">5.2.4</span> Starting the Feature Selection</h3>
<p>To start the feature selection, we pass the <code>FSelectInstanceSingleCrit</code> object to the <code>$optimize()</code> method of the initialized <code>FSelector</code> object:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-022_9d8f9fee6d266e44ac5555d39a481661">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a>fselector<span class="sc">$</span><span class="fu">optimize</span>(instance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The algorithm proceeds as follows</p>
<ol type="1">
<li>The <code>FSelector</code> proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting <code>batch_size</code>.</li>
<li>For each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.</li>
<li>All evaluations are stored in the archive of the <code>FSelectInstanceSingleCrit</code> object.</li>
<li>The terminator is queried if the budget is exhausted. If the budget is not exhausted, restart with 1) until it is.</li>
<li>Determine the feature subset with the best observed performance.</li>
<li>Store the best feature subset as the result in the instance object.</li>
</ol>
<p>The best feature subset and the corresponding measured performance can be accessed from the instance:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-023_64d2dfab42eda430ae8ceed8e3fe2e6f">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>result)[, .(features, classif.acc)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                         features classif.acc
1: bill_depth,bill_length,body_mass,flipper_length,island,sex,...   0.9391304</code></pre>
</div>
</div>
<p>As in the forward selection example above, one can investigate all resamplings which were undertaken, as they are stored in the archive of the <code>FSelectInstanceSingleCrit</code> object and can be accessed by using <code>as.data.table()</code>:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-024_9a5fc4885df1616ed9171317645bc61b">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, .(bill_depth, bill_length, body_mass, classif.acc)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    bill_depth bill_length body_mass classif.acc
 1:       TRUE        TRUE      TRUE   0.9391304
 2:      FALSE       FALSE      TRUE   0.7391304
 3:       TRUE        TRUE      TRUE   0.9391304
 4:       TRUE        TRUE      TRUE   0.9391304
 5:       TRUE        TRUE      TRUE   0.9391304
 6:      FALSE       FALSE     FALSE   0.6869565
 7:       TRUE       FALSE      TRUE   0.8086957
 8:      FALSE       FALSE      TRUE   0.7391304
 9:       TRUE       FALSE     FALSE   0.7739130
10:       TRUE       FALSE     FALSE   0.8000000
11:      FALSE       FALSE     FALSE   0.8086957
12:      FALSE       FALSE      TRUE   0.6869565
13:       TRUE        TRUE      TRUE   0.9391304
14:      FALSE       FALSE     FALSE   0.6173913
15:       TRUE        TRUE      TRUE   0.9217391
16:       TRUE        TRUE      TRUE   0.9391304
17:       TRUE        TRUE      TRUE   0.9043478
18:      FALSE       FALSE      TRUE   0.7391304
19:      FALSE       FALSE     FALSE   0.7739130
20:      FALSE       FALSE     FALSE   0.8086957</code></pre>
</div>
</div>
<p>Now the optimized feature subset can be used to subset the task and fit the model on all observations:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-025_219da3d250ed1da21568425a7a9dfe0f">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb44-2"><a href="#cb44-2"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb44-3"><a href="#cb44-3"></a></span>
<span id="cb44-4"><a href="#cb44-4"></a>task<span class="sc">$</span><span class="fu">select</span>(instance<span class="sc">$</span>result_feature_set)</span>
<span id="cb44-5"><a href="#cb44-5"></a>learner<span class="sc">$</span><span class="fu">train</span>(task)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The trained model can now be used to make a prediction on external data.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Predicting on observations present in the task used for feature selection should be avoided. The model has seen these observations already during feature selection and therefore performance evaluation results would be over-optimistic. Instead, to get unbiased performance estimates for the current task, nested resampling (see <a href="#sec-autofselect"><span>Section&nbsp;5.2.6</span></a> and <a href="optimization.html#sec-nested-resampling"><span>Section&nbsp;4.5</span></a>) is required.</p>
</div>
</div>
</section><section id="sec-multicrit-featsel" class="level3" data-number="5.2.5"><h3 data-number="5.2.5" class="anchored" data-anchor-id="sec-multicrit-featsel">
<span class="header-section-number">5.2.5</span> Optimizing Multiple Performance Measures</h3>
<p>You might want to use multiple criteria to evaluate the performance of the feature subsets. For example, you might want to select the subset with the highest classification accuracy and lowest time to train the model. However, these two subsets will generally not coincide, i.e.&nbsp;the subset with highest classification accuracy will probably be another subset than that with lowest training time. With <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a>, the result is the pareto-optimal solution, i.e.&nbsp;the best feature subset for each of the criteria that is not dominated by another subset. For the example with classification accuracy and training time, a feature subset that is best in accuracy <em>and</em> training time will dominate all other subsets and thus will be the only pareto-optimal solution. If, however, different subsets are best in the two criteria, both subsets are pareto-optimal.</p>
<p>We will expand the previous example and perform feature selection on the penguins dataset, however, this time we will use <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceMultiCrit.html"><code>FSelectInstanceMultiCrit</code></a> to select the subset of features that has the highest classification accuracy and the one with the lowest time to train the model.</p>
<p>The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-026_d4d6556661bdc33eed528ea02ce7e131">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a>instance <span class="ot">=</span> <span class="fu">fsi</span>(</span>
<span id="cb45-2"><a href="#cb45-2"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>),</span>
<span id="cb45-3"><a href="#cb45-3"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb45-4"><a href="#cb45-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb45-5"><a href="#cb45-5"></a>  <span class="at">measure =</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.acc"</span>, <span class="st">"time_train"</span>)),</span>
<span id="cb45-6"><a href="#cb45-6"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">5</span>)</span>
<span id="cb45-7"><a href="#cb45-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The function <a href="https://mlr3fselect.mlr-org.com/reference/fsi.html"><code>fsi</code></a> creates an instance of <code>FSelectInstanceMultiCrit</code> if more than one measure is selected. We now create an <code>FSelector</code> and call the <code>$optimize()</code> function of the <code>FSelector</code> with the <code>FSelectInstanceMultiCrit</code> object, to search for the subset of features with the best classification accuracy and time to train the model. This time, we use <a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_design_points.html"><code>design points</code></a> to manually specify two feature sets to try: one with only the feature <code>sex</code> and one with all features except <code>island</code>, <code>sex</code> and <code>year</code>. We expect the sex-only model to train fast and the model including many features to be accurate.</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-027_176b599d5a03bd6872f495ea0523791f">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1"></a>design <span class="ot">=</span> mlr3misc<span class="sc">::</span><span class="fu">rowwise_table</span>(</span>
<span id="cb46-2"><a href="#cb46-2"></a>  <span class="sc">~</span>bill_depth, <span class="sc">~</span>bill_length, <span class="sc">~</span>body_mass, <span class="sc">~</span>flipper_length, <span class="sc">~</span>island, <span class="sc">~</span>sex, <span class="sc">~</span>year,</span>
<span id="cb46-3"><a href="#cb46-3"></a>  <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>,</span>
<span id="cb46-4"><a href="#cb46-4"></a>  <span class="cn">TRUE</span>, <span class="cn">TRUE</span>, <span class="cn">TRUE</span>, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span></span>
<span id="cb46-5"><a href="#cb46-5"></a>)</span>
<span id="cb46-6"><a href="#cb46-6"></a></span>
<span id="cb46-7"><a href="#cb46-7"></a>fselector <span class="ot">=</span> <span class="fu">fs</span>(<span class="st">"design_points"</span>, <span class="at">design =</span> design)</span>
<span id="cb46-8"><a href="#cb46-8"></a>fselector<span class="sc">$</span><span class="fu">optimize</span>(instance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As above, the best feature subset and the corresponding measured performance can be accessed from the instance. However, in this simple case, if the fastest subset is not also the best performing subset, the result consists of two subsets: one with the lowest training time and one with the best classification accuracy:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-029_49e582bb23996f7645dbe226001efbe1">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>result)[, .(features, classif.acc, time_train)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                          features classif.acc time_train
1:                                             sex   0.4347826      0.003
2: bill_depth,bill_length,body_mass,flipper_length   0.9304348      0.004</code></pre>
</div>
</div>
<p>As explained above, the result is the pareto-optimal solution.</p>
</section><section id="sec-autofselect" class="level3" data-number="5.2.6"><h3 data-number="5.2.6" class="anchored" data-anchor-id="sec-autofselect">
<span class="header-section-number">5.2.6</span> Automating the Feature Selection</h3>
<p>The <code>AutoFSelector</code> class wraps a learner and augments it with an automatic feature selection for a given task. Because the <code>AutoFSelector</code> itself inherits from the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> base class, it can be used like any other learner. Below, a new learner is created. This learner is then wrapped in a random search feature selector, which automatically starts a feature selection on the given task using an inner resampling, as soon as the wrapped learner is trained. Here, the function <a href="https://mlr3fselect.mlr-org.com/reference/auto_fselector.html"><code>auto_fselector</code></a> creates an instance of <code>AutoFSelector</code>, i.e.&nbsp;it is short for <code>AutoFSelector$new()</code>.</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-030_ed209f54619c48e80f3d4ada8e5f6dcb">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a>at <span class="ot">=</span> <span class="fu">auto_fselector</span>(</span>
<span id="cb49-2"><a href="#cb49-2"></a>  <span class="at">method =</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>),</span>
<span id="cb49-3"><a href="#cb49-3"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.log_reg"</span>),</span>
<span id="cb49-4"><a href="#cb49-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb49-5"><a href="#cb49-5"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb49-6"><a href="#cb49-6"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">10</span>)</span>
<span id="cb49-7"><a href="#cb49-7"></a>)</span>
<span id="cb49-8"><a href="#cb49-8"></a>at</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;AutoFSelector:classif.log_reg.fselector&gt;
* Model: list
* Packages: mlr3, mlr3fselect, mlr3learners, stats
* Predict Type: response
* Feature Types: logical, integer, numeric, character, factor, ordered
* Properties: loglik, twoclass</code></pre>
</div>
</div>
<p>We can now, as with any other learner, call the <code>$train()</code> and <code>$predict()</code> method. This time however, we pass it to <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> to compare the optimized feature subset to the complete feature set. This way, the <code>AutoFSelector</code> will do its resampling for feature selection on the training set of the respective split of the outer resampling. The learner then undertakes predictions using the test set of the outer resampling. Here, the outer resampling refers to the resampling specified in <code>benchmark()</code>, whereas the inner resampling is that specified in <code>auto_fselector()</code>. This is called nested resampling (<a href="optimization.html#sec-nested-resampling"><span>Section&nbsp;4.5</span></a>) and yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.</p>
<p>In the call to <code>benchmark()</code>, we compare our wrapped learner <code>at</code> with a normal logistic regression <code>lrn("classif.log_reg")</code>. For that, we create a benchmark grid with the task, the learners and a 3-fold cross validation on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html"><code>sonar</code></a> data.</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-031_275319b1a1aa1ea96f04cc14571ebf60">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a>grid <span class="ot">=</span> <span class="fu">benchmark_grid</span>(</span>
<span id="cb51-2"><a href="#cb51-2"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb51-3"><a href="#cb51-3"></a>  <span class="at">learner =</span> <span class="fu">list</span>(at, <span class="fu">lrn</span>(<span class="st">"classif.log_reg"</span>)),</span>
<span id="cb51-4"><a href="#cb51-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb51-5"><a href="#cb51-5"></a>)</span>
<span id="cb51-6"><a href="#cb51-6"></a></span>
<span id="cb51-7"><a href="#cb51-7"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(grid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we compare those two learners regarding classification accuracy and training time:</p>
<div class="cell" data-hash="feature-selection_cache/html/feature-selection-032_4447b02d393f818853f38674ee56bf29">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1"></a>aggr <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.acc"</span>, <span class="st">"time_train"</span>)))</span>
<span id="cb52-2"><a href="#cb52-2"></a><span class="fu">as.data.table</span>(aggr)[, .(learner_id, classif.acc, time_train)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  learner_id classif.acc time_train
1: classif.log_reg.fselector   0.7351967   1.073333
2:           classif.log_reg   0.6585231   0.034000</code></pre>
</div>
</div>
<p>We can see that, in this example, the feature selection improves prediction performance but also drastically increases the training time, since the feature selection (including resampling and random search) is part of the model training of the wrapped learner.</p>
</section></section><section id="conclusion" class="level2 page-columns page-full" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">5.3</span> Conclusion</h2>
<p>In this chapter, we learned how to perform feature selection with mlr3. We introduced filter and wrapper methods, combined feature selection with pipelines, learned how to automate the feature selection and covered the optimization of multiple performance measures. <a href="#tbl-api-feature-selection">Table&nbsp;<span>5.1</span></a> gives an overview of the most important functions (S3) and classes (R6) used in this chapter.</p>
<div id="tbl-api-feature-selection" class="anchored">
<table class="table">
<caption>Table&nbsp;5.1: Core S3 ‘sugar’ functions for feature selection in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.</caption>
<thead><tr class="header">
<th>S3 function</th>
<th>R6 Class</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3filters.mlr-org.com/reference/flt.html"><code>flt()</code></a></td>
<td><a href="https://www.rdocumentation.org/packages/base/topics/funprog"><code>Filter</code></a></td>
<td>Selects features by calculating a score for each feature</td>
</tr>
<tr class="even">
<td><code>Filter$calculate()</code></td>
<td><a href="https://www.rdocumentation.org/packages/base/topics/funprog"><code>Filter</code></a></td>
<td>Calculates scores on a given task</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3fselect.mlr-org.com/reference/fselect.html"><code>fselect()</code></a></td>
<td>
<a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html"><code>FSelectInstanceSingleCrit</code></a> or <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceMultiCrit.html"><code>FSelectInstanceMultiCrit</code></a>
</td>
<td>Specifies a feature selection problem and stores the results</td>
</tr>
<tr class="even">
<td><a href="https://mlr3fselect.mlr-org.com/reference/fs.html"><code>fs()</code></a></td>
<td><a href="https://mlr3fselect.mlr-org.com/reference/FSelector.html"><code>FSelector</code></a></td>
<td>Specifies a feature selection algorithm</td>
</tr>
<tr class="odd">
<td><code>FSelector$optimize()</code></td>
<td><a href="https://mlr3fselect.mlr-org.com/reference/FSelector.html"><code>FSelector</code></a></td>
<td>Executes the features selection specified by the <code>FSelectInstance</code> with the algorithm specified by the <code>FSelector</code>
</td>
</tr>
<tr class="even">
<td><a href="https://mlr3fselect.mlr-org.com/reference/auto_fselector.html"><code>auto_fselector()</code></a></td>
<td><a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a></td>
<td>Defines a learner that includes feature selection</td>
</tr>
</tbody>
</table>
</div>
<section id="resources" class="level3 unnumbered unlisted page-columns page-full"><h3 class="unnumbered unlisted anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li>A list of implemented filters in the <a href="https://mlr3filters.mlr-org.com"><code>mlr3filters</code></a> package is provided on the <a href="https://mlr3filters.mlr-org.com">mlr3filters website</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</li>
<li>A summary of wrapper-based feature selection with the <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a> package is provided in the <a href="https://cheatsheets.mlr-org.com/mlr3fselect.pdf">mlr3fselect cheatsheet</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>An overview of feature selection methods is provided by <span class="citation" data-cites="chandrashekar2014">Chandrashekar and Sahin (<a href="references.html#ref-chandrashekar2014" role="doc-biblioref">2014</a>)</span>.</li>
<li>A more formal and detailed introduction to filters and wrappers is given in <span class="citation" data-cites="guyon2003">Guyon and Elisseeff (<a href="references.html#ref-guyon2003" role="doc-biblioref">2003</a>)</span>.</li>
<li>
<span class="citation" data-cites="bommert2020">Bommert et al. (<a href="references.html#ref-bommert2020" role="doc-biblioref">2020</a>)</span> perform a benchmark of filter methods.</li>
<li>Filters can be used as part of a machine learning pipeline (<a href="pipelines.html"><span>Chapter&nbsp;6</span></a>).</li>
<li>Filters can be optimized with hyperparameter optimization (<a href="optimization.html"><span>Chapter&nbsp;4</span></a>).</li>
</ul><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;<a href="https://mlr3filters.mlr-org.com">https://mlr3filters.mlr-org.com</a></p></li><li id="fn4"><p><sup>4</sup>&nbsp;<a href="https://cheatsheets.mlr-org.com/mlr3fselect.pdf">https://cheatsheets.mlr-org.com/mlr3fselect.pdf</a></p></li></div></section></section><section id="exercises" class="level2" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">5.4</span> Exercises</h2>
<ol type="1">
<li>Calculate a correlation filter on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_mtcars.html"><code>Motor Trend</code></a> data set (<code>mtcars</code>).</li>
<li>Use the filter from the first exercise to select the five best features in the <code>mtcars</code> data set.</li>
<li>Apply a backward selection to the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html"><code>penguins</code></a> data set with a classification tree learner <code>"classif.rpart"</code> and holdout resampling by the measure classification accuracy. Compare the results with those in <a href="#sec-fs-wrapper-example"><span>Section&nbsp;5.2.1</span></a>. Answer the following questions:
<ol type="a">
<li>Do the selected features differ?</li>
<li>Which feature selection method achieves a higher classification accuracy?</li>
<li>Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?</li>
</ol>
</li>
<li>Automate the feature selection as in <a href="#sec-autofselect"><span>Section&nbsp;5.2.6</span></a> with the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html"><code>spam</code></a> data set and a logistic regression learner (<code>"classif.log_reg"</code>). Hint: Remember to call <code><a href="https://mlr3learners.mlr-org.com">library("mlr3learners")</a></code> for the logistic regression learner.</li>
</ol>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-bommert2020" class="csl-entry" role="doc-biblioentry">
Bommert, Andrea, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel Lang. 2020. <span>“Benchmark for Filter Methods for Feature Selection in High-Dimensional Classification Data.”</span> <em>Computational Statistics &amp; Data Analysis</em> 143: 106839. https://doi.org/<a href="https://doi.org/10.1016/j.csda.2019.106839">https://doi.org/10.1016/j.csda.2019.106839</a>.
</div>
<div id="ref-chandrashekar2014" class="csl-entry" role="doc-biblioentry">
Chandrashekar, Girish, and Ferat Sahin. 2014. <span>“A Survey on Feature Selection Methods.”</span> <em>Computers and Electrical Engineering</em> 40 (1): 16–28. https://doi.org/<a href="https://doi.org/10.1016/j.compeleceng.2013.11.024">https://doi.org/10.1016/j.compeleceng.2013.11.024</a>.
</div>
<div id="ref-guyon2003" class="csl-entry" role="doc-biblioentry">
Guyon, Isabelle, and André Elisseeff. 2003. <span>“An Introduction to Variable and Feature Selection.”</span> <em>Journal of Machine Learning Research</em> 3 (Mar): 1157–82.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./optimization.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./pipelines.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pipelines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb54" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Marvin N. Wright</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-8542-6291</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">    email: wright@leibniz-bips.de</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Leibniz Institute for Prevention Research and Epidemiology – BIPS, Bremen, Germany</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: University of Bremen, Germany</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: University of Copenhagen, Denmark</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co">  Feature selection is a key component of data analysis and is part of most machine learning applications.</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co">  This chapter introduces two concepts of feature selection and how to use them in mlr3.</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co">  Filter methods are preprocessing steps that are independent of the model to be fitted, whereas wrapper methods work by fitting models on feature subsets and evaluating their performance.</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="co">  We start with simple examples such as univariate filters and sequential forward selection but also cover more advanced topics such as optimizing multiple performance measures simultaneously.</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co">  To integrate feature selection into complex machine learning pipelines and the mlr3 ecosystem, we show how to combine feature selection with mlr3 pipelines and how to automate feature selection by wrapping it in mlr3 learners.</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="fu"># Feature Selection {#sec-feature-selection}</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>{{&lt; include _setup.qmd &gt;}}</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>Feature selection, also known as variable or descriptor selection, is the process of finding a subset of features to use with a given task and learner.</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>Using an *optimal set* of features can have several benefits:</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>improved predictive performance, since we reduce overfitting on irrelevant features,</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>robust models that do not rely on noisy features,</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>simpler models that are easier to interpret,</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>faster model fitting, e.g. for model updates,</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>faster prediction, and</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>no need to collect potentially expensive features.</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>However, these objectives will not necessarily be optimized by the same *optimal set* of features and thus feature selection is inherently multi-objective.</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>In this chapter, we mostly focus on feature selection as a means of improving predictive performance, but also briefly cover optimization of multiple criteria (@sec-multicrit-featsel).</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>Reducing the amount of features can improve models across many scenarios, but it can be especially helpful in datasets that have a high number of features in comparison to the number of datapoints.</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>Many learners perform implicit, also called embedded, feature selection, e.g. via the choice of variables used for splitting in a decision tree.</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>Most other feature selection methods are model agnostic, i.e. they can be used together with any learner.</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>Of the many different approaches to identifying relevant features, we will focus on two general concepts, which are described in detail below: Filter and Wrapper methods <span class="co">[</span><span class="ot">@guyon2003;@chandrashekar2014</span><span class="co">]</span>.</span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>For this chapter, the reader should know the basic concepts of mlr3 (@sec-basics), i.e. know about tasks (@sec-tasks) and learners (@sec-learners).</span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>Basics about performance evaluation (@sec-performance), i.e. resampling (@sec-resampling) and benchmarking (@sec-benchmarking) are helpful but not strictly necessary.</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a><span class="fu">## Filters {#sec-fs-filter}</span></span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>Filter methods are preprocessing steps that can be applied before training a model.</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>A very simple filter approach could look like this:</span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>calculate the correlation coefficient $\rho$ between each feature and a numeric target variable, and</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>select all features with $\rho &gt; 0.2$ for further modelling steps.</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>This approach is a *univariate* filter because it only considers the univariate relationship between each feature and the target variable.</span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>Further, it can only be applied to regression tasks with continuous features and the threshold of $\rho &gt; 0.2$ is quite arbitrary.</span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>Thus, more advanced filter methods, e.g. *multivariate* filters based on feature importance, usually perform better <span class="co">[</span><span class="ot">@bommert2020</span><span class="co">]</span>.</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a>On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods.</span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>In the following, it is described how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.</span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>Filter algorithms select features by assigning numeric scores to each feature, e.g. correlation between feature and target variables, use these to rank the features and select a feature subset based on the ranking.</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>Features that are assigned lower scores can then be omitted in subsequent modeling steps.</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>All filters are implemented via the package <span class="in">`r mlr3filters`</span>.</span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>Below, we cover how to</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>instantiate a <span class="in">`Filter`</span> object,</span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>calculate scores for a given task, and</span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>use calculated scores to select or drop features.</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a>One special case of filters are feature importance filters (@sec-fs-var-imp-filters).</span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>They select features that are important according to the model induced by a selected <span class="in">`Learner`</span>.</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>Feature importance filters rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (@sec-interpretation) values for each feature.</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>Many filter methods are implemented in <span class="in">`r mlr3filters`</span>, for example:</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Correlation, calculating Pearson or Spearman correlation between numeric features and numeric targets (<span class="in">`r ref("FilterCorrelation")`</span>)</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Information gain, i.e. mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (<span class="in">`r ref("FilterInformationGain")`</span>)</span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Minimal joint mutual information maximization, minimizing the joint information between selected features to avoid redundancy (<span class="in">`r ref("FilterJMIM")`</span>)</span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Permutation score, which calculates permutation feature importance (see <span class="co">[</span><span class="ot">@sec-interpretation</span><span class="co">]</span>) with a given learner for each feature (<span class="in">`r ref("FilterPermutation")`</span>)</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Area under the ROC curve calculated for each feature separately (<span class="in">`r ref("FilterAUC")`</span>)</span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-78"><a href="#cb54-78" aria-hidden="true" tabindex="-1"></a>Most of the filter methods have some limitations, e.g. the correlation filter can only be calculated for regression tasks with numeric features.</span>
<span id="cb54-79"><a href="#cb54-79" aria-hidden="true" tabindex="-1"></a>For a full list of all implemented filter methods we refer the reader to the <span class="in">`r link("https://mlr3filters.mlr-org.com", "mlr3filters website")`</span>, which also shows the supported task and features types.</span>
<span id="cb54-80"><a href="#cb54-80" aria-hidden="true" tabindex="-1"></a>A benchmark of filter methods was performed by @bommert2020, who recommend to not rely on a single filter method but try several ones if the available computational resources allow.</span>
<span id="cb54-81"><a href="#cb54-81" aria-hidden="true" tabindex="-1"></a>If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see <span class="co">[</span><span class="ot">@sec-fs-var-imp-filters</span><span class="co">]</span>), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.</span>
<span id="cb54-82"><a href="#cb54-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-83"><a href="#cb54-83" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculating Filter Values {#sec-fs-calc}</span></span>
<span id="cb54-84"><a href="#cb54-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-85"><a href="#cb54-85" aria-hidden="true" tabindex="-1"></a>The first step is to create a new R object using the class of the desired filter method.</span>
<span id="cb54-86"><a href="#cb54-86" aria-hidden="true" tabindex="-1"></a>Similar to other instances in <span class="in">`r mlr3`</span>, these are registered in a dictionary (<span class="in">`r ref("mlr_filters")`</span>) with an associated shortcut function <span class="in">`r ref("flt()")`</span>.</span>
<span id="cb54-87"><a href="#cb54-87" aria-hidden="true" tabindex="-1"></a>Each object of class <span class="in">`Filter`</span> has a <span class="in">`$calculate()`</span> method which computes the filter values and ranks them in a descending order.</span>
<span id="cb54-88"><a href="#cb54-88" aria-hidden="true" tabindex="-1"></a>For example, we can use the information gain filter described above:</span>
<span id="cb54-89"><a href="#cb54-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-90"><a href="#cb54-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-001}</span></span>
<span id="cb54-91"><a href="#cb54-91" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mlr3verse"</span>)</span>
<span id="cb54-92"><a href="#cb54-92" aria-hidden="true" tabindex="-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb54-93"><a href="#cb54-93" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-94"><a href="#cb54-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-95"><a href="#cb54-95" aria-hidden="true" tabindex="-1"></a>Such a <span class="in">`Filter`</span> object can now be used to calculate the filter on the penguins data and get the results:</span>
<span id="cb54-96"><a href="#cb54-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-97"><a href="#cb54-97" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-002}</span></span>
<span id="cb54-98"><a href="#cb54-98" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-99"><a href="#cb54-99" aria-hidden="true" tabindex="-1"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb54-100"><a href="#cb54-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-101"><a href="#cb54-101" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(filter)</span>
<span id="cb54-102"><a href="#cb54-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-103"><a href="#cb54-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-104"><a href="#cb54-104" aria-hidden="true" tabindex="-1"></a>Some filters have hyperparameters, which can be changed similar to setting hyperparameters of a <span class="in">`r ref("Learner")`</span> using <span class="in">`$param_set$values`</span>.</span>
<span id="cb54-105"><a href="#cb54-105" aria-hidden="true" tabindex="-1"></a>For example, to calculate <span class="in">`"spearman"`</span> instead of <span class="in">`"pearson"`</span> correlation with the correlation filter:</span>
<span id="cb54-106"><a href="#cb54-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-107"><a href="#cb54-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-003}</span></span>
<span id="cb54-108"><a href="#cb54-108" aria-hidden="true" tabindex="-1"></a>filter_cor <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"correlation"</span>)</span>
<span id="cb54-109"><a href="#cb54-109" aria-hidden="true" tabindex="-1"></a>filter_cor<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> <span class="fu">list</span>(<span class="at">method =</span> <span class="st">"spearman"</span>)</span>
<span id="cb54-110"><a href="#cb54-110" aria-hidden="true" tabindex="-1"></a>filter_cor<span class="sc">$</span>param_set</span>
<span id="cb54-111"><a href="#cb54-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-112"><a href="#cb54-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-113"><a href="#cb54-113" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Importance Filters {#sec-fs-var-imp-filters}</span></span>
<span id="cb54-114"><a href="#cb54-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-115"><a href="#cb54-115" aria-hidden="true" tabindex="-1"></a>To use feature importance filters, we can use a learner with integrated feature importance methods.</span>
<span id="cb54-116"><a href="#cb54-116" aria-hidden="true" tabindex="-1"></a>All learners with the property "importance" have this functionality.</span>
<span id="cb54-117"><a href="#cb54-117" aria-hidden="true" tabindex="-1"></a>A list of all learners with this property can be found with</span>
<span id="cb54-118"><a href="#cb54-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-119"><a href="#cb54-119" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-004}</span></span>
<span id="cb54-120"><a href="#cb54-120" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_learners)[<span class="fu">sapply</span>(properties, <span class="cf">function</span>(x) <span class="st">"importance"</span> <span class="sc">%in%</span> x)]</span>
<span id="cb54-121"><a href="#cb54-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-122"><a href="#cb54-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-123"><a href="#cb54-123" aria-hidden="true" tabindex="-1"></a>or on the <span class="in">`r link("https://mlr-org.com/learners.html", "mlr3 website")`</span>.</span>
<span id="cb54-124"><a href="#cb54-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-125"><a href="#cb54-125" aria-hidden="true" tabindex="-1"></a>For some learners, the desired filter method needs to be set during learner creation.</span>
<span id="cb54-126"><a href="#cb54-126" aria-hidden="true" tabindex="-1"></a>For example, learner <span class="in">`r ref("mlr_learners_classif.ranger", text = "classif.ranger")`</span> comes with multiple integrated methods, c.f. the help page of <span class="in">`r ref("ranger::ranger()")`</span>.</span>
<span id="cb54-127"><a href="#cb54-127" aria-hidden="true" tabindex="-1"></a>To use the feature importance method "impurity", select it during learner construction:</span>
<span id="cb54-128"><a href="#cb54-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-129"><a href="#cb54-129" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-005}</span></span>
<span id="cb54-130"><a href="#cb54-130" aria-hidden="true" tabindex="-1"></a>lrn <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">importance =</span> <span class="st">"impurity"</span>)</span>
<span id="cb54-131"><a href="#cb54-131" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-132"><a href="#cb54-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-133"><a href="#cb54-133" aria-hidden="true" tabindex="-1"></a>Now we can use the <span class="in">`r ref("mlr_filters_importance", text = "FilterImportance")`</span> filter class:</span>
<span id="cb54-134"><a href="#cb54-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-135"><a href="#cb54-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-006}</span></span>
<span id="cb54-136"><a href="#cb54-136" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-137"><a href="#cb54-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-138"><a href="#cb54-138" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove observations with missing data</span></span>
<span id="cb54-139"><a href="#cb54-139" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">filter</span>(<span class="fu">which</span>(<span class="fu">complete.cases</span>(task<span class="sc">$</span><span class="fu">data</span>())))</span>
<span id="cb54-140"><a href="#cb54-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-141"><a href="#cb54-141" aria-hidden="true" tabindex="-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"importance"</span>, <span class="at">learner =</span> lrn)</span>
<span id="cb54-142"><a href="#cb54-142" aria-hidden="true" tabindex="-1"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb54-143"><a href="#cb54-143" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(filter)</span>
<span id="cb54-144"><a href="#cb54-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-145"><a href="#cb54-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-146"><a href="#cb54-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Embedded Methods {#sec-fs-embedded-methods}</span></span>
<span id="cb54-147"><a href="#cb54-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-148"><a href="#cb54-148" aria-hidden="true" tabindex="-1"></a>Many learners internally select a subset of the features which they find helpful for prediction, but ignore other features.</span>
<span id="cb54-149"><a href="#cb54-149" aria-hidden="true" tabindex="-1"></a>For example, a decision tree might never select some features for splitting.</span>
<span id="cb54-150"><a href="#cb54-150" aria-hidden="true" tabindex="-1"></a>These subsets can be used for feature selection, which we call embedded methods because the feature selection is embedded in the learner.</span>
<span id="cb54-151"><a href="#cb54-151" aria-hidden="true" tabindex="-1"></a>The selected features (and those not selected) can be queried if the learner has the <span class="in">`"selected_features"`</span> property, as the following example demonstrates:</span>
<span id="cb54-152"><a href="#cb54-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-153"><a href="#cb54-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-007}</span></span>
<span id="cb54-154"><a href="#cb54-154" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-155"><a href="#cb54-155" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb54-156"><a href="#cb54-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-157"><a href="#cb54-157" aria-hidden="true" tabindex="-1"></a><span class="co"># ensure that the learner selects features</span></span>
<span id="cb54-158"><a href="#cb54-158" aria-hidden="true" tabindex="-1"></a><span class="fu">stopifnot</span>(<span class="st">"selected_features"</span> <span class="sc">%in%</span> learner<span class="sc">$</span>properties)</span>
<span id="cb54-159"><a href="#cb54-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-160"><a href="#cb54-160" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> learner<span class="sc">$</span><span class="fu">train</span>(task)</span>
<span id="cb54-161"><a href="#cb54-161" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span><span class="fu">selected_features</span>()</span>
<span id="cb54-162"><a href="#cb54-162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-163"><a href="#cb54-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-164"><a href="#cb54-164" aria-hidden="true" tabindex="-1"></a>The features selected by the model can be extracted by a <span class="in">`Filter`</span> object, where <span class="in">`$calculate()`</span> corresponds to training the learner on the given task:</span>
<span id="cb54-165"><a href="#cb54-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-166"><a href="#cb54-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-008}</span></span>
<span id="cb54-167"><a href="#cb54-167" aria-hidden="true" tabindex="-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"selected_features"</span>, <span class="at">learner =</span> learner)</span>
<span id="cb54-168"><a href="#cb54-168" aria-hidden="true" tabindex="-1"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb54-169"><a href="#cb54-169" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(filter)</span>
<span id="cb54-170"><a href="#cb54-170" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-171"><a href="#cb54-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-172"><a href="#cb54-172" aria-hidden="true" tabindex="-1"></a>Contrary to other filter methods, embedded methods just return value of 1 (selected features) and 0 (dropped feature).</span>
<span id="cb54-173"><a href="#cb54-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-174"><a href="#cb54-174" aria-hidden="true" tabindex="-1"></a><span class="fu">### Filter-based Feature Selection</span></span>
<span id="cb54-175"><a href="#cb54-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-176"><a href="#cb54-176" aria-hidden="true" tabindex="-1"></a>After calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modelling steps.</span>
<span id="cb54-177"><a href="#cb54-177" aria-hidden="true" tabindex="-1"></a>For the <span class="in">`"selected_features"`</span> filter described in embedded methods (@sec-fs-embedded-methods), this step is straight-forward since the methods assigns either a value of 1 for a feature to be kept or 0 for a feature to be dropped. With <span class="in">`task$select()`</span> the features with a value of 1 can be selected:</span>
<span id="cb54-178"><a href="#cb54-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-179"><a href="#cb54-179" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-009}</span></span>
<span id="cb54-180"><a href="#cb54-180" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-181"><a href="#cb54-181" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb54-182"><a href="#cb54-182" aria-hidden="true" tabindex="-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"selected_features"</span>, <span class="at">learner =</span> learner)</span>
<span id="cb54-183"><a href="#cb54-183" aria-hidden="true" tabindex="-1"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb54-184"><a href="#cb54-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-185"><a href="#cb54-185" aria-hidden="true" tabindex="-1"></a><span class="co"># select all features used by rpart</span></span>
<span id="cb54-186"><a href="#cb54-186" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">which</span>(filter<span class="sc">$</span>scores <span class="sc">==</span> <span class="dv">1</span>))</span>
<span id="cb54-187"><a href="#cb54-187" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb54-188"><a href="#cb54-188" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span>feature_names</span>
<span id="cb54-189"><a href="#cb54-189" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-190"><a href="#cb54-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-191"><a href="#cb54-191" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb54-192"><a href="#cb54-192" aria-hidden="true" tabindex="-1"></a>To select features, we use the function <span class="in">`task$select()`</span> and not <span class="in">`task$filter()`</span>, which is used to filter rows (not columns) of the data matrix, see task mutators (@sec-tasks-mutators).</span>
<span id="cb54-193"><a href="#cb54-193" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb54-194"><a href="#cb54-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-195"><a href="#cb54-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-196"><a href="#cb54-196" aria-hidden="true" tabindex="-1"></a>For filter methods which assign continuous scores, there are essentially two ways to select features:</span>
<span id="cb54-197"><a href="#cb54-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-198"><a href="#cb54-198" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>select the top $k$ features, or</span>
<span id="cb54-199"><a href="#cb54-199" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>select all features with a score above a threshold $\tau$.</span>
<span id="cb54-200"><a href="#cb54-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-201"><a href="#cb54-201" aria-hidden="true" tabindex="-1"></a>Where the first option is equivalent to dropping the bottom $p-k$ features.</span>
<span id="cb54-202"><a href="#cb54-202" aria-hidden="true" tabindex="-1"></a>For both options, one has to decide on a threshold, which is often quite arbitrary.</span>
<span id="cb54-203"><a href="#cb54-203" aria-hidden="true" tabindex="-1"></a>For example, to implement the first option with the information gain filter:</span>
<span id="cb54-204"><a href="#cb54-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-205"><a href="#cb54-205" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-010}</span></span>
<span id="cb54-206"><a href="#cb54-206" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-207"><a href="#cb54-207" aria-hidden="true" tabindex="-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb54-208"><a href="#cb54-208" aria-hidden="true" tabindex="-1"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb54-209"><a href="#cb54-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-210"><a href="#cb54-210" aria-hidden="true" tabindex="-1"></a><span class="co"># select top 3 features from information gain filter</span></span>
<span id="cb54-211"><a href="#cb54-211" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">head</span>(filter<span class="sc">$</span>scores, <span class="dv">3</span>))</span>
<span id="cb54-212"><a href="#cb54-212" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb54-213"><a href="#cb54-213" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span>feature_names</span>
<span id="cb54-214"><a href="#cb54-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-215"><a href="#cb54-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-216"><a href="#cb54-216" aria-hidden="true" tabindex="-1"></a>Or, the second option with $\tau = 0.5$:</span>
<span id="cb54-217"><a href="#cb54-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-218"><a href="#cb54-218" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-011}</span></span>
<span id="cb54-219"><a href="#cb54-219" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-220"><a href="#cb54-220" aria-hidden="true" tabindex="-1"></a>filter <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb54-221"><a href="#cb54-221" aria-hidden="true" tabindex="-1"></a>filter<span class="sc">$</span><span class="fu">calculate</span>(task)</span>
<span id="cb54-222"><a href="#cb54-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-223"><a href="#cb54-223" aria-hidden="true" tabindex="-1"></a><span class="co"># select all features with score &gt;0.5 from information gain filter</span></span>
<span id="cb54-224"><a href="#cb54-224" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">which</span>(filter<span class="sc">$</span>scores <span class="sc">&gt;</span> <span class="fl">0.5</span>))</span>
<span id="cb54-225"><a href="#cb54-225" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb54-226"><a href="#cb54-226" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span>feature_names</span>
<span id="cb54-227"><a href="#cb54-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-228"><a href="#cb54-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-229"><a href="#cb54-229" aria-hidden="true" tabindex="-1"></a>Filters can be integrated into pipelines. While pipelines are described in detail in @sec-pipelines, here is a brief preview:</span>
<span id="cb54-230"><a href="#cb54-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-231"><a href="#cb54-231" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-012}</span></span>
<span id="cb54-232"><a href="#cb54-232" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3pipelines)</span>
<span id="cb54-233"><a href="#cb54-233" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-234"><a href="#cb54-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-235"><a href="#cb54-235" aria-hidden="true" tabindex="-1"></a><span class="co"># combine filter (keep top 3 features) with learner</span></span>
<span id="cb54-236"><a href="#cb54-236" aria-hidden="true" tabindex="-1"></a>graph <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"filter"</span>, <span class="at">filter =</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>), <span class="at">filter.nfeat =</span> <span class="dv">3</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb54-237"><a href="#cb54-237" aria-hidden="true" tabindex="-1"></a>  <span class="fu">po</span>(<span class="st">"learner"</span>, <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span>
<span id="cb54-238"><a href="#cb54-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-239"><a href="#cb54-239" aria-hidden="true" tabindex="-1"></a><span class="co"># now it can be used as any learner, but it includes the feature selection</span></span>
<span id="cb54-240"><a href="#cb54-240" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">as_learner</span>(graph)</span>
<span id="cb54-241"><a href="#cb54-241" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span><span class="fu">train</span>(task)</span>
<span id="cb54-242"><a href="#cb54-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-243"><a href="#cb54-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-244"><a href="#cb54-244" aria-hidden="true" tabindex="-1"></a>Pipelines can also be used to apply hyperparameter optimization (@sec-optimization) to the filter, i.e. tune the filter threshold to optimize the feature selection regarding prediction performance.</span>
<span id="cb54-245"><a href="#cb54-245" aria-hidden="true" tabindex="-1"></a>We first combine a filter with a learner,</span>
<span id="cb54-246"><a href="#cb54-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-247"><a href="#cb54-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-013}</span></span>
<span id="cb54-248"><a href="#cb54-248" aria-hidden="true" tabindex="-1"></a>graph <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"filter"</span>, <span class="at">filter =</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb54-249"><a href="#cb54-249" aria-hidden="true" tabindex="-1"></a>  <span class="fu">po</span>(<span class="st">"learner"</span>, <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span>
<span id="cb54-250"><a href="#cb54-250" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">as_learner</span>(graph)</span>
<span id="cb54-251"><a href="#cb54-251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-252"><a href="#cb54-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-253"><a href="#cb54-253" aria-hidden="true" tabindex="-1"></a>and tune how many feature to include</span>
<span id="cb54-254"><a href="#cb54-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-014, message=FALSE, warning=FALSE}</span></span>
<span id="cb54-255"><a href="#cb54-255" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mlr3tuning"</span>)</span>
<span id="cb54-256"><a href="#cb54-256" aria-hidden="true" tabindex="-1"></a>ps <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">information_gain.filter.nfeat =</span> <span class="fu">p_int</span>(<span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="dv">7</span>))</span>
<span id="cb54-257"><a href="#cb54-257" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> TuningInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(</span>
<span id="cb54-258"><a href="#cb54-258" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> task,</span>
<span id="cb54-259"><a href="#cb54-259" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb54-260"><a href="#cb54-260" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb54-261"><a href="#cb54-261" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb54-262"><a href="#cb54-262" aria-hidden="true" tabindex="-1"></a>  <span class="at">search_space =</span> ps,</span>
<span id="cb54-263"><a href="#cb54-263" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb54-264"><a href="#cb54-264" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-265"><a href="#cb54-265" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>)</span>
<span id="cb54-266"><a href="#cb54-266" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb54-267"><a href="#cb54-267" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-268"><a href="#cb54-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-269"><a href="#cb54-269" aria-hidden="true" tabindex="-1"></a>The output above shows only the best result.</span>
<span id="cb54-270"><a href="#cb54-270" aria-hidden="true" tabindex="-1"></a>To show the results of all tuning steps, retrieve them from the archive of the tuning instance:</span>
<span id="cb54-271"><a href="#cb54-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-272"><a href="#cb54-272" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-015}</span></span>
<span id="cb54-273"><a href="#cb54-273" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)</span>
<span id="cb54-274"><a href="#cb54-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-275"><a href="#cb54-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-276"><a href="#cb54-276" aria-hidden="true" tabindex="-1"></a>We can also plot the tuning results:</span>
<span id="cb54-277"><a href="#cb54-277" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-016}</span></span>
<span id="cb54-278"><a href="#cb54-278" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tunefilter</span></span>
<span id="cb54-279"><a href="#cb54-279" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Model performance with different numbers of features, selected by an information gain filter.</span></span>
<span id="cb54-280"><a href="#cb54-280" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Plot showing model performance in filter-based feature selection, showing that adding a second and third feature to the model improves performance, while adding more feature achieves no further performance gain.</span></span>
<span id="cb54-281"><a href="#cb54-281" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(instance)</span>
<span id="cb54-282"><a href="#cb54-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-283"><a href="#cb54-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-284"><a href="#cb54-284" aria-hidden="true" tabindex="-1"></a>For more details, see Pipelines (@sec-pipelines) and Hyperparameter Optimization (@sec-optimization).</span>
<span id="cb54-285"><a href="#cb54-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-286"><a href="#cb54-286" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wrapper Methods {#sec-fs-wrapper}</span></span>
<span id="cb54-287"><a href="#cb54-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-288"><a href="#cb54-288" aria-hidden="true" tabindex="-1"></a>Wrapper methods work by fitting models on selected feature subsets and evaluating their performance.</span>
<span id="cb54-289"><a href="#cb54-289" aria-hidden="true" tabindex="-1"></a>This can be done in a sequential fashion, e.g. by iteratively adding features to the model in the so-called sequential forward selection, or in a parallel fashion, e.g. by evaluating random feature subsets in a random search.</span>
<span id="cb54-290"><a href="#cb54-290" aria-hidden="true" tabindex="-1"></a>Below, the use of these simple approaches is described in a common framework along with more advanced methods such as genetic search.</span>
<span id="cb54-291"><a href="#cb54-291" aria-hidden="true" tabindex="-1"></a>It is further shown how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.</span>
<span id="cb54-292"><a href="#cb54-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-293"><a href="#cb54-293" aria-hidden="true" tabindex="-1"></a>In more detail, wrapper methods iteratively select features that optimize a performance measure.</span>
<span id="cb54-294"><a href="#cb54-294" aria-hidden="true" tabindex="-1"></a>Instead of ranking features, a model is fit on a selected subset of features in each iteration and evaluated in resampling with respect to a selected performance measure.</span>
<span id="cb54-295"><a href="#cb54-295" aria-hidden="true" tabindex="-1"></a>The strategy that determines which feature subset is used in each iteration is given by the <span class="in">`FSelector`</span> object.</span>
<span id="cb54-296"><a href="#cb54-296" aria-hidden="true" tabindex="-1"></a>A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively adds the feature that leads to the largest performance improvement.</span>
<span id="cb54-297"><a href="#cb54-297" aria-hidden="true" tabindex="-1"></a>Wrapper methods can be used with any learner but need to train the learner potentially many times, leading to a computationally intensive method.</span>
<span id="cb54-298"><a href="#cb54-298" aria-hidden="true" tabindex="-1"></a>All wrapper methods are implemented via the package <span class="in">`r mlr3fselect`</span>.</span>
<span id="cb54-299"><a href="#cb54-299" aria-hidden="true" tabindex="-1"></a>In this chapter, we cover how to</span>
<span id="cb54-300"><a href="#cb54-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-301"><a href="#cb54-301" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>instantiate an <span class="in">`FSelector`</span> object,</span>
<span id="cb54-302"><a href="#cb54-302" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>configure it, to e.g. respect a runtime limit or for different objectives,</span>
<span id="cb54-303"><a href="#cb54-303" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>run it or fuse it with a <span class="in">`Learner`</span> via an <span class="in">`AutoFSelector`</span>.</span>
<span id="cb54-304"><a href="#cb54-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-305"><a href="#cb54-305" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb54-306"><a href="#cb54-306" aria-hidden="true" tabindex="-1"></a>Wrapper-based feature selection is very similar to hyperparameter optimization (@sec-optimization). </span>
<span id="cb54-307"><a href="#cb54-307" aria-hidden="true" tabindex="-1"></a>The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations. </span>
<span id="cb54-308"><a href="#cb54-308" aria-hidden="true" tabindex="-1"></a>We will see below, that we can even use the same terminators, that some feature selection algorithms are similar to tuners and that we can also optimize multiple performance measures with feature selection.</span>
<span id="cb54-309"><a href="#cb54-309" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb54-310"><a href="#cb54-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-311"><a href="#cb54-311" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple Forward Selection Example {#sec-fs-wrapper-example}</span></span>
<span id="cb54-312"><a href="#cb54-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-313"><a href="#cb54-313" aria-hidden="true" tabindex="-1"></a>We start with the simple example from above and do sequential forward selection with the penguins data:</span>
<span id="cb54-314"><a href="#cb54-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-315"><a href="#cb54-315" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-017, message=FALSE}</span></span>
<span id="cb54-316"><a href="#cb54-316" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mlr3fselect"</span>)</span>
<span id="cb54-317"><a href="#cb54-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-318"><a href="#cb54-318" aria-hidden="true" tabindex="-1"></a><span class="co"># subset features to ease visualization</span></span>
<span id="cb54-319"><a href="#cb54-319" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-320"><a href="#cb54-320" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"bill_depth"</span>, <span class="st">"bill_length"</span>, <span class="st">"body_mass"</span>, <span class="st">"flipper_length"</span>))</span>
<span id="cb54-321"><a href="#cb54-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-322"><a href="#cb54-322" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">fselect</span>(</span>
<span id="cb54-323"><a href="#cb54-323" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"sequential"</span>,</span>
<span id="cb54-324"><a href="#cb54-324" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span>  task,</span>
<span id="cb54-325"><a href="#cb54-325" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb54-326"><a href="#cb54-326" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb54-327"><a href="#cb54-327" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>)</span>
<span id="cb54-328"><a href="#cb54-328" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-329"><a href="#cb54-329" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-330"><a href="#cb54-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-331"><a href="#cb54-331" aria-hidden="true" tabindex="-1"></a>To show all analyzed feature subsets and the corresponding performance, we use <span class="in">`as.data.table(instance$archive)`</span>.</span>
<span id="cb54-332"><a href="#cb54-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-333"><a href="#cb54-333" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-018}</span></span>
<span id="cb54-334"><a href="#cb54-334" aria-hidden="true" tabindex="-1"></a>dt <span class="ot">=</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)</span>
<span id="cb54-335"><a href="#cb54-335" aria-hidden="true" tabindex="-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb54-336"><a href="#cb54-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-337"><a href="#cb54-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-338"><a href="#cb54-338" aria-hidden="true" tabindex="-1"></a>We see that the feature <span class="in">`flipper_length`</span> achieved the highest prediction performance in the first iteration and is thus selected. </span>
<span id="cb54-339"><a href="#cb54-339" aria-hidden="true" tabindex="-1"></a>In the second round, adding <span class="in">`bill_length`</span> improves performance to over 90%:</span>
<span id="cb54-340"><a href="#cb54-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-341"><a href="#cb54-341" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-018-2}</span></span>
<span id="cb54-342"><a href="#cb54-342" aria-hidden="true" tabindex="-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb54-343"><a href="#cb54-343" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-344"><a href="#cb54-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-345"><a href="#cb54-345" aria-hidden="true" tabindex="-1"></a>However, adding a third feature does not improve performance</span>
<span id="cb54-346"><a href="#cb54-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-347"><a href="#cb54-347" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-018-3}</span></span>
<span id="cb54-348"><a href="#cb54-348" aria-hidden="true" tabindex="-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">3</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb54-349"><a href="#cb54-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-350"><a href="#cb54-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-351"><a href="#cb54-351" aria-hidden="true" tabindex="-1"></a>and the algorithm terminates. </span>
<span id="cb54-352"><a href="#cb54-352" aria-hidden="true" tabindex="-1"></a>To directly show the best feature set, we can use:</span>
<span id="cb54-353"><a href="#cb54-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-354"><a href="#cb54-354" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-019}</span></span>
<span id="cb54-355"><a href="#cb54-355" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result_feature_set</span>
<span id="cb54-356"><a href="#cb54-356" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-357"><a href="#cb54-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-358"><a href="#cb54-358" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb54-359"><a href="#cb54-359" aria-hidden="true" tabindex="-1"></a><span class="in">`instance$result_feature_set`</span> shows features in alphabetical order and not in the order selected.</span>
<span id="cb54-360"><a href="#cb54-360" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb54-361"><a href="#cb54-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-362"><a href="#cb54-362" aria-hidden="true" tabindex="-1"></a>Internally, the <span class="in">`fselect`</span> function creates an <span class="in">`FSelectInstanceSingleCrit`</span> object and executes the feature selection with an <span class="in">`FSelector`</span> object, based on the selected method, in this example an <span class="in">`FSelectorSequential`</span> object.</span>
<span id="cb54-363"><a href="#cb54-363" aria-hidden="true" tabindex="-1"></a>It uses the supplied resampling and measure to evaluate all feature subsets provided by the <span class="in">`FSelector`</span> on the task.</span>
<span id="cb54-364"><a href="#cb54-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-365"><a href="#cb54-365" aria-hidden="true" tabindex="-1"></a>At the heart of <span class="in">`r mlr3fselect`</span> are the R6 classes:</span>
<span id="cb54-366"><a href="#cb54-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-367"><a href="#cb54-367" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`FSelectInstanceSingleCrit`</span>, <span class="in">`FSelectInstanceMultiCrit`</span>: These two classes describe the feature selection problem and store the results.</span>
<span id="cb54-368"><a href="#cb54-368" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`FSelector`</span>: This class is the base class for implementations of feature selection algorithms.</span>
<span id="cb54-369"><a href="#cb54-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-370"><a href="#cb54-370" aria-hidden="true" tabindex="-1"></a>In the following two sections, these classes will be created manually, to learn more about the <span class="in">`r mlr3fselect`</span> package.</span>
<span id="cb54-371"><a href="#cb54-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-372"><a href="#cb54-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### The `FSelectInstance` Classes</span></span>
<span id="cb54-373"><a href="#cb54-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-374"><a href="#cb54-374" aria-hidden="true" tabindex="-1"></a>To create an <span class="in">`FSelectInstanceSingleCrit`</span> object, we use the sugar function <span class="in">`r ref("fsi")`</span>, which is short for <span class="in">`FSelectInstanceSingleCrit$new()`</span> or <span class="in">`FSelectInstanceMultiCrit$new()`</span>, depending on the selected measure(s):</span>
<span id="cb54-375"><a href="#cb54-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-376"><a href="#cb54-376" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-020}</span></span>
<span id="cb54-377"><a href="#cb54-377" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">fsi</span>(</span>
<span id="cb54-378"><a href="#cb54-378" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>),</span>
<span id="cb54-379"><a href="#cb54-379" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb54-380"><a href="#cb54-380" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb54-381"><a href="#cb54-381" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb54-382"><a href="#cb54-382" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>)</span>
<span id="cb54-383"><a href="#cb54-383" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-384"><a href="#cb54-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-385"><a href="#cb54-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-386"><a href="#cb54-386" aria-hidden="true" tabindex="-1"></a>Note that we have not selected a feature selection algorithm and thus did not select any features, yet.</span>
<span id="cb54-387"><a href="#cb54-387" aria-hidden="true" tabindex="-1"></a>We have also supplied a so-called <span class="in">`Terminator`</span>, which is used to stop the feature selection.</span>
<span id="cb54-388"><a href="#cb54-388" aria-hidden="true" tabindex="-1"></a>For the forward selection in the example above, we did not need a terminator because we simply tried all remaining features until the full model or no further performance improvement.</span>
<span id="cb54-389"><a href="#cb54-389" aria-hidden="true" tabindex="-1"></a>However, for other feature selection algorithms such as <span class="in">`r ref("FSelectorRandomSearch", text = "random search")`</span>, a terminator is required.</span>
<span id="cb54-390"><a href="#cb54-390" aria-hidden="true" tabindex="-1"></a>The following terminator are available:</span>
<span id="cb54-391"><a href="#cb54-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-392"><a href="#cb54-392" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Terminate after a given time (<span class="in">`r ref("TerminatorClockTime")`</span>)</span>
<span id="cb54-393"><a href="#cb54-393" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Terminate after a given amount of iterations (<span class="in">`r ref("TerminatorEvals")`</span>)</span>
<span id="cb54-394"><a href="#cb54-394" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Terminate after a specific performance is reached (<span class="in">`r ref("TerminatorPerfReached")`</span>)</span>
<span id="cb54-395"><a href="#cb54-395" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Terminate when feature selection does not improve (<span class="in">`r ref("TerminatorStagnation")`</span>)</span>
<span id="cb54-396"><a href="#cb54-396" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A combination of the above in an *ALL* or *ANY* fashion (<span class="in">`r ref("TerminatorCombo")`</span>)</span>
<span id="cb54-397"><a href="#cb54-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-398"><a href="#cb54-398" aria-hidden="true" tabindex="-1"></a>Above we used the sugar function <span class="in">`r ref("trm")`</span> to select <span class="in">`r ref("TerminatorEvals")`</span> with 20 evaluations.</span>
<span id="cb54-399"><a href="#cb54-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-400"><a href="#cb54-400" aria-hidden="true" tabindex="-1"></a>To start the feature selection, we still need to select an algorithm which are defined via the <span class="in">`FSelector`</span> class, described in the next section.</span>
<span id="cb54-401"><a href="#cb54-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-402"><a href="#cb54-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### The `FSelector` Class</span></span>
<span id="cb54-403"><a href="#cb54-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-404"><a href="#cb54-404" aria-hidden="true" tabindex="-1"></a>The <span class="in">`FSelector`</span> class is the base class for different feature selection algorithms.</span>
<span id="cb54-405"><a href="#cb54-405" aria-hidden="true" tabindex="-1"></a>The following algorithms are currently implemented in <span class="in">`r mlr3fselect`</span>:</span>
<span id="cb54-406"><a href="#cb54-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-407"><a href="#cb54-407" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Random search, trying random feature subsets until termination (<span class="in">`r ref("FSelectorRandomSearch")`</span>)</span>
<span id="cb54-408"><a href="#cb54-408" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Exhaustive search, trying all possible feature subsets (<span class="in">`r ref("FSelectorExhaustiveSearch")`</span>)</span>
<span id="cb54-409"><a href="#cb54-409" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Sequential search, i.e. sequential forward or backward selection (<span class="in">`r ref("FSelectorSequential")`</span>)</span>
<span id="cb54-410"><a href="#cb54-410" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Recursive feature elimination, which uses learner's importance scores to iteratively remove features with low feature importance (<span class="in">`r ref("FSelectorRFE")`</span>)</span>
<span id="cb54-411"><a href="#cb54-411" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Design points, trying all user-supplied feature sets (<span class="in">`r ref("FSelectorDesignPoints")`</span>)</span>
<span id="cb54-412"><a href="#cb54-412" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Genetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (<span class="in">`r ref("FSelectorGeneticSearch")`</span>)</span>
<span id="cb54-413"><a href="#cb54-413" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Shadow variable search, which adds permuted copies of all features (shadow variables) and stops when a shadow variable is selected (<span class="in">`r ref("FSelectorShadowVariableSearch")`</span>)</span>
<span id="cb54-414"><a href="#cb54-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-415"><a href="#cb54-415" aria-hidden="true" tabindex="-1"></a>In this example, we will use a simple random search and retrieve it from the dictionary <span class="in">`r ref("mlr_fselectors")`</span> with the <span class="in">`r ref("fs()")`</span> sugar function, which is short for <span class="in">`FSelectorRandomSearch$new()`</span>:</span>
<span id="cb54-416"><a href="#cb54-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-417"><a href="#cb54-417" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-021}</span></span>
<span id="cb54-418"><a href="#cb54-418" aria-hidden="true" tabindex="-1"></a>fselector <span class="ot">=</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>)</span>
<span id="cb54-419"><a href="#cb54-419" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-420"><a href="#cb54-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-421"><a href="#cb54-421" aria-hidden="true" tabindex="-1"></a><span class="fu">### Starting the Feature Selection</span></span>
<span id="cb54-422"><a href="#cb54-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-423"><a href="#cb54-423" aria-hidden="true" tabindex="-1"></a>To start the feature selection, we pass the <span class="in">`FSelectInstanceSingleCrit`</span> object to the <span class="in">`$optimize()`</span> method of the initialized <span class="in">`FSelector`</span> object:</span>
<span id="cb54-424"><a href="#cb54-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-425"><a href="#cb54-425" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-022, output=FALSE}</span></span>
<span id="cb54-426"><a href="#cb54-426" aria-hidden="true" tabindex="-1"></a>fselector<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb54-427"><a href="#cb54-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-428"><a href="#cb54-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-429"><a href="#cb54-429" aria-hidden="true" tabindex="-1"></a>The algorithm proceeds as follows</span>
<span id="cb54-430"><a href="#cb54-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-431"><a href="#cb54-431" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The <span class="in">`FSelector`</span> proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting <span class="in">`batch_size`</span>.</span>
<span id="cb54-432"><a href="#cb54-432" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>For each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.</span>
<span id="cb54-433"><a href="#cb54-433" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>All evaluations are stored in the archive of the <span class="in">`FSelectInstanceSingleCrit`</span> object.</span>
<span id="cb54-434"><a href="#cb54-434" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The terminator is queried if the budget is exhausted. If the budget is not exhausted, restart with 1) until it is.</span>
<span id="cb54-435"><a href="#cb54-435" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Determine the feature subset with the best observed performance.</span>
<span id="cb54-436"><a href="#cb54-436" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Store the best feature subset as the result in the instance object.</span>
<span id="cb54-437"><a href="#cb54-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-438"><a href="#cb54-438" aria-hidden="true" tabindex="-1"></a>The best feature subset and the corresponding measured performance can be accessed from the instance:</span>
<span id="cb54-439"><a href="#cb54-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-440"><a href="#cb54-440" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-023}</span></span>
<span id="cb54-441"><a href="#cb54-441" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>result)[, .(features, classif.acc)]</span>
<span id="cb54-442"><a href="#cb54-442" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-443"><a href="#cb54-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-444"><a href="#cb54-444" aria-hidden="true" tabindex="-1"></a>As in the forward selection example above, one can investigate all resamplings which were undertaken, as they are stored in the archive of the <span class="in">`FSelectInstanceSingleCrit`</span> object and can be accessed by using <span class="in">`as.data.table()`</span>:</span>
<span id="cb54-445"><a href="#cb54-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-446"><a href="#cb54-446" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-024}</span></span>
<span id="cb54-447"><a href="#cb54-447" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, .(bill_depth, bill_length, body_mass, classif.acc)]</span>
<span id="cb54-448"><a href="#cb54-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-449"><a href="#cb54-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-450"><a href="#cb54-450" aria-hidden="true" tabindex="-1"></a>Now the optimized feature subset can be used to subset the task and fit the model on all observations:</span>
<span id="cb54-451"><a href="#cb54-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-452"><a href="#cb54-452" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-025, eval=FALSE}</span></span>
<span id="cb54-453"><a href="#cb54-453" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb54-454"><a href="#cb54-454" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb54-455"><a href="#cb54-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-456"><a href="#cb54-456" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">select</span>(instance<span class="sc">$</span>result_feature_set)</span>
<span id="cb54-457"><a href="#cb54-457" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span><span class="fu">train</span>(task)</span>
<span id="cb54-458"><a href="#cb54-458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-459"><a href="#cb54-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-460"><a href="#cb54-460" aria-hidden="true" tabindex="-1"></a>The trained model can now be used to make a prediction on external data.</span>
<span id="cb54-461"><a href="#cb54-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-462"><a href="#cb54-462" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb54-463"><a href="#cb54-463" aria-hidden="true" tabindex="-1"></a>Predicting on observations present in the task used for feature selection should be avoided.</span>
<span id="cb54-464"><a href="#cb54-464" aria-hidden="true" tabindex="-1"></a>The model has seen these observations already during feature selection and therefore performance evaluation results would be over-optimistic.</span>
<span id="cb54-465"><a href="#cb54-465" aria-hidden="true" tabindex="-1"></a>Instead, to get unbiased performance estimates for the current task, nested resampling (see @sec-autofselect and @sec-nested-resampling) is required.</span>
<span id="cb54-466"><a href="#cb54-466" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb54-467"><a href="#cb54-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-468"><a href="#cb54-468" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimizing Multiple Performance Measures {#sec-multicrit-featsel}</span></span>
<span id="cb54-469"><a href="#cb54-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-470"><a href="#cb54-470" aria-hidden="true" tabindex="-1"></a>You might want to use multiple criteria to evaluate the performance of the feature subsets. </span>
<span id="cb54-471"><a href="#cb54-471" aria-hidden="true" tabindex="-1"></a>For example, you might want to select the subset with the highest classification accuracy and lowest time to train the model. </span>
<span id="cb54-472"><a href="#cb54-472" aria-hidden="true" tabindex="-1"></a>However, these two subsets will generally not coincide, i.e. the subset with highest classification accuracy will probably be another subset than that with lowest training time. </span>
<span id="cb54-473"><a href="#cb54-473" aria-hidden="true" tabindex="-1"></a>With <span class="in">`r mlr3fselect`</span>, the result is the pareto-optimal solution, i.e. the best feature subset for each of the criteria that is not dominated by another subset. </span>
<span id="cb54-474"><a href="#cb54-474" aria-hidden="true" tabindex="-1"></a>For the example with classification accuracy and training time, a feature subset that is best in accuracy *and* training time will dominate all other subsets and thus will be the only pareto-optimal solution. </span>
<span id="cb54-475"><a href="#cb54-475" aria-hidden="true" tabindex="-1"></a>If, however, different subsets are best in the two criteria, both subsets are pareto-optimal.</span>
<span id="cb54-476"><a href="#cb54-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-477"><a href="#cb54-477" aria-hidden="true" tabindex="-1"></a>We will expand the previous example and perform feature selection on the penguins dataset, however, this time we will use <span class="in">`r ref("FSelectInstanceMultiCrit")`</span> to select the subset of features that has the highest classification accuracy and the one with the lowest time to train the model.</span>
<span id="cb54-478"><a href="#cb54-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-479"><a href="#cb54-479" aria-hidden="true" tabindex="-1"></a>The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:</span>
<span id="cb54-480"><a href="#cb54-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-481"><a href="#cb54-481" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-026}</span></span>
<span id="cb54-482"><a href="#cb54-482" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">fsi</span>(</span>
<span id="cb54-483"><a href="#cb54-483" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>),</span>
<span id="cb54-484"><a href="#cb54-484" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>),</span>
<span id="cb54-485"><a href="#cb54-485" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb54-486"><a href="#cb54-486" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.acc"</span>, <span class="st">"time_train"</span>)),</span>
<span id="cb54-487"><a href="#cb54-487" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">5</span>)</span>
<span id="cb54-488"><a href="#cb54-488" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-489"><a href="#cb54-489" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-490"><a href="#cb54-490" aria-hidden="true" tabindex="-1"></a>The function <span class="in">`r ref("fsi")`</span> creates an instance of <span class="in">`FSelectInstanceMultiCrit`</span> if more than one measure is selected.</span>
<span id="cb54-491"><a href="#cb54-491" aria-hidden="true" tabindex="-1"></a>We now create an <span class="in">`FSelector`</span> and call the <span class="in">`$optimize()`</span> function of the <span class="in">`FSelector`</span> with the <span class="in">`FSelectInstanceMultiCrit`</span> object, to search for the subset of features with the best classification accuracy and time to train the model.</span>
<span id="cb54-492"><a href="#cb54-492" aria-hidden="true" tabindex="-1"></a>This time, we use <span class="in">`r ref("FSelectorDesignPoints", text = "design points")`</span> to manually specify two feature sets to try: one with only the feature <span class="in">`sex`</span> and one with all features except <span class="in">`island`</span>, <span class="in">`sex`</span> and <span class="in">`year`</span>. </span>
<span id="cb54-493"><a href="#cb54-493" aria-hidden="true" tabindex="-1"></a>We expect the sex-only model to train fast and the model including many features to be accurate.</span>
<span id="cb54-494"><a href="#cb54-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-495"><a href="#cb54-495" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-027, output=FALSE}</span></span>
<span id="cb54-496"><a href="#cb54-496" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> mlr3misc<span class="sc">::</span><span class="fu">rowwise_table</span>(</span>
<span id="cb54-497"><a href="#cb54-497" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>bill_depth, <span class="sc">~</span>bill_length, <span class="sc">~</span>body_mass, <span class="sc">~</span>flipper_length, <span class="sc">~</span>island, <span class="sc">~</span>sex, <span class="sc">~</span>year,</span>
<span id="cb54-498"><a href="#cb54-498" aria-hidden="true" tabindex="-1"></a>  <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>,</span>
<span id="cb54-499"><a href="#cb54-499" aria-hidden="true" tabindex="-1"></a>  <span class="cn">TRUE</span>, <span class="cn">TRUE</span>, <span class="cn">TRUE</span>, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span>, <span class="cn">FALSE</span></span>
<span id="cb54-500"><a href="#cb54-500" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-501"><a href="#cb54-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-502"><a href="#cb54-502" aria-hidden="true" tabindex="-1"></a>fselector <span class="ot">=</span> <span class="fu">fs</span>(<span class="st">"design_points"</span>, <span class="at">design =</span> design)</span>
<span id="cb54-503"><a href="#cb54-503" aria-hidden="true" tabindex="-1"></a>fselector<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb54-504"><a href="#cb54-504" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-505"><a href="#cb54-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-506"><a href="#cb54-506" aria-hidden="true" tabindex="-1"></a>As above, the best feature subset and the corresponding measured performance can be accessed from the instance. </span>
<span id="cb54-507"><a href="#cb54-507" aria-hidden="true" tabindex="-1"></a>However, in this simple case, if the fastest subset is not also the best performing subset, the result consists of two subsets: one with the lowest training time and one with the best classification accuracy:</span>
<span id="cb54-508"><a href="#cb54-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-509"><a href="#cb54-509" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-029}</span></span>
<span id="cb54-510"><a href="#cb54-510" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>result)[, .(features, classif.acc, time_train)]</span>
<span id="cb54-511"><a href="#cb54-511" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-512"><a href="#cb54-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-513"><a href="#cb54-513" aria-hidden="true" tabindex="-1"></a>As explained above, the result is the pareto-optimal solution.</span>
<span id="cb54-514"><a href="#cb54-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-515"><a href="#cb54-515" aria-hidden="true" tabindex="-1"></a><span class="fu">### Automating the Feature Selection {#sec-autofselect}</span></span>
<span id="cb54-516"><a href="#cb54-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-517"><a href="#cb54-517" aria-hidden="true" tabindex="-1"></a>The <span class="in">`AutoFSelector`</span> class wraps a learner and augments it with an automatic feature selection for a given task.</span>
<span id="cb54-518"><a href="#cb54-518" aria-hidden="true" tabindex="-1"></a>Because the <span class="in">`AutoFSelector`</span> itself inherits from the <span class="in">`r ref("Learner")`</span> base class, it can be used like any other learner.</span>
<span id="cb54-519"><a href="#cb54-519" aria-hidden="true" tabindex="-1"></a>Below, a new learner is created.</span>
<span id="cb54-520"><a href="#cb54-520" aria-hidden="true" tabindex="-1"></a>This learner is then wrapped in a random search feature selector, which automatically starts a feature selection on the given task using an inner resampling, as soon as the wrapped learner is trained.</span>
<span id="cb54-521"><a href="#cb54-521" aria-hidden="true" tabindex="-1"></a>Here, the function <span class="in">`r ref("auto_fselector")`</span> creates an instance of <span class="in">`AutoFSelector`</span>, i.e. it is short for <span class="in">`AutoFSelector$new()`</span>.</span>
<span id="cb54-522"><a href="#cb54-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-523"><a href="#cb54-523" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-030}</span></span>
<span id="cb54-524"><a href="#cb54-524" aria-hidden="true" tabindex="-1"></a>at <span class="ot">=</span> <span class="fu">auto_fselector</span>(</span>
<span id="cb54-525"><a href="#cb54-525" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>),</span>
<span id="cb54-526"><a href="#cb54-526" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.log_reg"</span>),</span>
<span id="cb54-527"><a href="#cb54-527" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb54-528"><a href="#cb54-528" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb54-529"><a href="#cb54-529" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">10</span>)</span>
<span id="cb54-530"><a href="#cb54-530" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-531"><a href="#cb54-531" aria-hidden="true" tabindex="-1"></a>at</span>
<span id="cb54-532"><a href="#cb54-532" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-533"><a href="#cb54-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-534"><a href="#cb54-534" aria-hidden="true" tabindex="-1"></a>We can now, as with any other learner, call the <span class="in">`$train()`</span> and <span class="in">`$predict()`</span> method.</span>
<span id="cb54-535"><a href="#cb54-535" aria-hidden="true" tabindex="-1"></a>This time however, we pass it to <span class="in">`r ref("benchmark()")`</span> to compare the optimized feature subset to the complete feature set.</span>
<span id="cb54-536"><a href="#cb54-536" aria-hidden="true" tabindex="-1"></a>This way, the <span class="in">`AutoFSelector`</span> will do its resampling for feature selection on the training set of the respective split of the outer resampling.</span>
<span id="cb54-537"><a href="#cb54-537" aria-hidden="true" tabindex="-1"></a>The learner then undertakes predictions using the test set of the outer resampling.</span>
<span id="cb54-538"><a href="#cb54-538" aria-hidden="true" tabindex="-1"></a>Here, the outer resampling refers to the resampling specified in <span class="in">`benchmark()`</span>, whereas the inner resampling is that specified in <span class="in">`auto_fselector()`</span>.</span>
<span id="cb54-539"><a href="#cb54-539" aria-hidden="true" tabindex="-1"></a>This is called nested resampling (@sec-nested-resampling) and yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.</span>
<span id="cb54-540"><a href="#cb54-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-541"><a href="#cb54-541" aria-hidden="true" tabindex="-1"></a>In the call to <span class="in">`benchmark()`</span>, we compare our wrapped learner <span class="in">`at`</span> with a normal logistic regression <span class="in">`lrn("classif.log_reg")`</span>.</span>
<span id="cb54-542"><a href="#cb54-542" aria-hidden="true" tabindex="-1"></a>For that, we create a benchmark grid with the task, the learners and a 3-fold cross validation on the <span class="in">`r ref("mlr_tasks_sonar", text = "sonar")`</span> data.</span>
<span id="cb54-543"><a href="#cb54-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-544"><a href="#cb54-544" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-031, warning=FALSE}</span></span>
<span id="cb54-545"><a href="#cb54-545" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">=</span> <span class="fu">benchmark_grid</span>(</span>
<span id="cb54-546"><a href="#cb54-546" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb54-547"><a href="#cb54-547" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">list</span>(at, <span class="fu">lrn</span>(<span class="st">"classif.log_reg"</span>)),</span>
<span id="cb54-548"><a href="#cb54-548" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb54-549"><a href="#cb54-549" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-550"><a href="#cb54-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-551"><a href="#cb54-551" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(grid)</span>
<span id="cb54-552"><a href="#cb54-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-553"><a href="#cb54-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-554"><a href="#cb54-554" aria-hidden="true" tabindex="-1"></a>Now, we compare those two learners regarding classification accuracy and training time:</span>
<span id="cb54-555"><a href="#cb54-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-556"><a href="#cb54-556" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature-selection-032}</span></span>
<span id="cb54-557"><a href="#cb54-557" aria-hidden="true" tabindex="-1"></a>aggr <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.acc"</span>, <span class="st">"time_train"</span>)))</span>
<span id="cb54-558"><a href="#cb54-558" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(aggr)[, .(learner_id, classif.acc, time_train)]</span>
<span id="cb54-559"><a href="#cb54-559" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb54-560"><a href="#cb54-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-561"><a href="#cb54-561" aria-hidden="true" tabindex="-1"></a>We can see that, in this example, the feature selection improves prediction performance but also drastically increases the training time, since the feature selection (including resampling and random search) is part of the model training of the wrapped learner.</span>
<span id="cb54-562"><a href="#cb54-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-563"><a href="#cb54-563" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb54-564"><a href="#cb54-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-565"><a href="#cb54-565" aria-hidden="true" tabindex="-1"></a>In this chapter, we learned how to perform feature selection with mlr3.</span>
<span id="cb54-566"><a href="#cb54-566" aria-hidden="true" tabindex="-1"></a>We introduced filter and wrapper methods, combined feature selection with pipelines, learned how to automate the feature selection and covered the optimization of multiple performance measures.</span>
<span id="cb54-567"><a href="#cb54-567" aria-hidden="true" tabindex="-1"></a>@tbl-api-feature-selection gives an overview of the most important functions (S3) and classes (R6) used in this chapter.</span>
<span id="cb54-568"><a href="#cb54-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-569"><a href="#cb54-569" aria-hidden="true" tabindex="-1"></a>| S3 function | R6 Class | Summary |</span>
<span id="cb54-570"><a href="#cb54-570" aria-hidden="true" tabindex="-1"></a>| --- | --- | --- |</span>
<span id="cb54-571"><a href="#cb54-571" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("flt()")`</span>   | <span class="in">`r ref("Filter")`</span> | Selects features by calculating a score for each feature |</span>
<span id="cb54-572"><a href="#cb54-572" aria-hidden="true" tabindex="-1"></a>| <span class="in">`Filter$calculate()`</span>   | <span class="in">`r ref("Filter")`</span> | Calculates scores on a given task |</span>
<span id="cb54-573"><a href="#cb54-573" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("fselect()")`</span> | <span class="in">`r ref("FSelectInstanceSingleCrit")`</span> or  <span class="in">`r ref("FSelectInstanceMultiCrit")`</span> | Specifies a feature selection problem and stores the results |</span>
<span id="cb54-574"><a href="#cb54-574" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("fs()")`</span> | <span class="in">`r ref("FSelector")`</span> | Specifies a feature selection algorithm |</span>
<span id="cb54-575"><a href="#cb54-575" aria-hidden="true" tabindex="-1"></a>| <span class="in">`FSelector$optimize()`</span> | <span class="in">`r ref("FSelector")`</span> | Executes the features selection specified by the <span class="in">`FSelectInstance`</span> with the algorithm specified by the <span class="in">`FSelector`</span> |</span>
<span id="cb54-576"><a href="#cb54-576" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("auto_fselector()")`</span> | <span class="in">`r ref("AutoFSelector")`</span> | Defines a learner that includes feature selection |</span>
<span id="cb54-577"><a href="#cb54-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-578"><a href="#cb54-578" aria-hidden="true" tabindex="-1"></a>:Core S3 'sugar' functions for feature selection in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-feature-selection}</span>
<span id="cb54-579"><a href="#cb54-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-580"><a href="#cb54-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-581"><a href="#cb54-581" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resources{.unnumbered .unlisted}</span></span>
<span id="cb54-582"><a href="#cb54-582" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A list of implemented filters in the <span class="in">`r mlr3filters`</span> package is provided on the <span class="in">`r link("https://mlr3filters.mlr-org.com", "mlr3filters website")`</span>.</span>
<span id="cb54-583"><a href="#cb54-583" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A summary of wrapper-based feature selection with the <span class="in">`r mlr3fselect`</span> package is provided in the <span class="in">`r link("https://cheatsheets.mlr-org.com/mlr3fselect.pdf", "mlr3fselect cheatsheet")`</span>.</span>
<span id="cb54-584"><a href="#cb54-584" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>An overview of feature selection methods is provided by @chandrashekar2014.</span>
<span id="cb54-585"><a href="#cb54-585" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A more formal and detailed introduction to filters and wrappers is given in @guyon2003.</span>
<span id="cb54-586"><a href="#cb54-586" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>@bommert2020 perform a benchmark of filter methods.</span>
<span id="cb54-587"><a href="#cb54-587" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Filters can be used as part of a machine learning pipeline (@sec-pipelines).</span>
<span id="cb54-588"><a href="#cb54-588" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Filters can be optimized with hyperparameter optimization (@sec-optimization).</span>
<span id="cb54-589"><a href="#cb54-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-590"><a href="#cb54-590" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb54-591"><a href="#cb54-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-592"><a href="#cb54-592" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate a correlation filter on the <span class="in">`r ref("mlr_tasks_mtcars", text = "Motor Trend")`</span> data set (<span class="in">`mtcars`</span>).</span>
<span id="cb54-593"><a href="#cb54-593" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use the filter from the first exercise to select the five best features in the <span class="in">`mtcars`</span> data set.</span>
<span id="cb54-594"><a href="#cb54-594" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Apply a backward selection to the <span class="in">`r ref("mlr_tasks_penguins", text = "penguins")`</span> data set with a classification tree learner <span class="in">`"classif.rpart"`</span> and holdout resampling by the measure classification accuracy. Compare the results with those in @sec-fs-wrapper-example. Answer the following questions:</span>
<span id="cb54-595"><a href="#cb54-595" aria-hidden="true" tabindex="-1"></a>    a. Do the selected features differ?</span>
<span id="cb54-596"><a href="#cb54-596" aria-hidden="true" tabindex="-1"></a>    b. Which feature selection method achieves a higher classification accuracy?</span>
<span id="cb54-597"><a href="#cb54-597" aria-hidden="true" tabindex="-1"></a>    c. Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?</span>
<span id="cb54-598"><a href="#cb54-598" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Automate the feature selection as in @sec-autofselect with the <span class="in">`r ref("mlr_tasks_spam", text = "spam")`</span> data set and a logistic regression learner (<span class="in">`"classif.log_reg"`</span>). Hint: Remember to call <span class="in">`library("mlr3learners")`</span> for the logistic regression learner.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licenced under <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.</div>   
      <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Written with <i class="bi bi-heart-fill"></i> for #rstats, ML and FOSS by the mlr-org team.</div>
  </div>
</footer>


<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>